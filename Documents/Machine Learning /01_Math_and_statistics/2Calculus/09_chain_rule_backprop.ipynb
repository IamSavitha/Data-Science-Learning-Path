{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain Rule and Backpropagation\n",
    "## The Foundation of Neural Network Training\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Chain Rule Fundamentals](#chain-rule)\n",
    "2. [Multivariate Chain Rule](#multivariate)\n",
    "3. [Computational Graphs](#comp-graphs)\n",
    "4. [Backpropagation Algorithm](#backprop)\n",
    "5. [Neural Network from Scratch](#neural-net)\n",
    "6. [Visualizations](#visualizations)\n",
    "7. [Practice Problems](#practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. CHAIN RULE FUNDAMENTALS\n",
    "\n",
    "## 1.1 What is the Chain Rule?\n",
    "\n",
    "The chain rule allows us to differentiate **composite functions** - functions of functions.\n",
    "\n",
    "If $y = f(g(x))$, then:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "Or using Leibniz notation with $u = g(x)$:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n",
    "\n",
    "### Intuition\n",
    "Think of it as: \"How does y change with respect to x?\" = \"How does y change with respect to u?\" × \"How does u change with respect to x?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple Chain Rule\n",
    "# y = (3x + 2)^2\n",
    "# Let u = 3x + 2, then y = u^2\n",
    "\n",
    "def f(x):\n",
    "    return (3*x + 2)**2\n",
    "\n",
    "# Manual derivative using chain rule:\n",
    "# dy/du = 2u = 2(3x + 2)\n",
    "# du/dx = 3\n",
    "# dy/dx = 2(3x + 2) * 3 = 6(3x + 2)\n",
    "\n",
    "def f_derivative(x):\n",
    "    return 6 * (3*x + 2)\n",
    "\n",
    "# Numerical verification\n",
    "def numerical_derivative(func, x, h=1e-7):\n",
    "    return (func(x + h) - func(x - h)) / (2 * h)\n",
    "\n",
    "x_test = 2.0\n",
    "analytical = f_derivative(x_test)\n",
    "numerical = numerical_derivative(f, x_test)\n",
    "\n",
    "print(f\"f(x) = (3x + 2)^2\")\n",
    "print(f\"\\nAt x = {x_test}:\")\n",
    "print(f\"Analytical derivative: {analytical}\")\n",
    "print(f\"Numerical derivative: {numerical:.6f}\")\n",
    "print(f\"Difference: {abs(analytical - numerical):.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the function and its derivative\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = f(x)\n",
    "dy = f_derivative(x)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Function\n",
    "axes[0].plot(x, y, 'b-', linewidth=2, label='$f(x) = (3x+2)^2$')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Function')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Derivative\n",
    "axes[1].plot(x, dy, 'r-', linewidth=2, label=\"$f'(x) = 6(3x+2)$\")\n",
    "axes[1].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('dy/dx')\n",
    "axes[1].set_title('Derivative (using Chain Rule)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Common Chain Rule Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Exponential function\n",
    "# y = e^(x^2)\n",
    "# dy/dx = e^(x^2) * 2x\n",
    "\n",
    "def exp_squared(x):\n",
    "    return np.exp(x**2)\n",
    "\n",
    "def exp_squared_derivative(x):\n",
    "    return np.exp(x**2) * 2 * x\n",
    "\n",
    "print(\"Example 2: y = e^(x²)\")\n",
    "print(\"dy/dx = e^(x²) · 2x\")\n",
    "print(f\"\\nAt x = 1: dy/dx = {exp_squared_derivative(1):.4f}\")\n",
    "print(f\"Numerical: {numerical_derivative(exp_squared, 1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Trigonometric composition\n",
    "# y = sin(x^3)\n",
    "# dy/dx = cos(x^3) * 3x^2\n",
    "\n",
    "def sin_cubed(x):\n",
    "    return np.sin(x**3)\n",
    "\n",
    "def sin_cubed_derivative(x):\n",
    "    return np.cos(x**3) * 3 * x**2\n",
    "\n",
    "print(\"Example 3: y = sin(x³)\")\n",
    "print(\"dy/dx = cos(x³) · 3x²\")\n",
    "print(f\"\\nAt x = 1: dy/dx = {sin_cubed_derivative(1):.4f}\")\n",
    "print(f\"Numerical: {numerical_derivative(sin_cubed, 1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Nested composition (double chain rule)\n",
    "# y = sin(cos(x))\n",
    "# dy/dx = cos(cos(x)) * (-sin(x)) = -cos(cos(x)) * sin(x)\n",
    "\n",
    "def sin_cos(x):\n",
    "    return np.sin(np.cos(x))\n",
    "\n",
    "def sin_cos_derivative(x):\n",
    "    return -np.cos(np.cos(x)) * np.sin(x)\n",
    "\n",
    "print(\"Example 4: y = sin(cos(x))\")\n",
    "print(\"dy/dx = -cos(cos(x)) · sin(x)\")\n",
    "print(f\"\\nAt x = π/4: dy/dx = {sin_cos_derivative(np.pi/4):.4f}\")\n",
    "print(f\"Numerical: {numerical_derivative(sin_cos, np.pi/4):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. MULTIVARIATE CHAIN RULE\n",
    "\n",
    "## 2.1 Chain Rule with Multiple Variables\n",
    "\n",
    "For $z = f(x, y)$ where $x = g(t)$ and $y = h(t)$:\n",
    "\n",
    "$$\\frac{dz}{dt} = \\frac{\\partial f}{\\partial x} \\cdot \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\cdot \\frac{dy}{dt}$$\n",
    "\n",
    "This is the **total derivative** and is crucial for backpropagation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: z = x^2 * y where x = t^2 and y = t^3\n",
    "# dz/dt = (∂z/∂x)(dx/dt) + (∂z/∂y)(dy/dt)\n",
    "#       = (2xy)(2t) + (x^2)(3t^2)\n",
    "#       = 2(t^2)(t^3)(2t) + (t^4)(3t^2)\n",
    "#       = 4t^6 + 3t^6 = 7t^6\n",
    "\n",
    "def z_of_t(t):\n",
    "    x = t**2\n",
    "    y = t**3\n",
    "    return x**2 * y\n",
    "\n",
    "def dz_dt_analytical(t):\n",
    "    return 7 * t**6\n",
    "\n",
    "t_test = 2.0\n",
    "print(\"z = x² · y, where x = t², y = t³\")\n",
    "print(\"\\nUsing multivariate chain rule:\")\n",
    "print(\"dz/dt = (∂z/∂x)(dx/dt) + (∂z/∂y)(dy/dt)\")\n",
    "print(\"      = (2xy)(2t) + (x²)(3t²)\")\n",
    "print(\"      = 7t⁶\")\n",
    "print(f\"\\nAt t = {t_test}:\")\n",
    "print(f\"Analytical dz/dt = {dz_dt_analytical(t_test)}\")\n",
    "print(f\"Numerical dz/dt = {numerical_derivative(z_of_t, t_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Jacobian Matrix\n",
    "\n",
    "For vector-valued functions, the chain rule involves the **Jacobian matrix**.\n",
    "\n",
    "If $\\mathbf{y} = f(\\mathbf{x})$ and $\\mathbf{x} = g(\\mathbf{t})$, then:\n",
    "\n",
    "$$\\frac{d\\mathbf{y}}{d\\mathbf{t}} = \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\cdot \\frac{d\\mathbf{x}}{d\\mathbf{t}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Computing Jacobian\n",
    "# f(x, y) = [x^2 + y, x*y]\n",
    "# Jacobian = [[∂f1/∂x, ∂f1/∂y],\n",
    "#             [∂f2/∂x, ∂f2/∂y]]\n",
    "#          = [[2x, 1],\n",
    "#             [y, x]]\n",
    "\n",
    "def f_vector(x, y):\n",
    "    return np.array([x**2 + y, x * y])\n",
    "\n",
    "def jacobian(x, y):\n",
    "    return np.array([[2*x, 1],\n",
    "                     [y, x]])\n",
    "\n",
    "x, y = 2.0, 3.0\n",
    "J = jacobian(x, y)\n",
    "\n",
    "print(\"f(x, y) = [x² + y, x·y]\")\n",
    "print(f\"\\nAt (x, y) = ({x}, {y}):\")\n",
    "print(f\"f = {f_vector(x, y)}\")\n",
    "print(f\"\\nJacobian matrix:\")\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. COMPUTATIONAL GRAPHS\n",
    "\n",
    "## 3.1 What are Computational Graphs?\n",
    "\n",
    "A computational graph represents a function as a **directed acyclic graph (DAG)** where:\n",
    "- **Nodes** represent operations or variables\n",
    "- **Edges** represent data flow\n",
    "\n",
    "This is how deep learning frameworks (PyTorch, TensorFlow) track computations for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: f(x, y, z) = (x + y) * z\n",
    "# \n",
    "# Computational Graph:\n",
    "#   x ----\\\n",
    "#          (+) = a ---\\\n",
    "#   y ----/            (*) = f\n",
    "#   z ----------------/\n",
    "\n",
    "class ComputationalGraph:\n",
    "    \"\"\"Simple computational graph for f = (x + y) * z\"\"\"\n",
    "    \n",
    "    def forward(self, x, y, z):\n",
    "        \"\"\"Forward pass: compute the output.\"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "        \n",
    "        # Intermediate computation\n",
    "        self.a = x + y  # a = x + y\n",
    "        \n",
    "        # Final output\n",
    "        self.f = self.a * z  # f = a * z\n",
    "        \n",
    "        return self.f\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backward pass: compute gradients using chain rule.\"\"\"\n",
    "        # Start with df/df = 1\n",
    "        df_df = 1\n",
    "        \n",
    "        # f = a * z\n",
    "        # df/da = z\n",
    "        # df/dz = a\n",
    "        df_da = self.z * df_df\n",
    "        df_dz = self.a * df_df\n",
    "        \n",
    "        # a = x + y\n",
    "        # da/dx = 1\n",
    "        # da/dy = 1\n",
    "        # Using chain rule:\n",
    "        # df/dx = df/da * da/dx\n",
    "        # df/dy = df/da * da/dy\n",
    "        df_dx = df_da * 1\n",
    "        df_dy = df_da * 1\n",
    "        \n",
    "        return df_dx, df_dy, df_dz\n",
    "\n",
    "# Test the computational graph\n",
    "graph = ComputationalGraph()\n",
    "x, y, z = 2.0, 3.0, 4.0\n",
    "\n",
    "# Forward pass\n",
    "f = graph.forward(x, y, z)\n",
    "print(f\"f(x, y, z) = (x + y) * z\")\n",
    "print(f\"\\nFor x={x}, y={y}, z={z}:\")\n",
    "print(f\"Forward pass: f = {f}\")\n",
    "\n",
    "# Backward pass\n",
    "df_dx, df_dy, df_dz = graph.backward()\n",
    "print(f\"\\nBackward pass (gradients):\")\n",
    "print(f\"∂f/∂x = {df_dx}\")\n",
    "print(f\"∂f/∂y = {df_dy}\")\n",
    "print(f\"∂f/∂z = {df_dz}\")\n",
    "\n",
    "# Verify: f = (x+y)*z, so df/dx = z, df/dy = z, df/dz = x+y\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Expected ∂f/∂x = z = {z}\")\n",
    "print(f\"Expected ∂f/∂y = z = {z}\")\n",
    "print(f\"Expected ∂f/∂z = x+y = {x+y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the computational graph\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Draw nodes\n",
    "nodes = {\n",
    "    'x': (0, 2), 'y': (0, 1), 'z': (0, 0),\n",
    "    '+': (2, 1.5), '*': (4, 0.75), 'f': (6, 0.75)\n",
    "}\n",
    "\n",
    "for name, (px, py) in nodes.items():\n",
    "    circle = plt.Circle((px, py), 0.3, color='lightblue', ec='black')\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(px, py, name, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Draw edges\n",
    "edges = [\n",
    "    ('x', '+'), ('y', '+'), ('+', '*'), ('z', '*'), ('*', 'f')\n",
    "]\n",
    "\n",
    "for start, end in edges:\n",
    "    x1, y1 = nodes[start]\n",
    "    x2, y2 = nodes[end]\n",
    "    ax.annotate('', xy=(x2-0.3, y2), xytext=(x1+0.3, y1),\n",
    "                arrowprops=dict(arrowstyle='->', color='black'))\n",
    "\n",
    "# Add labels for gradients\n",
    "ax.text(1, 2.3, 'df/dx = z', fontsize=10, color='red')\n",
    "ax.text(1, 0.7, 'df/dy = z', fontsize=10, color='red')\n",
    "ax.text(1, -0.3, 'df/dz = x+y', fontsize=10, color='red')\n",
    "\n",
    "ax.set_xlim(-1, 7)\n",
    "ax.set_ylim(-1, 3)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Computational Graph: f = (x + y) * z\\nwith gradient flow', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. BACKPROPAGATION ALGORITHM\n",
    "\n",
    "## 4.1 The Core Idea\n",
    "\n",
    "Backpropagation efficiently computes gradients using the chain rule, working **backwards** from the output.\n",
    "\n",
    "For a neural network with loss $L$:\n",
    "1. **Forward pass**: Compute all intermediate values and final loss\n",
    "2. **Backward pass**: Compute gradients from output to input using chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete example: Simple neuron with sigmoid activation\n",
    "# y = sigmoid(w*x + b)\n",
    "# Loss = (y - target)^2\n",
    "\n",
    "class SimpleNeuron:\n",
    "    \"\"\"A single neuron with sigmoid activation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w = np.random.randn()\n",
    "        self.b = np.random.randn()\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        self.x = x\n",
    "        self.z = self.w * x + self.b  # Linear combination\n",
    "        self.y = self.sigmoid(self.z)  # Activation\n",
    "        return self.y\n",
    "    \n",
    "    def compute_loss(self, target):\n",
    "        \"\"\"Compute MSE loss.\"\"\"\n",
    "        self.target = target\n",
    "        self.loss = (self.y - target) ** 2\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backward pass: compute gradients.\"\"\"\n",
    "        # Chain rule:\n",
    "        # dL/dw = dL/dy * dy/dz * dz/dw\n",
    "        # dL/db = dL/dy * dy/dz * dz/db\n",
    "        \n",
    "        # dL/dy = 2(y - target)\n",
    "        dL_dy = 2 * (self.y - self.target)\n",
    "        \n",
    "        # dy/dz = sigmoid'(z) = sigmoid(z) * (1 - sigmoid(z))\n",
    "        dy_dz = self.sigmoid_derivative(self.z)\n",
    "        \n",
    "        # dz/dw = x\n",
    "        dz_dw = self.x\n",
    "        \n",
    "        # dz/db = 1\n",
    "        dz_db = 1\n",
    "        \n",
    "        # Apply chain rule\n",
    "        self.dL_dw = dL_dy * dy_dz * dz_dw\n",
    "        self.dL_db = dL_dy * dy_dz * dz_db\n",
    "        \n",
    "        return self.dL_dw, self.dL_db\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"Update parameters using gradients.\"\"\"\n",
    "        self.w -= learning_rate * self.dL_dw\n",
    "        self.b -= learning_rate * self.dL_db\n",
    "\n",
    "# Test the neuron\n",
    "neuron = SimpleNeuron()\n",
    "x = 1.0\n",
    "target = 0.8\n",
    "\n",
    "print(\"Simple Neuron: y = sigmoid(w*x + b)\")\n",
    "print(f\"Initial w = {neuron.w:.4f}, b = {neuron.b:.4f}\")\n",
    "\n",
    "# Forward pass\n",
    "y = neuron.forward(x)\n",
    "loss = neuron.compute_loss(target)\n",
    "print(f\"\\nForward pass:\")\n",
    "print(f\"  x = {x}, target = {target}\")\n",
    "print(f\"  y = {y:.4f}, loss = {loss:.4f}\")\n",
    "\n",
    "# Backward pass\n",
    "dL_dw, dL_db = neuron.backward()\n",
    "print(f\"\\nBackward pass:\")\n",
    "print(f\"  dL/dw = {dL_dw:.4f}\")\n",
    "print(f\"  dL/db = {dL_db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neuron\n",
    "neuron = SimpleNeuron()\n",
    "x = 1.0\n",
    "target = 0.8\n",
    "learning_rate = 0.5\n",
    "epochs = 100\n",
    "\n",
    "losses = []\n",
    "predictions = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward\n",
    "    y = neuron.forward(x)\n",
    "    loss = neuron.compute_loss(target)\n",
    "    \n",
    "    losses.append(loss)\n",
    "    predictions.append(y)\n",
    "    \n",
    "    # Backward\n",
    "    neuron.backward()\n",
    "    \n",
    "    # Update\n",
    "    neuron.update(learning_rate)\n",
    "\n",
    "# Plot training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(losses, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(predictions, 'g-', linewidth=2, label='Prediction')\n",
    "axes[1].axhline(y=target, color='r', linestyle='--', linewidth=2, label='Target')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Output')\n",
    "axes[1].set_title('Prediction vs Target')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final prediction: {predictions[-1]:.4f}\")\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"Final loss: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. NEURAL NETWORK FROM SCRATCH\n",
    "\n",
    "## 5.1 Two-Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNN:\n",
    "    \"\"\"Simple 2-layer neural network.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights with small random values\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        # Layer 1\n",
    "        self.Z1 = X @ self.W1 + self.b1\n",
    "        self.A1 = self.relu(self.Z1)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.Z2 = self.A1 @ self.W2 + self.b2\n",
    "        self.A2 = self.sigmoid(self.Z2)\n",
    "        \n",
    "        return self.A2\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"Binary cross-entropy loss.\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y_true):\n",
    "        \"\"\"Backward pass: compute gradients.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dZ2 = self.A2 - y_true  # Derivative of BCE + sigmoid\n",
    "        self.dW2 = (1/m) * self.A1.T @ dZ2\n",
    "        self.db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer gradients (chain rule!)\n",
    "        dA1 = dZ2 @ self.W2.T\n",
    "        dZ1 = dA1 * self.relu_derivative(self.Z1)\n",
    "        self.dW1 = (1/m) * X.T @ dZ1\n",
    "        self.db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"Update parameters.\"\"\"\n",
    "        self.W1 -= learning_rate * self.dW1\n",
    "        self.b1 -= learning_rate * self.db1\n",
    "        self.W2 -= learning_rate * self.dW2\n",
    "        self.b2 -= learning_rate * self.db2\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        \"\"\"Train the network.\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward\n",
    "            y_pred = self.forward(X)\n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward\n",
    "            self.backward(X, y)\n",
    "            \n",
    "            # Update\n",
    "            self.update(learning_rate)\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return (self.forward(X) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])  # XOR\n",
    "\n",
    "print(\"XOR Problem:\")\n",
    "print(\"Input -> Output\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"{X[i]} -> {y[i][0]}\")\n",
    "\n",
    "# Train the network\n",
    "nn = TwoLayerNN(input_size=2, hidden_size=4, output_size=1)\n",
    "print(\"\\nTraining...\")\n",
    "losses = nn.train(X, y, epochs=1000, learning_rate=1.0)\n",
    "\n",
    "# Test\n",
    "predictions = nn.predict(X)\n",
    "print(\"\\nPredictions:\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"{X[i]} -> {predictions[i][0]} (expected: {y[i][0]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Neural Network Training Loss (XOR Problem)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100),\n",
    "                     np.linspace(-0.5, 1.5, 100))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = nn.forward(grid).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, levels=50, cmap='RdBu', alpha=0.7)\n",
    "plt.colorbar(label='Output')\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X[y.flatten()==0, 0], X[y.flatten()==0, 1], \n",
    "            c='blue', s=200, edgecolors='black', label='Class 0')\n",
    "plt.scatter(X[y.flatten()==1, 0], X[y.flatten()==1, 1], \n",
    "            c='red', s=200, edgecolors='black', label='Class 1')\n",
    "\n",
    "plt.xlabel('x₁')\n",
    "plt.ylabel('x₂')\n",
    "plt.title('Neural Network Decision Boundary for XOR')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. VISUALIZATIONS\n",
    "\n",
    "## 6.1 Gradient Flow Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how gradients flow through activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "sigmoid_grad = sigmoid * (1 - sigmoid)\n",
    "\n",
    "# Tanh\n",
    "tanh = np.tanh(x)\n",
    "tanh_grad = 1 - tanh**2\n",
    "\n",
    "# ReLU\n",
    "relu = np.maximum(0, x)\n",
    "relu_grad = (x > 0).astype(float)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0, 0].plot(x, sigmoid, 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Sigmoid')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(x, sigmoid_grad, 'r-', linewidth=2)\n",
    "axes[1, 0].set_title('Sigmoid Gradient')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_ylim([0, 0.3])\n",
    "\n",
    "# Tanh\n",
    "axes[0, 1].plot(x, tanh, 'b-', linewidth=2)\n",
    "axes[0, 1].set_title('Tanh')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(x, tanh_grad, 'r-', linewidth=2)\n",
    "axes[1, 1].set_title('Tanh Gradient')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# ReLU\n",
    "axes[0, 2].plot(x, relu, 'b-', linewidth=2)\n",
    "axes[0, 2].set_title('ReLU')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 2].plot(x, relu_grad, 'r-', linewidth=2)\n",
    "axes[1, 2].set_title('ReLU Gradient')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Activation Functions and Their Gradients', y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"- Sigmoid gradient vanishes for large |x| (vanishing gradient problem)\")\n",
    "print(\"- Tanh has similar issues but gradients are stronger\")\n",
    "print(\"- ReLU has constant gradient of 1 for positive values (no vanishing gradient)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. PRACTICE PROBLEMS\n",
    "\n",
    "## Problem 1: Compute Chain Rule Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Find dy/dx for y = ln(sin(x²))\n",
    "# Solution:\n",
    "# Let u = x², v = sin(u), y = ln(v)\n",
    "# dy/dx = dy/dv * dv/du * du/dx\n",
    "#       = (1/v) * cos(u) * 2x\n",
    "#       = (1/sin(x²)) * cos(x²) * 2x\n",
    "#       = 2x * cot(x²)\n",
    "\n",
    "def y_func(x):\n",
    "    return np.log(np.sin(x**2))\n",
    "\n",
    "def dy_dx(x):\n",
    "    return 2 * x * (np.cos(x**2) / np.sin(x**2))\n",
    "\n",
    "# Verify\n",
    "x_test = 1.5\n",
    "print(\"y = ln(sin(x²))\")\n",
    "print(\"dy/dx = 2x·cot(x²)\")\n",
    "print(f\"\\nAt x = {x_test}:\")\n",
    "print(f\"Analytical: {dy_dx(x_test):.6f}\")\n",
    "print(f\"Numerical: {numerical_derivative(y_func, x_test):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Implement Backprop for Custom Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement backprop for: f(x, y) = x² + xy + y²\n",
    "\n",
    "class QuadraticFunction:\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x**2 + x*y + y**2\n",
    "    \n",
    "    def backward(self):\n",
    "        # df/dx = 2x + y\n",
    "        # df/dy = x + 2y\n",
    "        df_dx = 2 * self.x + self.y\n",
    "        df_dy = self.x + 2 * self.y\n",
    "        return df_dx, df_dy\n",
    "\n",
    "# Test\n",
    "func = QuadraticFunction()\n",
    "x, y = 3.0, 2.0\n",
    "\n",
    "f = func.forward(x, y)\n",
    "df_dx, df_dy = func.backward()\n",
    "\n",
    "print(f\"f(x, y) = x² + xy + y²\")\n",
    "print(f\"\\nAt (x, y) = ({x}, {y}):\")\n",
    "print(f\"f = {f}\")\n",
    "print(f\"∂f/∂x = {df_dx}\")\n",
    "print(f\"∂f/∂y = {df_dy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Chain Rule**: Derivative of composite functions\n",
    "   - $\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$\n",
    "\n",
    "2. **Multivariate Chain Rule**: For functions of multiple variables\n",
    "   - Total derivative includes all paths\n",
    "\n",
    "3. **Computational Graphs**: Visual representation of computations\n",
    "   - Nodes = operations, Edges = data flow\n",
    "\n",
    "4. **Backpropagation**: Efficient gradient computation\n",
    "   - Forward pass: compute values\n",
    "   - Backward pass: compute gradients\n",
    "\n",
    "### Why It Matters for ML:\n",
    "\n",
    "- Neural networks are compositions of simple functions\n",
    "- Training requires gradients of loss w.r.t. all parameters\n",
    "- Backprop computes all gradients in one backward pass\n",
    "- Understanding chain rule = understanding deep learning!\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "1. Implement backprop for more complex architectures\n",
    "2. Study automatic differentiation frameworks\n",
    "3. Learn about gradient flow and vanishing/exploding gradients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
