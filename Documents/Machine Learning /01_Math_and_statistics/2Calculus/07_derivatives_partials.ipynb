{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Guide to Derivatives and Partial Derivatives\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "1. What derivatives are and why they matter in ML\n",
    "2. Computing derivatives by hand and with code\n",
    "3. Derivative rules and techniques\n",
    "4. Partial derivatives for multivariable functions\n",
    "5. Gradients and their role in optimization\n",
    "6. Applications in machine learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a Derivative?\n",
    "\n",
    "### Intuitive Understanding\n",
    "\n",
    "A **derivative** tells us how fast something is changing.\n",
    "\n",
    "**Real-world examples:**\n",
    "- **Speed** is the derivative of position (how fast position changes)\n",
    "- **Acceleration** is the derivative of speed (how fast speed changes)\n",
    "- **Slope** of a function at a point\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "For a function $f(x)$, the derivative $f'(x)$ is:\n",
    "\n",
    "$$f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "**In ML:** Derivatives tell us how to adjust our model parameters to reduce error!\n",
    "\n",
    "### Why Derivatives Matter in ML\n",
    "\n",
    "1. **Optimization**: Finding minimum of loss function\n",
    "2. **Backpropagation**: Training neural networks\n",
    "3. **Feature importance**: Understanding model behavior\n",
    "4. **Convergence**: Ensuring training stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1k/26p0wvvn3pq_5gdg1nyk14340000gn/T/ipykernel_37562/508780730.py:3: DeprecationWarning: scipy.misc is deprecated and will be removed in 2.0.0\n",
      "  from scipy.misc import derivative\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'derivative' from 'scipy.misc' (/Users/savithavijayarangan/miniconda3/lib/python3.13/site-packages/scipy/misc/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m derivative\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Set style\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'derivative' from 'scipy.misc' (/Users/savithavijayarangan/miniconda3/lib/python3.13/site-packages/scipy/misc/__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import derivative\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Visualize the concept of derivative\n",
    "def f(x):\n",
    "    \"\"\"Example function: x^2\"\"\"\n",
    "    return x**2\n",
    "\n",
    "# Create data\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = f(x)\n",
    "\n",
    "# Pick a point\n",
    "x0 = 1\n",
    "y0 = f(x0)\n",
    "\n",
    "# Calculate derivative at x0 (for x^2, derivative is 2x)\n",
    "slope = 2 * x0  # f'(x) = 2x\n",
    "\n",
    "# Tangent line: y = mx + b, where m is slope\n",
    "tangent_x = np.linspace(x0 - 1, x0 + 1, 50)\n",
    "tangent_y = slope * (tangent_x - x0) + y0\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Left plot: Function and tangent\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='f(x) = x¬≤')\n",
    "plt.plot(tangent_x, tangent_y, 'r--', linewidth=2, label=f'Tangent (slope={slope})')\n",
    "plt.scatter([x0], [y0], color='red', s=100, zorder=5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Derivative = Slope of Tangent Line')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Right plot: Derivative function\n",
    "plt.subplot(1, 2, 2)\n",
    "derivative_y = 2 * x  # f'(x) = 2x\n",
    "plt.plot(x, derivative_y, 'g-', linewidth=2, label=\"f'(x) = 2x\")\n",
    "plt.scatter([x0], [slope], color='red', s=100, zorder=5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.title('Derivative Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"At x = {x0}:\")\n",
    "print(f\"  Function value: f({x0}) = {y0}\")\n",
    "print(f\"  Derivative (slope): f'({x0}) = {slope}\")\n",
    "print(f\"  Interpretation: Function increasing at rate of {slope} units per unit change in x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Basic Derivative Rules\n",
    "\n",
    "### Power Rule\n",
    "$$\\frac{d}{dx}[x^n] = nx^{n-1}$$\n",
    "\n",
    "### Constant Rule\n",
    "$$\\frac{d}{dx}[c] = 0$$\n",
    "\n",
    "### Constant Multiple Rule\n",
    "$$\\frac{d}{dx}[cf(x)] = c \\cdot f'(x)$$\n",
    "\n",
    "### Sum Rule\n",
    "$$\\frac{d}{dx}[f(x) + g(x)] = f'(x) + g'(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate basic rules\n",
    "\n",
    "print(\"=== POWER RULE ===\")\n",
    "print(\"f(x) = x¬≥  ‚Üí  f'(x) = 3x¬≤\")\n",
    "print(\"f(x) = x‚Åµ  ‚Üí  f'(x) = 5x‚Å¥\")\n",
    "print(\"f(x) = x   ‚Üí  f'(x) = 1\")\n",
    "print(\"f(x) = x‚Å∞  ‚Üí  f'(x) = 0 (constant)\")\n",
    "\n",
    "print(\"\\n=== CONSTANT RULE ===\")\n",
    "print(\"f(x) = 5   ‚Üí  f'(x) = 0\")\n",
    "print(\"f(x) = 100 ‚Üí  f'(x) = 0\")\n",
    "\n",
    "print(\"\\n=== CONSTANT MULTIPLE RULE ===\")\n",
    "print(\"f(x) = 3x¬≤ ‚Üí  f'(x) = 3¬∑(2x) = 6x\")\n",
    "print(\"f(x) = 5x¬≥ ‚Üí  f'(x) = 5¬∑(3x¬≤) = 15x¬≤\")\n",
    "\n",
    "print(\"\\n=== SUM RULE ===\")\n",
    "print(\"f(x) = x¬≤ + x¬≥  ‚Üí  f'(x) = 2x + 3x¬≤\")\n",
    "print(\"f(x) = 4x¬≤ - 2x + 7  ‚Üí  f'(x) = 8x - 2\")\n",
    "\n",
    "# Verify with code\n",
    "def verify_derivative(f, f_prime, x_vals, label):\n",
    "    \"\"\"Verify derivative numerically\"\"\"\n",
    "    print(f\"\\n{label}\")\n",
    "    for x in x_vals:\n",
    "        analytical = f_prime(x)\n",
    "        numerical = derivative(f, x, dx=1e-8)\n",
    "        print(f\"  x={x}: Analytical={analytical:.4f}, Numerical={numerical:.4f}, Match={np.isclose(analytical, numerical)}\")\n",
    "\n",
    "# Test examples\n",
    "verify_derivative(\n",
    "    f=lambda x: x**3,\n",
    "    f_prime=lambda x: 3*x**2,\n",
    "    x_vals=[0, 1, 2],\n",
    "    label=\"Verify: f(x)=x¬≥, f'(x)=3x¬≤\"\n",
    ")\n",
    "\n",
    "verify_derivative(\n",
    "    f=lambda x: 4*x**2 - 2*x + 7,\n",
    "    f_prime=lambda x: 8*x - 2,\n",
    "    x_vals=[0, 1, 2],\n",
    "    label=\"Verify: f(x)=4x¬≤-2x+7, f'(x)=8x-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Advanced Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PRODUCT RULE ===\")\n",
    "print(\"d/dx[f(x)¬∑g(x)] = f'(x)¬∑g(x) + f(x)¬∑g'(x)\")\n",
    "print(\"\\nExample: f(x) = x¬≤¬∑sin(x)\")\n",
    "print(\"f'(x) = 2x¬∑sin(x) + x¬≤¬∑cos(x)\")\n",
    "\n",
    "print(\"\\n=== QUOTIENT RULE ===\")\n",
    "print(\"d/dx[f(x)/g(x)] = [f'(x)¬∑g(x) - f(x)¬∑g'(x)] / [g(x)]¬≤\")\n",
    "print(\"\\nExample: f(x) = x¬≤/x¬≥ = 1/x\")\n",
    "print(\"f'(x) = -1/x¬≤\")\n",
    "\n",
    "print(\"\\n=== EXPONENTIAL ===\")\n",
    "print(\"d/dx[eÀ£] = eÀ£\")\n",
    "print(\"d/dx[aÀ£] = aÀ£¬∑ln(a)\")\n",
    "\n",
    "print(\"\\n=== LOGARITHM ===\")\n",
    "print(\"d/dx[ln(x)] = 1/x\")\n",
    "print(\"d/dx[log_a(x)] = 1/(x¬∑ln(a))\")\n",
    "\n",
    "print(\"\\n=== TRIGONOMETRIC ===\")\n",
    "print(\"d/dx[sin(x)] = cos(x)\")\n",
    "print(\"d/dx[cos(x)] = -sin(x)\")\n",
    "print(\"d/dx[tan(x)] = sec¬≤(x) = 1/cos¬≤(x)\")\n",
    "\n",
    "# Visualize common derivatives\n",
    "x = np.linspace(-2, 2, 200)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "fig.suptitle('Common Functions and Their Derivatives', fontsize=16)\n",
    "\n",
    "# x^2\n",
    "axes[0, 0].plot(x, x**2, 'b-', label='f(x)=x¬≤', linewidth=2)\n",
    "axes[0, 0].plot(x, 2*x, 'r--', label=\"f'(x)=2x\", linewidth=2)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_title('Polynomial')\n",
    "\n",
    "# e^x\n",
    "axes[0, 1].plot(x, np.exp(x), 'b-', label='f(x)=eÀ£', linewidth=2)\n",
    "axes[0, 1].plot(x, np.exp(x), 'r--', label=\"f'(x)=eÀ£\", linewidth=2)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_title('Exponential')\n",
    "axes[0, 1].set_ylim(-1, 8)\n",
    "\n",
    "# ln(x)\n",
    "x_pos = x[x > 0.1]\n",
    "axes[0, 2].plot(x_pos, np.log(x_pos), 'b-', label='f(x)=ln(x)', linewidth=2)\n",
    "axes[0, 2].plot(x_pos, 1/x_pos, 'r--', label=\"f'(x)=1/x\", linewidth=2)\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "axes[0, 2].set_title('Logarithm')\n",
    "axes[0, 2].set_ylim(-2, 2)\n",
    "\n",
    "# sin(x)\n",
    "axes[1, 0].plot(x, np.sin(x), 'b-', label='f(x)=sin(x)', linewidth=2)\n",
    "axes[1, 0].plot(x, np.cos(x), 'r--', label=\"f'(x)=cos(x)\", linewidth=2)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_title('Sine')\n",
    "\n",
    "# cos(x)\n",
    "axes[1, 1].plot(x, np.cos(x), 'b-', label='f(x)=cos(x)', linewidth=2)\n",
    "axes[1, 1].plot(x, -np.sin(x), 'r--', label=\"f'(x)=-sin(x)\", linewidth=2)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_title('Cosine')\n",
    "\n",
    "# 1/x\n",
    "x_nonzero = x[np.abs(x) > 0.1]\n",
    "axes[1, 2].plot(x_nonzero, 1/x_nonzero, 'b-', label='f(x)=1/x', linewidth=2)\n",
    "axes[1, 2].plot(x_nonzero, -1/x_nonzero**2, 'r--', label=\"f'(x)=-1/x¬≤\", linewidth=2)\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "axes[1, 2].set_title('Reciprocal')\n",
    "axes[1, 2].set_ylim(-5, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Partial Derivatives\n",
    "\n",
    "### What are Partial Derivatives?\n",
    "\n",
    "For functions with multiple variables $f(x, y, z, ...)$, a **partial derivative** measures how the function changes when we vary ONE variable while keeping others constant.\n",
    "\n",
    "**Notation:**\n",
    "- $\\frac{\\partial f}{\\partial x}$ = partial derivative with respect to $x$\n",
    "- $f_x$ = shorthand notation\n",
    "\n",
    "### Example\n",
    "\n",
    "For $f(x, y) = x^2 + xy + y^2$:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = 2x + y \\quad \\text{(treat y as constant)}$$\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial y} = x + 2y \\quad \\text{(treat x as constant)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize partial derivatives\n",
    "def f(x, y):\n",
    "    \"\"\"Function of two variables\"\"\"\n",
    "    return x**2 + x*y + y**2\n",
    "\n",
    "# Create mesh\n",
    "x = np.linspace(-3, 3, 50)\n",
    "y = np.linspace(-3, 3, 50)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "# Partial derivatives\n",
    "def df_dx(x, y):\n",
    "    return 2*x + y\n",
    "\n",
    "def df_dy(x, y):\n",
    "    return x + 2*y\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x,y)')\n",
    "ax1.set_title('f(x,y) = x¬≤ + xy + y¬≤')\n",
    "plt.colorbar(surf, ax=ax1, shrink=0.5)\n",
    "\n",
    "# Partial derivative with respect to x\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "dZ_dx = df_dx(X, Y)\n",
    "surf2 = ax2.plot_surface(X, Y, dZ_dx, cmap='Reds', alpha=0.8)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_zlabel('‚àÇf/‚àÇx')\n",
    "ax2.set_title('Partial Derivative: ‚àÇf/‚àÇx = 2x + y')\n",
    "plt.colorbar(surf2, ax=ax2, shrink=0.5)\n",
    "\n",
    "# Partial derivative with respect to y\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "dZ_dy = df_dy(X, Y)\n",
    "surf3 = ax3.plot_surface(X, Y, dZ_dy, cmap='Blues', alpha=0.8)\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.set_zlabel('‚àÇf/‚àÇy')\n",
    "ax3.set_title('Partial Derivative: ‚àÇf/‚àÇy = x + 2y')\n",
    "plt.colorbar(surf3, ax=ax3, shrink=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Numerical example\n",
    "x0, y0 = 1, 2\n",
    "print(f\"\\nAt point ({x0}, {y0}):\")\n",
    "print(f\"  f({x0}, {y0}) = {f(x0, y0)}\")\n",
    "print(f\"  ‚àÇf/‚àÇx = {df_dx(x0, y0)} (rate of change in x-direction)\")\n",
    "print(f\"  ‚àÇf/‚àÇy = {df_dy(x0, y0)} (rate of change in y-direction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Partial Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== COMPUTING PARTIAL DERIVATIVES ===\")\n",
    "print(\"\\nExample 1: f(x, y) = x¬≤y + y¬≥\")\n",
    "print(\"  ‚àÇf/‚àÇx = 2xy (treat y as constant)\")\n",
    "print(\"  ‚àÇf/‚àÇy = x¬≤ + 3y¬≤ (treat x as constant)\")\n",
    "\n",
    "print(\"\\nExample 2: f(x, y, z) = x¬≤yz + sin(xy) + z¬≥\")\n",
    "print(\"  ‚àÇf/‚àÇx = 2xyz + y¬∑cos(xy)\")\n",
    "print(\"  ‚àÇf/‚àÇy = x¬≤z + x¬∑cos(xy)\")\n",
    "print(\"  ‚àÇf/‚àÇz = x¬≤y + 3z¬≤\")\n",
    "\n",
    "print(\"\\nExample 3: f(x, y) = e^(xy)\")\n",
    "print(\"  ‚àÇf/‚àÇx = y¬∑e^(xy)\")\n",
    "print(\"  ‚àÇf/‚àÇy = x¬∑e^(xy)\")\n",
    "\n",
    "# Verify with numerical differentiation\n",
    "def numerical_partial(f, x, y, var='x', h=1e-8):\n",
    "    \"\"\"Compute partial derivative numerically\"\"\"\n",
    "    if var == 'x':\n",
    "        return (f(x + h, y) - f(x, y)) / h\n",
    "    else:\n",
    "        return (f(x, y + h) - f(x, y)) / h\n",
    "\n",
    "# Test function\n",
    "def test_f(x, y):\n",
    "    return x**2 * y + y**3\n",
    "\n",
    "x0, y0 = 2, 3\n",
    "\n",
    "# Analytical\n",
    "analytical_x = 2 * x0 * y0\n",
    "analytical_y = x0**2 + 3 * y0**2\n",
    "\n",
    "# Numerical\n",
    "numerical_x = numerical_partial(test_f, x0, y0, var='x')\n",
    "numerical_y = numerical_partial(test_f, x0, y0, var='y')\n",
    "\n",
    "print(f\"\\n=== VERIFICATION ===\")\n",
    "print(f\"At ({x0}, {y0}):\")\n",
    "print(f\"  ‚àÇf/‚àÇx: Analytical={analytical_x}, Numerical={numerical_x:.6f}\")\n",
    "print(f\"  ‚àÇf/‚àÇy: Analytical={analytical_y}, Numerical={numerical_y:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. The Gradient Vector\n",
    "\n",
    "### Definition\n",
    "\n",
    "The **gradient** $\\nabla f$ is a vector of all partial derivatives:\n",
    "\n",
    "$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}$$\n",
    "\n",
    "### Key Properties:\n",
    "1. **Points in direction of steepest ascent**\n",
    "2. **Magnitude = rate of steepest increase**\n",
    "3. **Perpendicular to level curves/surfaces**\n",
    "\n",
    "### In Machine Learning:\n",
    "The negative gradient $-\\nabla f$ points toward the minimum - this is the foundation of gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient vectors\n",
    "def f(x, y):\n",
    "    return x**2 + y**2  # Simple bowl shape\n",
    "\n",
    "def gradient(x, y):\n",
    "    \"\"\"Compute gradient vector\"\"\"\n",
    "    df_dx = 2 * x\n",
    "    df_dy = 2 * y\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "# Create grid\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "# Sample points for gradient vectors\n",
    "x_sample = np.linspace(-2, 2, 8)\n",
    "y_sample = np.linspace(-2, 2, 8)\n",
    "X_sample, Y_sample = np.meshgrid(x_sample, y_sample)\n",
    "\n",
    "# Compute gradients at sample points\n",
    "U = 2 * X_sample  # ‚àÇf/‚àÇx\n",
    "V = 2 * Y_sample  # ‚àÇf/‚àÇy\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Contour plot with gradient vectors\n",
    "contour = ax1.contour(X, Y, Z, levels=15, cmap='viridis')\n",
    "ax1.clabel(contour, inline=True, fontsize=8)\n",
    "ax1.quiver(X_sample, Y_sample, U, V, color='red', alpha=0.7, scale=20)\n",
    "ax1.scatter([0], [0], color='yellow', s=200, marker='*', \n",
    "           edgecolors='black', linewidths=2, label='Minimum', zorder=5)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('Gradient Vectors Point Uphill\\n(Perpendicular to Contours)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# 3D surface with gradient\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)\n",
    "# Add a few gradient vectors in 3D\n",
    "for i in range(0, len(x_sample), 2):\n",
    "    for j in range(0, len(y_sample), 2):\n",
    "        x0, y0 = x_sample[i], y_sample[j]\n",
    "        z0 = f(x0, y0)\n",
    "        grad = gradient(x0, y0)\n",
    "        ax2.quiver(x0, y0, z0, grad[0], grad[1], 0, \n",
    "                  color='red', arrow_length_ratio=0.3, linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_zlabel('f(x,y)')\n",
    "ax2.set_title('3D View: Gradients Point Uphill')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGradient Properties:\")\n",
    "print(\"‚úì Gradient points in direction of steepest ascent\")\n",
    "print(\"‚úì Negative gradient (-‚àáf) points toward minimum\")\n",
    "print(\"‚úì Gradient is perpendicular to contour lines\")\n",
    "print(\"‚úì Magnitude of gradient = rate of steepest increase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradients in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(f, x, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Numerically compute gradient using finite differences\n",
    "    \n",
    "    Args:\n",
    "        f: function to differentiate\n",
    "        x: point at which to evaluate gradient (numpy array)\n",
    "        epsilon: small perturbation for numerical differentiation\n",
    "    \n",
    "    Returns:\n",
    "        gradient vector\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += epsilon\n",
    "        \n",
    "        x_minus = x.copy()\n",
    "        x_minus[i] -= epsilon\n",
    "        \n",
    "        grad[i] = (f(x_plus) - f(x_minus)) / (2 * epsilon)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "# Example 1: Simple quadratic\n",
    "def f1(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "x1 = np.array([2.0, 3.0])\n",
    "grad1_numerical = compute_gradient(f1, x1)\n",
    "grad1_analytical = 2 * x1  # [2x, 2y]\n",
    "\n",
    "print(\"=== Example 1: f(x,y) = x¬≤ + y¬≤ ===\")\n",
    "print(f\"At point {x1}:\")\n",
    "print(f\"  Numerical gradient: {grad1_numerical}\")\n",
    "print(f\"  Analytical gradient: {grad1_analytical}\")\n",
    "print(f\"  Match: {np.allclose(grad1_numerical, grad1_analytical)}\")\n",
    "\n",
    "# Example 2: More complex function\n",
    "def f2(x):\n",
    "    return x[0]**2 * x[1] + np.sin(x[0]) + x[1]**3\n",
    "\n",
    "def f2_gradient_analytical(x):\n",
    "    df_dx = 2*x[0]*x[1] + np.cos(x[0])\n",
    "    df_dy = x[0]**2 + 3*x[1]**2\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "x2 = np.array([1.0, 2.0])\n",
    "grad2_numerical = compute_gradient(f2, x2)\n",
    "grad2_analytical = f2_gradient_analytical(x2)\n",
    "\n",
    "print(\"\\n=== Example 2: f(x,y) = x¬≤y + sin(x) + y¬≥ ===\")\n",
    "print(f\"At point {x2}:\")\n",
    "print(f\"  Numerical gradient: {grad2_numerical}\")\n",
    "print(f\"  Analytical gradient: {grad2_analytical}\")\n",
    "print(f\"  Match: {np.allclose(grad2_numerical, grad2_analytical)}\")\n",
    "\n",
    "# Example 3: Higher dimensions\n",
    "def f3(x):\n",
    "    \"\"\"Sum of squares in n dimensions\"\"\"\n",
    "    return np.sum(x**2)\n",
    "\n",
    "x3 = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "grad3_numerical = compute_gradient(f3, x3)\n",
    "grad3_analytical = 2 * x3\n",
    "\n",
    "print(\"\\n=== Example 3: f(x) = Œ£x·µ¢¬≤ (4D) ===\")\n",
    "print(f\"At point {x3}:\")\n",
    "print(f\"  Numerical gradient: {grad3_numerical}\")\n",
    "print(f\"  Analytical gradient: {grad3_analytical}\")\n",
    "print(f\"  Match: {np.allclose(grad3_numerical, grad3_analytical)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Applications in Machine Learning\n",
    "\n",
    "### 5.1 Linear Regression - Computing Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression example\n",
    "# Model: y = wx + b\n",
    "# Loss: L = (1/n)Œ£(y_pred - y_true)¬≤\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X_data = np.linspace(0, 10, 50)\n",
    "y_data = 2 * X_data + 1 + np.random.randn(50) * 2  # True: y = 2x + 1 + noise\n",
    "\n",
    "# Initial parameters\n",
    "w = 0.0  # weight\n",
    "b = 0.0  # bias\n",
    "\n",
    "def predict(X, w, b):\n",
    "    \"\"\"Make predictions\"\"\"\n",
    "    return w * X + b\n",
    "\n",
    "def mse_loss(X, y, w, b):\n",
    "    \"\"\"Mean squared error loss\"\"\"\n",
    "    y_pred = predict(X, w, b)\n",
    "    return np.mean((y_pred - y)**2)\n",
    "\n",
    "def compute_gradients(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute gradients of MSE loss with respect to w and b\n",
    "    \n",
    "    L = (1/n)Œ£(wx + b - y)¬≤\n",
    "    ‚àÇL/‚àÇw = (2/n)Œ£(wx + b - y)¬∑x\n",
    "    ‚àÇL/‚àÇb = (2/n)Œ£(wx + b - y)\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    y_pred = predict(X, w, b)\n",
    "    error = y_pred - y\n",
    "    \n",
    "    dL_dw = (2 / n) * np.sum(error * X)\n",
    "    dL_db = (2 / n) * np.sum(error)\n",
    "    \n",
    "    return dL_dw, dL_db\n",
    "\n",
    "# Compute gradients at initial parameters\n",
    "dL_dw, dL_db = compute_gradients(X_data, y_data, w, b)\n",
    "\n",
    "print(\"=== LINEAR REGRESSION GRADIENTS ===\")\n",
    "print(f\"Initial parameters: w={w:.4f}, b={b:.4f}\")\n",
    "print(f\"Initial loss: {mse_loss(X_data, y_data, w, b):.4f}\")\n",
    "print(f\"\\nGradients:\")\n",
    "print(f\"  ‚àÇL/‚àÇw = {dL_dw:.4f}\")\n",
    "print(f\"  ‚àÇL/‚àÇb = {dL_db:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if dL_dw > 0:\n",
    "    print(f\"  ‚Ä¢ Decreasing w will reduce loss (gradient is positive)\")\n",
    "else:\n",
    "    print(f\"  ‚Ä¢ Increasing w will reduce loss (gradient is negative)\")\n",
    "    \n",
    "if dL_db > 0:\n",
    "    print(f\"  ‚Ä¢ Decreasing b will reduce loss (gradient is positive)\")\n",
    "else:\n",
    "    print(f\"  ‚Ä¢ Increasing b will reduce loss (gradient is negative)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Data and initial model\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_data, y_data, alpha=0.5, label='Data')\n",
    "plt.plot(X_data, predict(X_data, w, b), 'r-', linewidth=2, label=f'Initial: y={w:.2f}x+{b:.2f}')\n",
    "plt.plot(X_data, predict(X_data, 2, 1), 'g--', linewidth=2, label='True: y=2x+1')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Linear Regression')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss surface and gradient\n",
    "plt.subplot(1, 2, 2)\n",
    "w_range = np.linspace(-1, 4, 50)\n",
    "b_range = np.linspace(-2, 3, 50)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "L = np.zeros_like(W)\n",
    "\n",
    "for i in range(len(w_range)):\n",
    "    for j in range(len(b_range)):\n",
    "        L[j, i] = mse_loss(X_data, y_data, W[j, i], B[j, i])\n",
    "\n",
    "contour = plt.contour(W, B, L, levels=20, cmap='viridis')\n",
    "plt.clabel(contour, inline=True, fontsize=8)\n",
    "plt.scatter([w], [b], color='red', s=200, marker='o', label='Current position', zorder=5)\n",
    "plt.arrow(w, b, -0.3*dL_dw, -0.3*dL_db, head_width=0.1, head_length=0.1, \n",
    "         fc='red', ec='red', linewidth=2, label='Negative gradient')\n",
    "plt.scatter([2], [1], color='yellow', s=200, marker='*', \n",
    "           edgecolors='black', linewidths=2, label='True optimum', zorder=5)\n",
    "plt.xlabel('w (weight)')\n",
    "plt.ylabel('b (bias)')\n",
    "plt.title('Loss Surface')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Logistic Regression Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression for binary classification\n",
    "# Model: p = sigmoid(wx + b)\n",
    "# Loss: L = -(1/n)Œ£[y¬∑log(p) + (1-y)¬∑log(1-p)]  (Binary Cross-Entropy)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X_pos = np.random.randn(50, 2) + np.array([2, 2])\n",
    "X_neg = np.random.randn(50, 2) + np.array([-2, -2])\n",
    "X = np.vstack([X_pos, X_neg])\n",
    "y = np.hstack([np.ones(50), np.zeros(50)])\n",
    "\n",
    "def predict_proba(X, w, b):\n",
    "    \"\"\"Predict probabilities\"\"\"\n",
    "    z = X @ w + b\n",
    "    return sigmoid(z)\n",
    "\n",
    "def binary_cross_entropy(X, y, w, b):\n",
    "    \"\"\"Binary cross-entropy loss\"\"\"\n",
    "    p = predict_proba(X, w, b)\n",
    "    epsilon = 1e-15  # Avoid log(0)\n",
    "    p = np.clip(p, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "\n",
    "def compute_gradients_logistic(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute gradients for logistic regression\n",
    "    \n",
    "    ‚àÇL/‚àÇw = (1/n)Œ£(p - y)¬∑x\n",
    "    ‚àÇL/‚àÇb = (1/n)Œ£(p - y)\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    p = predict_proba(X, w, b)\n",
    "    error = p - y\n",
    "    \n",
    "    dL_dw = (1 / n) * (X.T @ error)\n",
    "    dL_db = (1 / n) * np.sum(error)\n",
    "    \n",
    "    return dL_dw, dL_db\n",
    "\n",
    "# Initialize parameters\n",
    "w = np.zeros(2)\n",
    "b = 0.0\n",
    "\n",
    "# Compute gradients\n",
    "dL_dw, dL_db = compute_gradients_logistic(X, y, w, b)\n",
    "\n",
    "print(\"=== LOGISTIC REGRESSION GRADIENTS ===\")\n",
    "print(f\"Initial parameters: w={w}, b={b:.4f}\")\n",
    "print(f\"Initial loss: {binary_cross_entropy(X, y, w, b):.4f}\")\n",
    "print(f\"\\nGradients:\")\n",
    "print(f\"  ‚àÇL/‚àÇw = {dL_dw}\")\n",
    "print(f\"  ‚àÇL/‚àÇb = {dL_db:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_pos[:, 0], X_pos[:, 1], c='blue', label='Class 1', alpha=0.6)\n",
    "plt.scatter(X_neg[:, 0], X_neg[:, 1], c='red', label='Class 0', alpha=0.6)\n",
    "\n",
    "# Decision boundary (wx + b = 0)\n",
    "if w[1] != 0:\n",
    "    x_boundary = np.linspace(-5, 5, 100)\n",
    "    y_boundary = -(w[0] * x_boundary + b) / w[1]\n",
    "    plt.plot(x_boundary, y_boundary, 'k-', linewidth=2, label='Decision boundary')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Binary Classification Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_pos[:, 0], X_pos[:, 1], c='blue', alpha=0.3)\n",
    "plt.scatter(X_neg[:, 0], X_neg[:, 1], c='red', alpha=0.3)\n",
    "plt.arrow(0, 0, -2*dL_dw[0], -2*dL_dw[1], head_width=0.3, head_length=0.3,\n",
    "         fc='green', ec='green', linewidth=3, label='Negative gradient')\n",
    "plt.xlabel('w1 direction')\n",
    "plt.ylabel('w2 direction')\n",
    "plt.title('Gradient Direction')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Neural Network: Single Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple neural network with one hidden layer\n",
    "# Forward: h = sigmoid(W1¬∑x + b1), y = sigmoid(W2¬∑h + b2)\n",
    "\n",
    "def initialize_network(input_size, hidden_size, output_size):\n",
    "    \"\"\"Initialize network parameters\"\"\"\n",
    "    np.random.seed(42)\n",
    "    params = {\n",
    "        'W1': np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        'b1': np.zeros(hidden_size),\n",
    "        'W2': np.random.randn(output_size, hidden_size) * 0.01,\n",
    "        'b2': np.zeros(output_size)\n",
    "    }\n",
    "    return params\n",
    "\n",
    "def forward_pass(X, params):\n",
    "    \"\"\"Forward propagation\"\"\"\n",
    "    # Layer 1\n",
    "    z1 = params['W1'] @ X + params['b1'].reshape(-1, 1)\n",
    "    h = sigmoid(z1)\n",
    "    \n",
    "    # Layer 2\n",
    "    z2 = params['W2'] @ h + params['b2'].reshape(-1, 1)\n",
    "    y_pred = sigmoid(z2)\n",
    "    \n",
    "    cache = {'X': X, 'z1': z1, 'h': h, 'z2': z2, 'y_pred': y_pred}\n",
    "    return y_pred, cache\n",
    "\n",
    "def compute_loss(y_pred, y_true):\n",
    "    \"\"\"Binary cross-entropy loss\"\"\"\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def backward_pass(y_true, cache, params):\n",
    "    \"\"\"\n",
    "    Backward propagation - compute gradients\n",
    "    \n",
    "    Uses chain rule to compute gradients layer by layer\n",
    "    \"\"\"\n",
    "    m = y_true.shape[1]  # number of examples\n",
    "    \n",
    "    # Output layer gradients\n",
    "    dz2 = cache['y_pred'] - y_true  # ‚àÇL/‚àÇz2\n",
    "    dW2 = (1/m) * (dz2 @ cache['h'].T)  # ‚àÇL/‚àÇW2\n",
    "    db2 = (1/m) * np.sum(dz2, axis=1, keepdims=True)  # ‚àÇL/‚àÇb2\n",
    "    \n",
    "    # Hidden layer gradients (using chain rule)\n",
    "    dh = params['W2'].T @ dz2  # ‚àÇL/‚àÇh\n",
    "    dz1 = dh * cache['h'] * (1 - cache['h'])  # ‚àÇL/‚àÇz1 (sigmoid derivative)\n",
    "    dW1 = (1/m) * (dz1 @ cache['X'].T)  # ‚àÇL/‚àÇW1\n",
    "    db1 = (1/m) * np.sum(dz1, axis=1, keepdims=True)  # ‚àÇL/‚àÇb1\n",
    "    \n",
    "    gradients = {\n",
    "        'dW1': dW1,\n",
    "        'db1': db1.flatten(),\n",
    "        'dW2': dW2,\n",
    "        'db2': db2.flatten()\n",
    "    }\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Example with XOR problem (classic non-linear problem)\n",
    "X_train = np.array([[0, 0, 1, 1],\n",
    "                    [0, 1, 0, 1]])\n",
    "y_train = np.array([[0, 1, 1, 0]])  # XOR outputs\n",
    "\n",
    "# Initialize network\n",
    "params = initialize_network(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "# Forward pass\n",
    "y_pred, cache = forward_pass(X_train, params)\n",
    "loss = compute_loss(y_pred, y_train)\n",
    "\n",
    "# Backward pass\n",
    "grads = backward_pass(y_train, cache, params)\n",
    "\n",
    "print(\"=== NEURAL NETWORK GRADIENTS (XOR Problem) ===\")\n",
    "print(f\"\\nInput shape: {X_train.shape}\")\n",
    "print(f\"Hidden size: 4\")\n",
    "print(f\"Output shape: {y_train.shape}\")\n",
    "print(f\"\\nInitial loss: {loss:.4f}\")\n",
    "print(f\"\\nPredictions: {y_pred.flatten()}\")\n",
    "print(f\"True labels:  {y_train.flatten()}\")\n",
    "print(f\"\\nGradient shapes:\")\n",
    "print(f\"  dW1: {grads['dW1'].shape} (hidden √ó input)\")\n",
    "print(f\"  db1: {grads['db1'].shape} (hidden,)\")\n",
    "print(f\"  dW2: {grads['dW2'].shape} (output √ó hidden)\")\n",
    "print(f\"  db2: {grads['db2'].shape} (output,)\")\n",
    "print(f\"\\nSample gradient values:\")\n",
    "print(f\"  dW1[0,0] = {grads['dW1'][0,0]:.6f}\")\n",
    "print(f\"  dW2[0,0] = {grads['dW2'][0,0]:.6f}\")\n",
    "print(f\"\\n‚úì These gradients tell us how to adjust each weight to reduce loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Compute Derivatives\n",
    "Find the derivatives of the following functions:\n",
    "1. $f(x) = 3x^4 - 2x^2 + 5$\n",
    "2. $f(x) = e^{2x}$\n",
    "3. $f(x) = \\frac{1}{x^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - Your code here\n",
    "\n",
    "# Define functions and their derivatives\n",
    "# Verify numerically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Partial Derivatives\n",
    "For $f(x, y) = x^2y + e^{xy}$, compute:\n",
    "1. $\\frac{\\partial f}{\\partial x}$\n",
    "2. $\\frac{\\partial f}{\\partial y}$\n",
    "3. Evaluate both at point $(1, 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 - Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Gradient Computation\n",
    "Compute the gradient of $f(x, y, z) = x^2 + 2y^2 + 3z^2$ at point $(1, 1, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 - Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: MSE Gradient\n",
    "Derive and implement the gradient of MSE loss for linear regression:\n",
    "$L(w, b) = \\frac{1}{n}\\sum_{i=1}^{n}(wx_i + b - y_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 - Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SOLUTIONS ===\")\n",
    "\n",
    "# Exercise 1\n",
    "print(\"\\nExercise 1:\")\n",
    "print(\"1. f(x) = 3x‚Å¥ - 2x¬≤ + 5\")\n",
    "print(\"   f'(x) = 12x¬≥ - 4x\")\n",
    "print(\"\\n2. f(x) = e^(2x)\")\n",
    "print(\"   f'(x) = 2e^(2x)\")\n",
    "print(\"\\n3. f(x) = 1/x¬≤ = x^(-2)\")\n",
    "print(\"   f'(x) = -2x^(-3) = -2/x¬≥\")\n",
    "\n",
    "# Exercise 2\n",
    "print(\"\\nExercise 2: f(x,y) = x¬≤y + e^(xy)\")\n",
    "print(\"‚àÇf/‚àÇx = 2xy + ye^(xy)\")\n",
    "print(\"‚àÇf/‚àÇy = x¬≤ + xe^(xy)\")\n",
    "x, y = 1, 2\n",
    "df_dx = 2*x*y + y*np.exp(x*y)\n",
    "df_dy = x**2 + x*np.exp(x*y)\n",
    "print(f\"At (1,2): ‚àÇf/‚àÇx = {df_dx:.4f}, ‚àÇf/‚àÇy = {df_dy:.4f}\")\n",
    "\n",
    "# Exercise 3\n",
    "print(\"\\nExercise 3: f(x,y,z) = x¬≤ + 2y¬≤ + 3z¬≤\")\n",
    "print(\"‚àáf = [2x, 4y, 6z]\")\n",
    "grad = np.array([2*1, 4*1, 6*1])\n",
    "print(f\"At (1,1,1): ‚àáf = {grad}\")\n",
    "\n",
    "# Exercise 4\n",
    "print(\"\\nExercise 4: MSE Loss Gradient\")\n",
    "print(\"‚àÇL/‚àÇw = (2/n)Œ£(wx+b-y)¬∑x\")\n",
    "print(\"‚àÇL/‚àÇb = (2/n)Œ£(wx+b-y)\")\n",
    "print(\"(See implementation in Section 5.1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Key Takeaways\n",
    "\n",
    "### Core Concepts:\n",
    "1. ‚úÖ **Derivative** = rate of change = slope of tangent line\n",
    "2. ‚úÖ **Partial derivative** = derivative with respect to one variable (others constant)\n",
    "3. ‚úÖ **Gradient** = vector of all partial derivatives\n",
    "4. ‚úÖ **Chain rule** = foundation of backpropagation (next notebook!)\n",
    "\n",
    "### Essential Rules:\n",
    "- Power rule: $\\frac{d}{dx}[x^n] = nx^{n-1}$\n",
    "- Product rule: $(f \\cdot g)' = f'g + fg'$\n",
    "- Chain rule: $(f(g(x)))' = f'(g(x)) \\cdot g'(x)$\n",
    "\n",
    "### ML Applications:\n",
    "1. **Gradient descent**: Follow negative gradient to minimize loss\n",
    "2. **Backpropagation**: Compute gradients efficiently using chain rule\n",
    "3. **Optimization**: Adjust parameters to reduce error\n",
    "4. **Feature importance**: Gradient magnitude shows sensitivity\n",
    "\n",
    "### Computational Tips:\n",
    "- Use numerical differentiation for verification\n",
    "- Analytical gradients are faster and more accurate\n",
    "- Modern frameworks (PyTorch, TensorFlow) compute gradients automatically\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You understand derivatives and gradients! üéâ**\n",
    "\n",
    "**Next: Gradient Descent and Optimization - where we USE these gradients!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
