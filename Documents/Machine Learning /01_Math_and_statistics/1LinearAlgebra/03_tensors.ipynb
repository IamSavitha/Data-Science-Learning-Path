{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Guide to Tensor Operations\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "1. What tensors are and why they matter in deep learning\n",
    "2. Creating and manipulating tensors in NumPy\n",
    "3. Tensor operations: reshaping, broadcasting, slicing\n",
    "4. Advanced operations for deep learning\n",
    "5. Real-world applications in neural networks and computer vision\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a Tensor?\n",
    "\n",
    "### The Hierarchy of Data Structures\n",
    "\n",
    "Think of tensors as a generalization:\n",
    "- **Scalar** (0D tensor): A single number â†’ `5`\n",
    "- **Vector** (1D tensor): An array of numbers â†’ `[1, 2, 3]`\n",
    "- **Matrix** (2D tensor): A grid of numbers â†’ `[[1, 2], [3, 4]]`\n",
    "- **Tensor** (3D+ tensor): Multi-dimensional arrays â†’ `[[[...]]]`\n",
    "\n",
    "### Real-World Examples:\n",
    "- **3D Tensor:** Video frame (height Ã— width Ã— RGB channels)\n",
    "- **4D Tensor:** Batch of images (batch_size Ã— height Ã— width Ã— channels)\n",
    "- **5D Tensor:** Video batch (batch Ã— frames Ã— height Ã— width Ã— channels)\n",
    "\n",
    "### Why Tensors Matter in ML:\n",
    "Deep learning frameworks (PyTorch, TensorFlow) operate on tensors. Understanding tensor operations is **essential** for building neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Demonstrate the hierarchy\n",
    "scalar = 5\n",
    "vector = np.array([1, 2, 3, 4])\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "tensor_3d = np.array([[[1, 2], [3, 4]],\n",
    "                      [[5, 6], [7, 8]]])\n",
    "\n",
    "print(\"Scalar (0D):\")\n",
    "print(f\"Value: {scalar}\")\n",
    "print(f\"Shape: () - no dimensions\\n\")\n",
    "\n",
    "print(\"Vector (1D):\")\n",
    "print(f\"Value: {vector}\")\n",
    "print(f\"Shape: {vector.shape} - 1 dimension with 4 elements\\n\")\n",
    "\n",
    "print(\"Matrix (2D):\")\n",
    "print(matrix)\n",
    "print(f\"Shape: {matrix.shape} - 2 rows Ã— 3 columns\\n\")\n",
    "\n",
    "print(\"3D Tensor:\")\n",
    "print(tensor_3d)\n",
    "print(f\"Shape: {tensor_3d.shape} - 2 Ã— 2 Ã— 2\\n\")\n",
    "\n",
    "print(f\"Number of dimensions (ndim):\")\n",
    "print(f\"Scalar: {np.ndim(scalar)}D\")\n",
    "print(f\"Vector: {vector.ndim}D\")\n",
    "print(f\"Matrix: {matrix.ndim}D\")\n",
    "print(f\"3D Tensor: {tensor_3d.ndim}D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Creating Tensors\n",
    "\n",
    "### Various Ways to Create Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: From lists\n",
    "tensor_from_list = np.array([[[1, 2], [3, 4]], \n",
    "                             [[5, 6], [7, 8]]])\n",
    "\n",
    "# Method 2: Zeros, ones, full\n",
    "zeros_3d = np.zeros((2, 3, 4))  # 2Ã—3Ã—4 tensor of zeros\n",
    "ones_3d = np.ones((3, 2, 2))    # 3Ã—2Ã—2 tensor of ones\n",
    "full_3d = np.full((2, 2, 3), 7) # 2Ã—2Ã—3 tensor filled with 7\n",
    "\n",
    "# Method 3: Random tensors\n",
    "random_3d = np.random.rand(2, 3, 4)        # Uniform [0, 1)\n",
    "randn_3d = np.random.randn(2, 3, 4)        # Normal distribution\n",
    "randint_3d = np.random.randint(0, 10, (2, 3, 4))  # Random integers\n",
    "\n",
    "# Method 4: Using arange and reshape\n",
    "sequential = np.arange(24).reshape(2, 3, 4)\n",
    "\n",
    "print(\"Zeros tensor (2Ã—3Ã—4):\")\n",
    "print(zeros_3d)\n",
    "print(f\"Shape: {zeros_3d.shape}\\n\")\n",
    "\n",
    "print(\"Sequential tensor (2Ã—3Ã—4):\")\n",
    "print(sequential)\n",
    "print(f\"Shape: {sequential.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Tensor Shape\n",
    "\n",
    "For a tensor with shape `(d1, d2, d3, ...)`:\n",
    "- **d1**: Number of \"blocks\" in the outermost dimension\n",
    "- **d2**: Rows in each block\n",
    "- **d3**: Columns in each row\n",
    "- And so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D tensor to understand shape\n",
    "tensor = np.arange(24).reshape(2, 3, 4)\n",
    "\n",
    "print(\"3D Tensor with shape (2, 3, 4):\")\n",
    "print(tensor)\n",
    "print(f\"\\nShape breakdown:\")\n",
    "print(f\"- Dimension 0 (depth): {tensor.shape[0]} blocks\")\n",
    "print(f\"- Dimension 1 (rows): {tensor.shape[1]} rows per block\")\n",
    "print(f\"- Dimension 2 (cols): {tensor.shape[2]} columns per row\")\n",
    "print(f\"\\nTotal elements: {tensor.size} = {2*3*4}\")\n",
    "\n",
    "# Access elements\n",
    "print(f\"\\nFirst block [0, :, :]:\")\n",
    "print(tensor[0, :, :])\n",
    "print(f\"\\nFirst row of first block [0, 0, :]:\")\n",
    "print(tensor[0, 0, :])\n",
    "print(f\"\\nSingle element [0, 0, 0]: {tensor[0, 0, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Tensor Operations\n",
    "\n",
    "### 3.1 Reshaping Tensors\n",
    "Changing the shape while keeping the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original 1D array\n",
    "original = np.arange(24)\n",
    "print(\"Original 1D array (24 elements):\")\n",
    "print(original)\n",
    "print(f\"Shape: {original.shape}\\n\")\n",
    "\n",
    "# Reshape to different dimensions\n",
    "reshaped_2d = original.reshape(4, 6)\n",
    "reshaped_3d = original.reshape(2, 3, 4)\n",
    "reshaped_4d = original.reshape(2, 2, 3, 2)\n",
    "\n",
    "print(\"Reshaped to 2D (4Ã—6):\")\n",
    "print(reshaped_2d)\n",
    "print(f\"Shape: {reshaped_2d.shape}\\n\")\n",
    "\n",
    "print(\"Reshaped to 3D (2Ã—3Ã—4):\")\n",
    "print(reshaped_3d)\n",
    "print(f\"Shape: {reshaped_3d.shape}\\n\")\n",
    "\n",
    "print(\"Reshaped to 4D (2Ã—2Ã—3Ã—2):\")\n",
    "print(reshaped_4d)\n",
    "print(f\"Shape: {reshaped_4d.shape}\\n\")\n",
    "\n",
    "# Using -1 to infer dimension\n",
    "auto_reshape = original.reshape(4, -1)  # Auto-calculate: 24/4 = 6\n",
    "print(\"Auto-reshape (4, -1) â†’ (4, 6):\")\n",
    "print(auto_reshape)\n",
    "print(f\"Shape: {auto_reshape.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Flattening Tensors\n",
    "Converting multi-dimensional tensors to 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_3d = np.arange(24).reshape(2, 3, 4)\n",
    "\n",
    "print(\"Original 3D tensor (2Ã—3Ã—4):\")\n",
    "print(tensor_3d)\n",
    "\n",
    "# Method 1: flatten (creates copy)\n",
    "flattened = tensor_3d.flatten()\n",
    "print(\"\\nFlattened:\")\n",
    "print(flattened)\n",
    "\n",
    "# Method 2: ravel (returns view if possible)\n",
    "raveled = tensor_3d.ravel()\n",
    "print(\"\\nRaveled:\")\n",
    "print(raveled)\n",
    "\n",
    "# Method 3: reshape to -1\n",
    "reshaped_flat = tensor_3d.reshape(-1)\n",
    "print(\"\\nReshaped to -1:\")\n",
    "print(reshaped_flat)\n",
    "\n",
    "print(f\"\\nAll produce same 1D array with {len(flattened)} elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Transpose and Axis Permutation\n",
    "Rearranging dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D tensor\n",
    "tensor = np.arange(24).reshape(2, 3, 4)\n",
    "print(\"Original tensor shape:\", tensor.shape, \"(2Ã—3Ã—4)\")\n",
    "print(tensor)\n",
    "\n",
    "# Transpose (reverse all axes)\n",
    "transposed = tensor.T\n",
    "print(\"\\nTransposed shape:\", transposed.shape, \"(4Ã—3Ã—2)\")\n",
    "print(transposed)\n",
    "\n",
    "# Custom axis permutation\n",
    "# (0, 1, 2) â†’ (1, 2, 0)\n",
    "permuted = np.transpose(tensor, (1, 2, 0))\n",
    "print(\"\\nPermuted (1,2,0) shape:\", permuted.shape, \"(3Ã—4Ã—2)\")\n",
    "print(permuted)\n",
    "\n",
    "# Common in image processing: (H, W, C) â†’ (C, H, W)\n",
    "image = np.random.rand(224, 224, 3)  # Height Ã— Width Ã— Channels\n",
    "image_channels_first = np.transpose(image, (2, 0, 1))  # Channels Ã— Height Ã— Width\n",
    "\n",
    "print(f\"\\nImage format conversion:\")\n",
    "print(f\"Original (H,W,C): {image.shape}\")\n",
    "print(f\"Channels first (C,H,W): {image_channels_first.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Expanding and Squeezing Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original array\n",
    "arr = np.array([1, 2, 3, 4])\n",
    "print(\"Original array:\")\n",
    "print(arr)\n",
    "print(f\"Shape: {arr.shape}\\n\")\n",
    "\n",
    "# Add dimension at axis 0 (add batch dimension)\n",
    "expanded_0 = np.expand_dims(arr, axis=0)\n",
    "print(\"Expanded at axis 0:\")\n",
    "print(expanded_0)\n",
    "print(f\"Shape: {expanded_0.shape}\\n\")\n",
    "\n",
    "# Add dimension at axis 1\n",
    "expanded_1 = np.expand_dims(arr, axis=1)\n",
    "print(\"Expanded at axis 1:\")\n",
    "print(expanded_1)\n",
    "print(f\"Shape: {expanded_1.shape}\\n\")\n",
    "\n",
    "# Alternative: using None or np.newaxis\n",
    "expanded_none = arr[np.newaxis, :]\n",
    "print(\"Using np.newaxis:\")\n",
    "print(expanded_none)\n",
    "print(f\"Shape: {expanded_none.shape}\\n\")\n",
    "\n",
    "# Squeeze: remove dimensions of size 1\n",
    "squeezed = np.squeeze(expanded_0)\n",
    "print(\"After squeezing:\")\n",
    "print(squeezed)\n",
    "print(f\"Shape: {squeezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Broadcasting\n",
    "\n",
    "### The Magic of Broadcasting\n",
    "NumPy can perform operations on arrays of different shapes by automatically \"stretching\" smaller arrays.\n",
    "\n",
    "**Rules:**\n",
    "1. If arrays have different number of dimensions, pad smaller shape with 1s on the left\n",
    "2. Arrays are compatible if dimensions are equal or one of them is 1\n",
    "3. After broadcasting, arrays behave as if they have the same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Add scalar to array (most common)\n",
    "arr = np.array([1, 2, 3, 4])\n",
    "result = arr + 10\n",
    "\n",
    "print(\"Broadcasting scalar:\")\n",
    "print(f\"{arr} + 10 = {result}\\n\")\n",
    "\n",
    "# Example 2: Add 1D array to 2D array\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])\n",
    "row = np.array([10, 20, 30])\n",
    "\n",
    "result = matrix + row\n",
    "\n",
    "print(\"Broadcasting 1D to 2D:\")\n",
    "print(\"Matrix:\")\n",
    "print(matrix)\n",
    "print(\"\\nAdd row:\", row)\n",
    "print(\"\\nResult:\")\n",
    "print(result)\n",
    "print(\"\\n(Row is broadcast to each row of matrix)\\n\")\n",
    "\n",
    "# Example 3: Column vector + row vector\n",
    "col = np.array([[1], [2], [3]])  # 3Ã—1\n",
    "row = np.array([10, 20, 30])     # 1Ã—3\n",
    "\n",
    "result = col + row  # Results in 3Ã—3\n",
    "\n",
    "print(\"Broadcasting column + row:\")\n",
    "print(\"Column (3Ã—1):\")\n",
    "print(col)\n",
    "print(\"\\nRow (3,):\")\n",
    "print(row)\n",
    "print(\"\\nResult (3Ã—3):\")\n",
    "print(result)\n",
    "print(\"\\nEach element of column is added to entire row!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize broadcasting\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "row = np.array([10, 20, 30])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original matrix\n",
    "axes[0].imshow(matrix, cmap='Blues', aspect='auto')\n",
    "axes[0].set_title('Original Matrix (2Ã—3)')\n",
    "for i in range(matrix.shape[0]):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        axes[0].text(j, i, str(matrix[i, j]), ha='center', va='center', fontsize=14)\n",
    "\n",
    "# Row to broadcast\n",
    "axes[1].imshow(row.reshape(1, -1), cmap='Greens', aspect='auto')\n",
    "axes[1].set_title('Row to Broadcast (1Ã—3)')\n",
    "for j in range(len(row)):\n",
    "    axes[1].text(j, 0, str(row[j]), ha='center', va='center', fontsize=14)\n",
    "\n",
    "# Result\n",
    "result = matrix + row\n",
    "axes[2].imshow(result, cmap='Reds', aspect='auto')\n",
    "axes[2].set_title('Result (2Ã—3)')\n",
    "for i in range(result.shape[0]):\n",
    "    for j in range(result.shape[1]):\n",
    "        axes[2].text(j, i, str(result[i, j]), ha='center', va='center', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Advanced Tensor Operations\n",
    "\n",
    "### 5.1 Concatenation and Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample tensors\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "print(\"Tensor A:\")\n",
    "print(a)\n",
    "print(\"\\nTensor B:\")\n",
    "print(b)\n",
    "\n",
    "# Concatenate along axis 0 (vertically)\n",
    "concat_0 = np.concatenate([a, b], axis=0)\n",
    "print(\"\\nConcatenate axis=0 (vertical):\")\n",
    "print(concat_0)\n",
    "print(f\"Shape: {concat_0.shape}\")\n",
    "\n",
    "# Concatenate along axis 1 (horizontally)\n",
    "concat_1 = np.concatenate([a, b], axis=1)\n",
    "print(\"\\nConcatenate axis=1 (horizontal):\")\n",
    "print(concat_1)\n",
    "print(f\"Shape: {concat_1.shape}\")\n",
    "\n",
    "# Stack: creates new dimension\n",
    "stacked = np.stack([a, b], axis=0)\n",
    "print(\"\\nStacked (new dimension):\")\n",
    "print(stacked)\n",
    "print(f\"Shape: {stacked.shape}\")\n",
    "\n",
    "# Vertical and horizontal stack (convenience functions)\n",
    "vstack = np.vstack([a, b])  # Same as concatenate axis=0\n",
    "hstack = np.hstack([a, b])  # Same as concatenate axis=1\n",
    "\n",
    "print(\"\\nvstack:\")\n",
    "print(vstack)\n",
    "print(\"\\nhstack:\")\n",
    "print(hstack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Splitting Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor to split\n",
    "tensor = np.arange(12).reshape(4, 3)\n",
    "\n",
    "print(\"Original tensor:\")\n",
    "print(tensor)\n",
    "print(f\"Shape: {tensor.shape}\\n\")\n",
    "\n",
    "# Split into 2 parts along axis 0\n",
    "split_0 = np.split(tensor, 2, axis=0)\n",
    "print(\"Split into 2 along axis 0:\")\n",
    "for i, part in enumerate(split_0):\n",
    "    print(f\"Part {i}:\")\n",
    "    print(part)\n",
    "    print()\n",
    "\n",
    "# Split at specific indices\n",
    "split_indices = np.split(tensor, [1, 3], axis=0)\n",
    "print(\"Split at indices [1, 3] along axis 0:\")\n",
    "for i, part in enumerate(split_indices):\n",
    "    print(f\"Part {i}:\")\n",
    "    print(part)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Reduction Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D tensor\n",
    "tensor = np.arange(24).reshape(2, 3, 4)\n",
    "\n",
    "print(\"3D Tensor (2Ã—3Ã—4):\")\n",
    "print(tensor)\n",
    "print(f\"Shape: {tensor.shape}\\n\")\n",
    "\n",
    "# Sum operations\n",
    "sum_all = np.sum(tensor)  # Sum all elements\n",
    "sum_axis0 = np.sum(tensor, axis=0)  # Sum along axis 0\n",
    "sum_axis1 = np.sum(tensor, axis=1)  # Sum along axis 1\n",
    "sum_axis2 = np.sum(tensor, axis=2)  # Sum along axis 2\n",
    "\n",
    "print(\"Sum all elements:\", sum_all)\n",
    "print(\"\\nSum along axis 0 (shape becomes 3Ã—4):\")\n",
    "print(sum_axis0)\n",
    "print(\"\\nSum along axis 1 (shape becomes 2Ã—4):\")\n",
    "print(sum_axis1)\n",
    "print(\"\\nSum along axis 2 (shape becomes 2Ã—3):\")\n",
    "print(sum_axis2)\n",
    "\n",
    "# Other reduction operations\n",
    "print(\"\\nOther reductions:\")\n",
    "print(\"Mean:\", np.mean(tensor))\n",
    "print(\"Max:\", np.max(tensor))\n",
    "print(\"Min:\", np.min(tensor))\n",
    "print(\"Std:\", np.std(tensor))\n",
    "\n",
    "# Keepdims: preserve dimension\n",
    "sum_keepdims = np.sum(tensor, axis=1, keepdims=True)\n",
    "print(f\"\\nSum axis=1 with keepdims:\")\n",
    "print(f\"Shape: {sum_keepdims.shape} (2Ã—1Ã—4)\")\n",
    "print(sum_keepdims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Real-World ML Applications\n",
    "\n",
    "### 6.1 Batch Processing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a batch of images\n",
    "# Shape: (batch_size, height, width, channels)\n",
    "batch_size = 32\n",
    "height, width = 224, 224\n",
    "channels = 3  # RGB\n",
    "\n",
    "images = np.random.rand(batch_size, height, width, channels)\n",
    "\n",
    "print(\"Batch of images:\")\n",
    "print(f\"Shape: {images.shape}\")\n",
    "print(f\"Interpretation: {batch_size} images of {height}Ã—{width} with {channels} channels\\n\")\n",
    "\n",
    "# Access single image\n",
    "first_image = images[0]\n",
    "print(f\"First image shape: {first_image.shape}\")\n",
    "\n",
    "# Process: Normalize batch (subtract mean, divide by std)\n",
    "mean = np.mean(images, axis=(1, 2), keepdims=True)\n",
    "std = np.std(images, axis=(1, 2), keepdims=True)\n",
    "normalized = (images - mean) / (std + 1e-7)\n",
    "\n",
    "print(f\"\\nNormalized batch shape: {normalized.shape}\")\n",
    "print(f\"Mean of normalized batch: {np.mean(normalized):.6f} (should be ~0)\")\n",
    "print(f\"Std of normalized batch: {np.std(normalized):.6f} (should be ~1)\")\n",
    "\n",
    "# Convert to channels-first (common in PyTorch)\n",
    "images_channels_first = np.transpose(images, (0, 3, 1, 2))\n",
    "print(f\"\\nChannels-first format: {images_channels_first.shape}\")\n",
    "print(f\"(batch, channels, height, width)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Convolution Operation (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2D convolution demonstration\n",
    "# Input: small image\n",
    "image = np.array([[1, 2, 3, 4],\n",
    "                  [5, 6, 7, 8],\n",
    "                  [9, 10, 11, 12],\n",
    "                  [13, 14, 15, 16]], dtype=float)\n",
    "\n",
    "# Filter/Kernel (edge detector)\n",
    "kernel = np.array([[-1, -1, -1],\n",
    "                   [0, 0, 0],\n",
    "                   [1, 1, 1]])\n",
    "\n",
    "print(\"Input Image (4Ã—4):\")\n",
    "print(image)\n",
    "print(\"\\nKernel (3Ã—3):\")\n",
    "print(kernel)\n",
    "\n",
    "# Manual convolution for understanding\n",
    "output_height = image.shape[0] - kernel.shape[0] + 1\n",
    "output_width = image.shape[1] - kernel.shape[1] + 1\n",
    "output = np.zeros((output_height, output_width))\n",
    "\n",
    "for i in range(output_height):\n",
    "    for j in range(output_width):\n",
    "        # Extract patch\n",
    "        patch = image[i:i+3, j:j+3]\n",
    "        # Element-wise multiply and sum\n",
    "        output[i, j] = np.sum(patch * kernel)\n",
    "\n",
    "print(\"\\nOutput after convolution (2Ã—2):\")\n",
    "print(output)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].imshow(image, cmap='gray')\n",
    "axes[0].set_title('Input Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(kernel, cmap='coolwarm')\n",
    "axes[1].set_title('Kernel (Edge Detector)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(output, cmap='gray')\n",
    "axes[2].set_title('Output Feature Map')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Attention Mechanism (Simplified)\n",
    "Core operation in Transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified attention mechanism\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Softmax function\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "# Query, Key, Value matrices\n",
    "seq_length = 4\n",
    "d_model = 8\n",
    "\n",
    "Q = np.random.randn(seq_length, d_model)  # Queries\n",
    "K = np.random.randn(seq_length, d_model)  # Keys\n",
    "V = np.random.randn(seq_length, d_model)  # Values\n",
    "\n",
    "print(\"Attention Mechanism Components:\")\n",
    "print(f\"Q (Queries): {Q.shape}\")\n",
    "print(f\"K (Keys): {K.shape}\")\n",
    "print(f\"V (Values): {V.shape}\\n\")\n",
    "\n",
    "# Step 1: Calculate attention scores\n",
    "scores = Q @ K.T / np.sqrt(d_model)\n",
    "print(f\"Attention scores: {scores.shape}\")\n",
    "print(scores)\n",
    "\n",
    "# Step 2: Apply softmax to get attention weights\n",
    "attention_weights = softmax(scores, axis=-1)\n",
    "print(f\"\\nAttention weights (after softmax):\")\n",
    "print(attention_weights)\n",
    "print(f\"Sum of each row: {np.sum(attention_weights, axis=1)}\")\n",
    "\n",
    "# Step 3: Apply attention to values\n",
    "output = attention_weights @ V\n",
    "print(f\"\\nOutput: {output.shape}\")\n",
    "print(output)\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention_weights, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.title('Attention Weight Matrix')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Batch Matrix Multiplication (BMM)\n",
    "Essential for batched neural network operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch matrix multiplication\n",
    "# Multiply multiple matrix pairs at once\n",
    "\n",
    "batch_size = 3\n",
    "m, n, p = 2, 3, 4\n",
    "\n",
    "# Batch of matrices A (batch Ã— m Ã— n)\n",
    "A_batch = np.random.randn(batch_size, m, n)\n",
    "\n",
    "# Batch of matrices B (batch Ã— n Ã— p)\n",
    "B_batch = np.random.randn(batch_size, n, p)\n",
    "\n",
    "print(f\"A_batch shape: {A_batch.shape}\")\n",
    "print(f\"B_batch shape: {B_batch.shape}\\n\")\n",
    "\n",
    "# Batch matrix multiplication\n",
    "C_batch = A_batch @ B_batch\n",
    "\n",
    "print(f\"C_batch shape: {C_batch.shape}\")\n",
    "print(\"\\nThis multiplies:\")\n",
    "print(f\"- A_batch[0] @ B_batch[0]\")\n",
    "print(f\"- A_batch[1] @ B_batch[1]\")\n",
    "print(f\"- A_batch[2] @ B_batch[2]\")\n",
    "print(\"\\nAll at once (in parallel)!\")\n",
    "\n",
    "# Verify for first batch\n",
    "manual_result = A_batch[0] @ B_batch[0]\n",
    "print(\"\\nFirst batch result (manual):\")\n",
    "print(manual_result)\n",
    "print(\"\\nFirst batch result (BMM):\")\n",
    "print(C_batch[0])\n",
    "print(\"\\nAre they equal?\", np.allclose(manual_result, C_batch[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tensor Manipulation\n",
    "Create a tensor of shape (3, 4, 5) with sequential values 0-59.\n",
    "1. Reshape it to (5, 12)\n",
    "2. Transpose it\n",
    "3. Flatten it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - Your code here\n",
    "\n",
    "# Create tensor\n",
    "tensor = # YOUR CODE\n",
    "\n",
    "# 1. Reshape\n",
    "reshaped = # YOUR CODE\n",
    "\n",
    "# 2. Transpose\n",
    "transposed = # YOUR CODE\n",
    "\n",
    "# 3. Flatten\n",
    "flattened = # YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Broadcasting\n",
    "Given a matrix (3Ã—4) and a column vector (3Ã—1), add them using broadcasting.\n",
    "Then multiply the result by a row vector (4,) element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 - Your code here\n",
    "matrix = np.arange(12).reshape(3, 4)\n",
    "col_vector = np.array([[1], [2], [3]])\n",
    "row_vector = np.array([10, 20, 30, 40])\n",
    "\n",
    "# YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Image Batch Processing\n",
    "Create a batch of 16 grayscale images (28Ã—28).\n",
    "1. Calculate the mean pixel value for each image\n",
    "2. Normalize each image (subtract mean, divide by std)\n",
    "3. Reshape the batch to (16, 784) for a fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 - Your code here\n",
    "images = np.random.rand(16, 28, 28)\n",
    "\n",
    "# 1. Mean per image\n",
    "means = # YOUR CODE\n",
    "\n",
    "# 2. Normalize\n",
    "normalized = # YOUR CODE\n",
    "\n",
    "# 3. Reshape\n",
    "flattened_batch = # YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Mini Neural Network\n",
    "Implement a 2-layer neural network:\n",
    "- Input: batch of 8 samples with 10 features\n",
    "- Hidden layer: 20 neurons with ReLU\n",
    "- Output layer: 5 neurons with softmax\n",
    "\n",
    "Use random weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 - Your code here\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "# Input\n",
    "X = np.random.randn(8, 10)\n",
    "\n",
    "# YOUR CODE\n",
    "# Layer 1: X (8Ã—10) â†’ hidden (8Ã—20)\n",
    "# Layer 2: hidden (8Ã—20) â†’ output (8Ã—5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SOLUTIONS ===\")\n",
    "\n",
    "# Exercise 1\n",
    "print(\"\\nExercise 1:\")\n",
    "tensor = np.arange(60).reshape(3, 4, 5)\n",
    "print(\"Original shape:\", tensor.shape)\n",
    "reshaped = tensor.reshape(5, 12)\n",
    "print(\"Reshaped:\", reshaped.shape)\n",
    "transposed = reshaped.T\n",
    "print(\"Transposed:\", transposed.shape)\n",
    "flattened = tensor.flatten()\n",
    "print(\"Flattened:\", flattened.shape)\n",
    "\n",
    "# Exercise 2\n",
    "print(\"\\nExercise 2:\")\n",
    "matrix = np.arange(12).reshape(3, 4)\n",
    "col_vector = np.array([[1], [2], [3]])\n",
    "row_vector = np.array([10, 20, 30, 40])\n",
    "result = (matrix + col_vector) * row_vector\n",
    "print(\"Result shape:\", result.shape)\n",
    "print(result)\n",
    "\n",
    "# Exercise 3\n",
    "print(\"\\nExercise 3:\")\n",
    "images = np.random.rand(16, 28, 28)\n",
    "means = np.mean(images, axis=(1, 2), keepdims=True)\n",
    "stds = np.std(images, axis=(1, 2), keepdims=True)\n",
    "normalized = (images - means) / (stds + 1e-7)\n",
    "flattened_batch = normalized.reshape(16, -1)\n",
    "print(\"Flattened batch shape:\", flattened_batch.shape)\n",
    "\n",
    "# Exercise 4\n",
    "print(\"\\nExercise 4:\")\n",
    "X = np.random.randn(8, 10)\n",
    "W1 = np.random.randn(10, 20) * 0.01\n",
    "b1 = np.zeros(20)\n",
    "W2 = np.random.randn(20, 5) * 0.01\n",
    "b2 = np.zeros(5)\n",
    "\n",
    "hidden = relu(X @ W1 + b1)\n",
    "output = softmax(hidden @ W2 + b2, axis=1)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Sum of probabilities per sample:\", np.sum(output, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Key Takeaways\n",
    "\n",
    "### What You Learned:\n",
    "1. âœ… Tensors are multi-dimensional arrays (generalization of vectors and matrices)\n",
    "2. âœ… Shape manipulation: reshape, transpose, expand_dims, squeeze\n",
    "3. âœ… Broadcasting enables operations on different-shaped arrays\n",
    "4. âœ… Concatenation and stacking combine tensors\n",
    "5. âœ… Reduction operations (sum, mean) along specific axes\n",
    "6. âœ… Real ML applications: batched images, convolutions, attention\n",
    "\n",
    "### Critical Skills for Deep Learning:\n",
    "- **Batch processing:** Always think in batches (4D for images)\n",
    "- **Axis manipulation:** Understanding which axis to reduce/transpose\n",
    "- **Broadcasting:** Efficiently operate on different shapes\n",
    "- **Memory layout:** Channels-first vs channels-last\n",
    "- **Matrix multiplication:** Core of neural networks\n",
    "\n",
    "### Next Steps:\n",
    "1. Learn PyTorch/TensorFlow (builds on NumPy concepts)\n",
    "2. Study specific architectures (CNNs, Transformers)\n",
    "3. Practice with real datasets\n",
    "4. Understand GPU acceleration\n",
    "\n",
    "### Common Tensor Shapes in Deep Learning:\n",
    "- **Images:** (batch, height, width, channels) or (batch, channels, height, width)\n",
    "- **Text:** (batch, sequence_length, embedding_dim)\n",
    "- **Time series:** (batch, timesteps, features)\n",
    "- **Video:** (batch, frames, height, width, channels)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've mastered tensor operations! ðŸŽ‰**\n",
    "\n",
    "**You're now ready to dive into deep learning frameworks!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
