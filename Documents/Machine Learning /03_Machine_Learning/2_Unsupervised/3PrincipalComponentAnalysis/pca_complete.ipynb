{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) - Complete Guide\n",
    "\n",
    "## From Linear Algebra to Dimensionality Reduction\n",
    "\n",
    "PCA is one of the most important **dimensionality reduction techniques** in machine learning. It finds the directions of maximum variance in high-dimensional data and projects it onto a lower-dimensional subspace.\n",
    "\n",
    "### What You'll Learn\n",
    "1. What PCA is and why we need dimensionality reduction\n",
    "2. The mathematical foundation (covariance, eigenvalues, eigenvectors)\n",
    "3. Step-by-step PCA algorithm\n",
    "4. Implementation from scratch\n",
    "5. Scikit-learn implementation\n",
    "6. Choosing the number of components\n",
    "7. Visualization and interpretation\n",
    "8. Real-world applications\n",
    "9. Limitations and alternatives\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Principal Component Analysis (PCA) - Complete Guide\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Dimensionality Reduction?\n",
    "\n",
    "### The Curse of Dimensionality\n",
    "\n",
    "As the number of features increases, the volume of space increases exponentially. This causes:\n",
    "- **Data sparsity**: Points become very far apart\n",
    "- **Overfitting**: Models memorize noise\n",
    "- **Computational cost**: More features = slower training\n",
    "- **Visualization difficulty**: Can't visualize >3D\n",
    "\n",
    "### Why Reduce Dimensions?\n",
    "\n",
    "- ✅ **Visualization**: Project high-D data to 2D/3D\n",
    "- ✅ **Noise reduction**: Remove less informative dimensions\n",
    "- ✅ **Faster training**: Fewer features = faster models\n",
    "- ✅ **Prevent overfitting**: Reduce model complexity\n",
    "- ✅ **Feature extraction**: Create better features\n",
    "\n",
    "### Types of Dimensionality Reduction\n",
    "\n",
    "1. **Linear**: PCA, Factor Analysis\n",
    "2. **Non-linear**: t-SNE, UMAP, Autoencoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the concept of dimensionality reduction\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create 3D data\n",
    "n_samples = 100\n",
    "X_3d = np.random.randn(n_samples, 3)\n",
    "X_3d[:, 2] = 0.5 * X_3d[:, 0] + 0.5 * X_3d[:, 1] + 0.1 * np.random.randn(n_samples)\n",
    "\n",
    "# Apply PCA to reduce to 2D\n",
    "pca_vis = PCA(n_components=2)\n",
    "X_2d = pca_vis.fit_transform(X_3d)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original 3D data (projected to 2D views)\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], alpha=0.6, s=50)\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.set_zlabel('Feature 3')\n",
    "ax1.set_title('Original 3D Data', fontweight='bold')\n",
    "\n",
    "# 2D projection (first two features)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.scatter(X_3d[:, 0], X_3d[:, 1], alpha=0.6, s=50)\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "ax2.set_title('Simple 2D Projection\\n(Most information lost)', fontweight='bold', color='orange')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# PCA 2D projection\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.6, s=50)\n",
    "ax3.set_xlabel('PC1 (Principal Component 1)')\n",
    "ax3.set_ylabel('PC2 (Principal Component 2)')\n",
    "ax3.set_title('PCA 2D Projection\\n(Maximum variance preserved)', fontweight='bold', color='green')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "variance_explained = pca_vis.explained_variance_ratio_\n",
    "print(f\"Variance explained by PC1: {variance_explained[0]:.1%}\")\n",
    "print(f\"Variance explained by PC2: {variance_explained[1]:.1%}\")\n",
    "print(f\"Total variance preserved: {variance_explained.sum():.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is Principal Component Analysis?\n",
    "\n",
    "### Core Idea\n",
    "\n",
    "**PCA** finds the directions (principal components) in which the data varies the most. It projects data onto these directions to reduce dimensionality while preserving maximum variance.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Principal Components (PCs)**: New orthogonal directions in feature space\n",
    "2. **Variance**: Measure of data spread in each direction\n",
    "3. **Eigenvectors**: Directions of principal components\n",
    "4. **Eigenvalues**: Amount of variance along each component\n",
    "\n",
    "### Mathematical Goal\n",
    "\n",
    "Find a lower-dimensional representation that:\n",
    "- Maximizes variance in the projected space\n",
    "- Minimizes reconstruction error\n",
    "- Removes correlations between features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mathematical Foundation\n",
    "\n",
    "### Step-by-Step Mathematics\n",
    "\n",
    "**Step 1: Standardize the Data**\n",
    "\n",
    "Center the data (subtract mean):\n",
    "$$X_{centered} = X - \\bar{X}$$\n",
    "\n",
    "Scale the data (divide by std, if needed):\n",
    "$$X_{scaled} = \\frac{X_{centered}}{\\sigma}$$\n",
    "\n",
    "**Step 2: Compute Covariance Matrix**\n",
    "\n",
    "$$C = \\frac{1}{n-1} X^T X$$\n",
    "\n",
    "where $n$ is the number of samples.\n",
    "\n",
    "**Step 3: Eigenvalue Decomposition**\n",
    "\n",
    "Find eigenvalues $\\lambda_i$ and eigenvectors $v_i$ of $C$:\n",
    "$$C v_i = \\lambda_i v_i$$\n",
    "\n",
    "**Step 4: Select Principal Components**\n",
    "\n",
    "- Eigenvalues: Amount of variance along each direction\n",
    "- Eigenvectors: Directions of principal components\n",
    "- Sort by eigenvalues (largest first)\n",
    "\n",
    "**Step 5: Project Data**\n",
    "\n",
    "Transform data to new space:\n",
    "$$Y = X \\times V_k$$\n",
    "\n",
    "where $V_k$ contains the top $k$ eigenvectors (columns).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA mathematically - 2D example\n",
    "np.random.seed(42)\n",
    "mean = [0, 0]\n",
    "cov = [[3, 1.5], [1.5, 1]]  # Covariance matrix\n",
    "X_data = np.random.multivariate_normal(mean, cov, 200)\n",
    "\n",
    "# Center the data\n",
    "X_centered = X_data - X_data.mean(axis=0)\n",
    "\n",
    "# Compute covariance matrix\n",
    "cov_matrix = np.cov(X_centered.T)\n",
    "print(\"Covariance Matrix:\")\n",
    "print(cov_matrix)\n",
    "\n",
    "# Eigenvalue decomposition\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort by eigenvalue (descending)\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "print(f\"\\nEigenvalues (variances): {eigenvalues}\")\n",
    "print(f\"\\nEigenvector 1 (PC1): {eigenvectors[:, 0]}\")\n",
    "print(f\"Eigenvector 2 (PC2): {eigenvectors[:, 1]}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Original data with principal components\n",
    "ax = axes[0]\n",
    "ax.scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.6, s=50)\n",
    "\n",
    "# Plot principal components\n",
    "origin = X_centered.mean(axis=0)\n",
    "for i, (eigenval, eigenvec) in enumerate(zip(eigenvalues, eigenvectors.T)):\n",
    "    ax.arrow(origin[0], origin[1], eigenvec[0] * 3 * np.sqrt(eigenval), \n",
    "             eigenvec[1] * 3 * np.sqrt(eigenval),\n",
    "             head_width=0.3, head_length=0.2, fc=f'C{i}', ec=f'C{i}', \n",
    "             linewidth=2, label=f'PC{i+1} (λ={eigenval:.2f})')\n",
    "\n",
    "ax.set_xlabel('Feature 1', fontsize=12)\n",
    "ax.set_ylabel('Feature 2', fontsize=12)\n",
    "ax.set_title('Original Data with Principal Components', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Projected data\n",
    "X_projected = X_centered @ eigenvectors\n",
    "ax = axes[1]\n",
    "ax.scatter(X_projected[:, 0], X_projected[:, 1], alpha=0.6, s=50, c='green')\n",
    "ax.set_xlabel('PC1 (Principal Component 1)', fontsize=12)\n",
    "ax.set_ylabel('PC2 (Principal Component 2)', fontsize=12)\n",
    "ax.set_title('Projected Data (Rotated to PC space)', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "variance_ratio = eigenvalues / eigenvalues.sum()\n",
    "print(f\"\\nVariance explained by PC1: {variance_ratio[0]:.1%}\")\n",
    "print(f\"Variance explained by PC2: {variance_ratio[1]:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation from Scratch\n",
    "\n",
    "Let's implement PCA step-by-step to understand every detail!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAScratch:\n",
    "    \"\"\"Principal Component Analysis from Scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=None):\n",
    "        self.n_components = n_components\n",
    "        self.components_ = None\n",
    "        self.mean_ = None\n",
    "        self.explained_variance_ = None\n",
    "        self.explained_variance_ratio_ = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit PCA to data\"\"\"\n",
    "        # Step 1: Center the data\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean_\n",
    "        \n",
    "        # Step 2: Compute covariance matrix\n",
    "        n_samples = X.shape[0]\n",
    "        cov_matrix = np.dot(X_centered.T, X_centered) / (n_samples - 1)\n",
    "        \n",
    "        # Step 3: Eigenvalue decomposition\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "        \n",
    "        # Step 4: Sort by eigenvalue (descending)\n",
    "        idx = eigenvalues.argsort()[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "        \n",
    "        # Step 5: Select number of components\n",
    "        if self.n_components is None:\n",
    "            self.n_components = min(X.shape[0], X.shape[1])\n",
    "        \n",
    "        self.components_ = eigenvectors[:, :self.n_components].T\n",
    "        \n",
    "        # Store explained variance\n",
    "        self.explained_variance_ = eigenvalues[:self.n_components]\n",
    "        self.explained_variance_ratio_ = self.explained_variance_ / eigenvalues.sum()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data to principal component space\"\"\"\n",
    "        if self.components_ is None:\n",
    "            raise ValueError(\"Model must be fitted first\")\n",
    "        X_centered = X - self.mean_\n",
    "        return np.dot(X_centered, self.components_.T)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "# Test our implementation\n",
    "X_test = np.random.randn(100, 4)\n",
    "X_test[:, 3] = 0.5 * X_test[:, 0] + 0.5 * X_test[:, 1] + 0.1 * np.random.randn(100)\n",
    "\n",
    "pca_scratch = PCAScratch(n_components=2)\n",
    "X_transformed = pca_scratch.fit_transform(X_test)\n",
    "\n",
    "print(\"PCA from Scratch Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original shape: {X_test.shape}\")\n",
    "print(f\"Transformed shape: {X_transformed.shape}\")\n",
    "print(f\"\\nExplained variance ratio:\")\n",
    "print(f\"  PC1: {pca_scratch.explained_variance_ratio_[0]:.1%}\")\n",
    "print(f\"  PC2: {pca_scratch.explained_variance_ratio_[1]:.1%}\")\n",
    "print(f\"  Total: {pca_scratch.explained_variance_ratio_.sum():.1%}\")\n",
    "print(f\"\\nFirst Principal Component:\")\n",
    "print(f\"  {pca_scratch.components_[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scikit-learn Implementation\n",
    "\n",
    "### Using scikit-learn PCA\n",
    "\n",
    "Scikit-learn provides optimized, production-ready PCA implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "print(\"Iris Dataset PCA Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "\n",
    "# Standardize the data (important for PCA!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\nAfter PCA shape: {X_pca.shape}\")\n",
    "print(f\"\\nExplained variance ratio:\")\n",
    "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {ratio:.1%}\")\n",
    "print(f\"  Total: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Original data (first 2 features)\n",
    "ax = axes[0]\n",
    "scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7, s=60)\n",
    "ax.set_xlabel(feature_names[0], fontsize=11)\n",
    "ax.set_ylabel(feature_names[1], fontsize=11)\n",
    "ax.set_title('Original Data (First 2 Features)', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax, label='Species')\n",
    "\n",
    "# PCA projection\n",
    "ax = axes[1]\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7, s=60)\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)\n",
    "ax.set_title('PCA Projection (2D)', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax, label='Species')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show component loadings\n",
    "print(\"\\nComponent Loadings (How features contribute to PCs):\")\n",
    "components_df = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(2)],\n",
    "    index=feature_names\n",
    ")\n",
    "print(components_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Choosing the Number of Components\n",
    "\n",
    "### How Many Components Should We Keep?\n",
    "\n",
    "We need to balance:\n",
    "- **Information retention**: Keep enough variance\n",
    "- **Dimensionality reduction**: Reduce dimensions significantly\n",
    "\n",
    "### Methods for Choosing Components\n",
    "\n",
    "1. **Variance Explained**: Keep components that explain X% of variance (e.g., 95%)\n",
    "2. **Elbow Method**: Plot cumulative variance, look for elbow\n",
    "3. **Scree Plot**: Plot eigenvalues, look for drop-off\n",
    "4. **Kaiser Criterion**: Keep components with eigenvalue > 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze variance explained for different numbers of components\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit PCA with all components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# Calculate cumulative variance\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Variance explained per component\n",
    "ax = axes[0]\n",
    "ax.bar(range(1, 5), pca_full.explained_variance_ratio_, alpha=0.7, color='steelblue')\n",
    "ax.axhline(y=0.1, color='r', linestyle='--', alpha=0.7, label='10% threshold')\n",
    "ax.set_xlabel('Principal Component', fontsize=11)\n",
    "ax.set_ylabel('Variance Explained Ratio', fontsize=11)\n",
    "ax.set_title('Variance Explained per Component', fontweight='bold')\n",
    "ax.set_xticks(range(1, 5))\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Cumulative variance\n",
    "ax = axes[1]\n",
    "ax.plot(range(1, 5), cumulative_variance, 'o-', linewidth=2, markersize=8, color='green')\n",
    "ax.axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95% threshold')\n",
    "ax.axhline(y=0.99, color='orange', linestyle='--', alpha=0.7, label='99% threshold')\n",
    "ax.fill_between(range(1, 5), cumulative_variance, alpha=0.3, color='green')\n",
    "ax.set_xlabel('Number of Components', fontsize=11)\n",
    "ax.set_ylabel('Cumulative Variance Explained', fontsize=11)\n",
    "ax.set_title('Cumulative Variance Explained', fontweight='bold')\n",
    "ax.set_xticks(range(1, 5))\n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Scree plot (eigenvalues)\n",
    "ax = axes[2]\n",
    "ax.plot(range(1, 5), pca_full.explained_variance_, 's-', linewidth=2, markersize=8, color='red')\n",
    "ax.axhline(y=1, color='orange', linestyle='--', alpha=0.7, label='Kaiser criterion (λ=1)')\n",
    "ax.set_xlabel('Principal Component', fontsize=11)\n",
    "ax.set_ylabel('Eigenvalue (Variance)', fontsize=11)\n",
    "ax.set_title('Scree Plot (Eigenvalues)', fontweight='bold')\n",
    "ax.set_xticks(range(1, 5))\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Component Selection Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "for i, (var, cum_var) in enumerate(zip(pca_full.explained_variance_ratio_, cumulative_variance), 1):\n",
    "    print(f\"PC{i}: {var:.1%} variance (Cumulative: {cum_var:.1%})\")\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"\\nComponents needed for 95% variance: {n_components_95}\")\n",
    "print(f\"Components needed for 99% variance: {np.argmax(cumulative_variance >= 0.99) + 1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-World Application: Feature Reduction for Classification\n",
    "\n",
    "Use PCA to reduce dimensions before training a classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Breast Cancer dataset (high-dimensional)\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "print(\"Breast Cancer Dataset - PCA for Classification\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train classifier on original data\n",
    "lr_original = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_original.fit(X_train_scaled, y_train)\n",
    "acc_original = accuracy_score(y_test, lr_original.predict(X_test_scaled))\n",
    "\n",
    "# Apply PCA and train classifier\n",
    "pca = PCA(n_components=0.95)  # Keep 95% variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"\\nAfter PCA (95% variance):\")\n",
    "print(f\"  Shape: {X_train_pca.shape}\")\n",
    "print(f\"  Components: {pca.n_components_}\")\n",
    "print(f\"  Variance explained: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "\n",
    "lr_pca = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_pca.fit(X_train_pca, y_train)\n",
    "acc_pca = accuracy_score(y_test, lr_pca.predict(X_test_pca))\n",
    "\n",
    "print(f\"\\nClassification Accuracy:\")\n",
    "print(f\"  Original ({X_train_scaled.shape[1]} features): {acc_original:.3f}\")\n",
    "print(f\"  After PCA ({pca.n_components_} components): {acc_pca:.3f}\")\n",
    "print(f\"\\n  Reduction: {X_train_scaled.shape[1] - pca.n_components_} features removed\")\n",
    "print(f\"  Accuracy change: {acc_pca - acc_original:+.3f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Variance explained\n",
    "ax = axes[0]\n",
    "ax.plot(range(1, min(15, pca.n_components_) + 1), \n",
    "        pca.explained_variance_ratio_[:15], 'o-', linewidth=2)\n",
    "ax.set_xlabel('Principal Component', fontsize=11)\n",
    "ax.set_ylabel('Variance Explained Ratio', fontsize=11)\n",
    "ax.set_title('Variance Explained by Each Component', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "ax = axes[1]\n",
    "ax.bar(['Original', 'PCA'], [acc_original, acc_pca], alpha=0.7, color=['steelblue', 'green'])\n",
    "ax.set_ylabel('Accuracy', fontsize=11)\n",
    "ax.set_title('Classification Accuracy Comparison', fontweight='bold')\n",
    "ax.set_ylim([0.9, 1.0])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for i, acc in enumerate([acc_original, acc_pca]):\n",
    "    ax.text(i, acc + 0.005, f'{acc:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Limitations of PCA\n",
    "\n",
    "### When PCA Works Well:\n",
    "- ✅ **Linear relationships** between features\n",
    "- ✅ **High-dimensional data** with correlations\n",
    "- ✅ **Noise reduction** needed\n",
    "- ✅ **Fast computation** required\n",
    "\n",
    "### When PCA Fails:\n",
    "- ❌ **Non-linear relationships** between features\n",
    "- ❌ **All features are independent** (no correlation)\n",
    "- ❌ **Interpretability** is critical (PCs are linear combinations)\n",
    "- ❌ **Outliers** present (sensitive to outliers)\n",
    "\n",
    "### Alternatives:\n",
    "- **Kernel PCA**: Non-linear PCA using kernel trick\n",
    "- **t-SNE / UMAP**: Non-linear dimensionality reduction for visualization\n",
    "- **Autoencoders**: Neural network-based dimensionality reduction\n",
    "- **ICA**: Independent Component Analysis (for independent sources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Practice Problems\n",
    "\n",
    "### Problem 1: Face Recognition with PCA\n",
    "\n",
    "Use PCA to reduce dimensions of face images while preserving maximum variance.\n",
    "\n",
    "### Problem 2: Feature Selection Analysis\n",
    "\n",
    "Apply PCA to a high-dimensional dataset and analyze which original features contribute most to each principal component.\n",
    "\n",
    "### Problem 3: Optimal Component Selection\n",
    "\n",
    "Given a dataset, find the optimal number of components using variance explained and visualization quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Key Takeaways\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **PCA Algorithm**:\n",
    "   - Center and standardize data\n",
    "   - Compute covariance matrix\n",
    "   - Find eigenvalues and eigenvectors\n",
    "   - Select top k components\n",
    "   - Transform data\n",
    "\n",
    "2. **Principal Components**:\n",
    "   - Orthogonal directions of maximum variance\n",
    "   - Ordered by variance explained\n",
    "   - Uncorrelated with each other\n",
    "\n",
    "3. **When to Use**:\n",
    "   - High-dimensional data\n",
    "   - Visualization needs (2D/3D projection)\n",
    "   - Noise reduction\n",
    "   - Feature extraction\n",
    "\n",
    "4. **Component Selection**:\n",
    "   - Variance explained threshold (e.g., 95%)\n",
    "   - Elbow method\n",
    "   - Scree plot\n",
    "   - Kaiser criterion (eigenvalue > 1)\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "- ✅ **Always standardize** before PCA (unless all features are on same scale)\n",
    "- ✅ **Interpretability**: PCs are linear combinations, harder to interpret\n",
    "- ✅ **No labels needed**: PCA is unsupervised\n",
    "- ⚠️ **Linear only**: Assumes linear relationships\n",
    "- ⚠️ **Outlier sensitive**: Outliers can dominate principal components\n",
    "\n",
    "### Time Complexity:\n",
    "- Covariance matrix: O(n × d²) where n=samples, d=features\n",
    "- Eigenvalue decomposition: O(d³)\n",
    "- Transformation: O(n × d × k) where k=components\n",
    "\n",
    "### Next Steps:\n",
    "1. **Kernel PCA**: Non-linear dimensionality reduction\n",
    "2. **t-SNE**: Non-linear visualization technique\n",
    "3. **Factor Analysis**: Related dimensionality reduction method\n",
    "4. **Autoencoders**: Deep learning approach to dimensionality reduction\n",
    "\n",
    "---\n",
    "\n",
    "**Resources:**\n",
    "- Scikit-learn Documentation: https://scikit-learn.org/stable/modules/decomposition.html#pca\n",
    "- \"The Elements of Statistical Learning\" by Hastie, Tibshirani, Friedman\n",
    "- \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
