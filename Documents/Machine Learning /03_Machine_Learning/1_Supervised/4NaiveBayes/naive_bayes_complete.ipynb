{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - Complete Guide\n",
    "\n",
    "## From Probability Theory to Implementation\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on **Bayes' Theorem** with a \"naive\" assumption of feature independence.\n",
    "\n",
    "### What You'll Learn\n",
    "1. Bayes' Theorem foundations\n",
    "2. Naive independence assumption\n",
    "3. Gaussian Naive Bayes\n",
    "4. Multinomial Naive Bayes (text classification)\n",
    "5. Bernoulli Naive Bayes\n",
    "6. Implementation from scratch\n",
    "7. Real-world applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from scipy.stats import norm\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bayes' Theorem\n",
    "\n",
    "$$P(C_k|\\mathbf{x}) = \\frac{P(\\mathbf{x}|C_k) \\cdot P(C_k)}{P(\\mathbf{x})}$$\n",
    "\n",
    "Where:\n",
    "- $P(C_k|\\mathbf{x})$ = **Posterior**: Probability of class $C_k$ given features $\\mathbf{x}$\n",
    "- $P(\\mathbf{x}|C_k)$ = **Likelihood**: Probability of features given class\n",
    "- $P(C_k)$ = **Prior**: Probability of class $C_k$\n",
    "- $P(\\mathbf{x})$ = **Evidence**: Probability of features (normalization constant)\n",
    "\n",
    "### Naive Assumption\n",
    "\n",
    "Features are conditionally independent given the class:\n",
    "\n",
    "$$P(\\mathbf{x}|C_k) = P(x_1, x_2, ..., x_n|C_k) = \\prod_{i=1}^{n} P(x_i|C_k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Bayes' Theorem concept\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Example: Medical test\n",
    "# P(Disease) = 0.01\n",
    "# P(Positive|Disease) = 0.9 (sensitivity)\n",
    "# P(Positive|No Disease) = 0.1 (false positive rate)\n",
    "\n",
    "prior_disease = 0.01\n",
    "prior_no_disease = 0.99\n",
    "likelihood_pos_given_disease = 0.9\n",
    "likelihood_pos_given_no_disease = 0.1\n",
    "\n",
    "# Calculate posterior using Bayes' theorem\n",
    "evidence = (likelihood_pos_given_disease * prior_disease + \n",
    "            likelihood_pos_given_no_disease * prior_no_disease)\n",
    "\n",
    "posterior_disease = (likelihood_pos_given_disease * prior_disease) / evidence\n",
    "\n",
    "# Prior vs Posterior\n",
    "categories = ['Disease', 'No Disease']\n",
    "priors = [prior_disease, prior_no_disease]\n",
    "posteriors = [posterior_disease, 1 - posterior_disease]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, priors, width, label='Prior P(C)', color='lightblue')\n",
    "axes[0].bar(x + width/2, posteriors, width, label='Posterior P(C|Test+)', color='coral')\n",
    "axes[0].set_ylabel('Probability', fontsize=12)\n",
    "axes[0].set_title('Bayes Theorem: Updating Beliefs\\n(Medical Test Example)', fontsize=14)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(categories)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Flow diagram\n",
    "axes[1].text(0.5, 0.9, 'Bayes\\' Theorem Flow', ha='center', fontsize=16, fontweight='bold')\n",
    "axes[1].text(0.5, 0.75, 'Prior P(C)', ha='center', fontsize=12, \n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "axes[1].arrow(0.5, 0.7, 0, -0.08, head_width=0.05, head_length=0.03, fc='black')\n",
    "axes[1].text(0.5, 0.55, 'Likelihood P(X|C)', ha='center', fontsize=12,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "axes[1].arrow(0.5, 0.5, 0, -0.08, head_width=0.05, head_length=0.03, fc='black')\n",
    "axes[1].text(0.5, 0.35, 'Evidence P(X)', ha='center', fontsize=12,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "axes[1].arrow(0.5, 0.3, 0, -0.08, head_width=0.05, head_length=0.03, fc='black')\n",
    "axes[1].text(0.5, 0.15, 'Posterior P(C|X)', ha='center', fontsize=12,\n",
    "            bbox=dict(boxstyle='round', facecolor='coral', alpha=0.8))\n",
    "axes[1].set_xlim(0, 1)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Prior probability of disease: {prior_disease:.1%}\")\n",
    "print(f\"Posterior probability given positive test: {posterior_disease:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gaussian Naive Bayes\n",
    "\n",
    "Assumes features follow a **Gaussian (normal) distribution**:\n",
    "\n",
    "$$P(x_i|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_{k}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{k})^2}{2\\sigma_{k}^2}\\right)$$\n",
    "\n",
    "Used for continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Visualize feature distributions by class\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature_idx in enumerate(range(4)):\n",
    "    for class_idx in range(3):\n",
    "        feature_values = X_train[y_train == class_idx, feature_idx]\n",
    "        \n",
    "        # Plot histogram\n",
    "        axes[idx].hist(feature_values, bins=20, alpha=0.5, label=target_names[class_idx])\n",
    "        \n",
    "        # Fit Gaussian and plot\n",
    "        mu, sigma = feature_values.mean(), feature_values.std()\n",
    "        x = np.linspace(feature_values.min(), feature_values.max(), 100)\n",
    "        axes[idx].plot(x, norm.pdf(x, mu, sigma) * len(feature_values) * \n",
    "                      (feature_values.max() - feature_values.min()) / 20, linewidth=2)\n",
    "    \n",
    "    axes[idx].set_xlabel(feature_names[idx], fontsize=12)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[idx].set_title(f'Distribution: {feature_names[idx]}', fontsize=14)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian Naive Bayes Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayesScratch:\n",
    "    \"\"\"Gaussian Naive Bayes from scratch\"\"\"\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Calculate priors, means, and variances\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        # Initialize storage\n",
    "        self.priors = np.zeros(n_classes)\n",
    "        self.means = np.zeros((n_classes, n_features))\n",
    "        self.vars = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        # Calculate statistics for each class\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.priors[idx] = X_c.shape[0] / n_samples\n",
    "            self.means[idx, :] = X_c.mean(axis=0)\n",
    "            self.vars[idx, :] = X_c.var(axis=0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _gaussian_probability(self, x, mean, var):\n",
    "        \"\"\"Calculate Gaussian probability density\"\"\"\n",
    "        eps = 1e-9  # Avoid division by zero\n",
    "        coeff = 1.0 / np.sqrt(2.0 * np.pi * var + eps)\n",
    "        exponent = np.exp(-(x - mean)**2 / (2 * var + eps))\n",
    "        return coeff * exponent\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        predictions = [self._predict_single(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def _predict_single(self, x):\n",
    "        \"\"\"Predict single sample\"\"\"\n",
    "        posteriors = []\n",
    "        \n",
    "        for idx, c in enumerate(self.classes):\n",
    "            # Log prior\n",
    "            prior = np.log(self.priors[idx])\n",
    "            \n",
    "            # Log likelihood\n",
    "            likelihood = np.sum(np.log(\n",
    "                self._gaussian_probability(x, self.means[idx, :], self.vars[idx, :]) + 1e-9\n",
    "            ))\n",
    "            \n",
    "            # Log posterior\n",
    "            posterior = prior + likelihood\n",
    "            posteriors.append(posterior)\n",
    "        \n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "# Train and evaluate\n",
    "gnb_scratch = GaussianNaiveBayesScratch()\n",
    "gnb_scratch.fit(X_train, y_train)\n",
    "y_pred_scratch = gnb_scratch.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy (from scratch): {accuracy_score(y_test, y_pred_scratch):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_scratch, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scikit-learn Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn's GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "y_prob = gnb.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy (sklearn): {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"\\nClass Priors: {gnb.class_prior_}\")\n",
    "print(f\"\\nFeature Means per Class:\")\n",
    "print(pd.DataFrame(gnb.theta_, columns=feature_names, index=target_names))\n",
    "print(f\"\\nFeature Variances per Class:\")\n",
    "print(pd.DataFrame(gnb.var_, columns=feature_names, index=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "axes[0].set_title('Confusion Matrix', fontsize=14)\n",
    "\n",
    "# Prediction Probabilities\n",
    "sample_probs = y_prob[:10]\n",
    "x_pos = np.arange(len(sample_probs))\n",
    "width = 0.25\n",
    "\n",
    "for i, class_name in enumerate(target_names):\n",
    "    axes[1].bar(x_pos + i*width, sample_probs[:, i], width, label=class_name)\n",
    "\n",
    "axes[1].set_xlabel('Sample', fontsize=12)\n",
    "axes[1].set_ylabel('Probability', fontsize=12)\n",
    "axes[1].set_title('Prediction Probabilities (First 10 samples)', fontsize=14)\n",
    "axes[1].set_xticks(x_pos + width)\n",
    "axes[1].set_xticklabels(range(10))\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multinomial Naive Bayes (Text Classification)\n",
    "\n",
    "Used for **discrete count data** (e.g., word counts in text):\n",
    "\n",
    "$$P(x_i|C_k) = \\frac{N_{x_i,C_k} + \\alpha}{N_{C_k} + \\alpha n}$$\n",
    "\n",
    "Where:\n",
    "- $N_{x_i,C_k}$ = count of feature $x_i$ in class $C_k$\n",
    "- $\\alpha$ = Laplace smoothing parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text classification example\n",
    "documents = [\n",
    "    \"Python is great for machine learning\",\n",
    "    \"I love Python programming\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"Java is used for enterprise applications\",\n",
    "    \"I prefer Java over other languages\",\n",
    "    \"Enterprise software often uses Java\",\n",
    "    \"Deep learning uses neural networks\",\n",
    "    \"Python is perfect for data science\"\n",
    "]\n",
    "\n",
    "labels = ['Python', 'Python', 'ML', 'Java', 'Java', 'Java', 'ML', 'Python']\n",
    "\n",
    "# Convert to numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "X_text = vectorizer.fit_transform(documents)\n",
    "feature_names_text = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Train Multinomial NB\n",
    "mnb = MultinomialNB(alpha=1.0)  # alpha is Laplace smoothing\n",
    "mnb.fit(X_text, labels)\n",
    "\n",
    "# Test\n",
    "test_docs = [\n",
    "    \"Python is amazing\",\n",
    "    \"Java enterprise development\",\n",
    "    \"Machine learning with Python\"\n",
    "]\n",
    "\n",
    "X_test_text = vectorizer.transform(test_docs)\n",
    "predictions = mnb.predict(X_test_text)\n",
    "probabilities = mnb.predict_proba(X_test_text)\n",
    "\n",
    "print(\"Text Classification Results:\\n\")\n",
    "for doc, pred, prob in zip(test_docs, predictions, probabilities):\n",
    "    print(f\"Document: '{doc}'\")\n",
    "    print(f\"Prediction: {pred}\")\n",
    "    print(f\"Probabilities: {dict(zip(mnb.classes_, prob))}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word importance per class\n",
    "log_prob = mnb.feature_log_prob_\n",
    "classes = mnb.classes_\n",
    "\n",
    "fig, axes = plt.subplots(1, len(classes), figsize=(16, 5))\n",
    "\n",
    "for idx, class_name in enumerate(classes):\n",
    "    # Get top 10 words for this class\n",
    "    top_indices = np.argsort(log_prob[idx])[-10:]\n",
    "    top_features = [feature_names_text[i] for i in top_indices]\n",
    "    top_scores = log_prob[idx][top_indices]\n",
    "    \n",
    "    axes[idx].barh(top_features, np.exp(top_scores), color='skyblue')\n",
    "    axes[idx].set_xlabel('Probability', fontsize=12)\n",
    "    axes[idx].set_title(f'Top Words for \"{class_name}\"', fontsize=14)\n",
    "    axes[idx].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-World Example: Spam Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate spam/ham emails\n",
    "spam_emails = [\n",
    "    \"Win money now! Click here for free cash!\",\n",
    "    \"Congratulations! You won a lottery!\",\n",
    "    \"Free viagra! Buy now!\",\n",
    "    \"Make money fast! Limited time offer!\",\n",
    "    \"Click here to claim your prize money!\",\n",
    "    \"Get rich quick! Amazing opportunity!\"\n",
    "]\n",
    "\n",
    "ham_emails = [\n",
    "    \"Meeting scheduled for tomorrow at 3pm\",\n",
    "    \"Can you send me the project report?\",\n",
    "    \"Let's have lunch next week\",\n",
    "    \"The presentation looks great\",\n",
    "    \"Please review the attached document\",\n",
    "    \"Thank you for your help yesterday\"\n",
    "]\n",
    "\n",
    "all_emails = spam_emails + ham_emails\n",
    "email_labels = ['spam'] * len(spam_emails) + ['ham'] * len(ham_emails)\n",
    "\n",
    "# Convert to features using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=50)\n",
    "X_email = tfidf.fit_transform(all_emails)\n",
    "\n",
    "# Train-test split\n",
    "X_train_email, X_test_email, y_train_email, y_test_email = train_test_split(\n",
    "    X_email, email_labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train Multinomial NB\n",
    "spam_classifier = MultinomialNB()\n",
    "spam_classifier.fit(X_train_email, y_train_email)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_email = spam_classifier.predict(X_test_email)\n",
    "print(f\"Spam Classifier Accuracy: {accuracy_score(y_test_email, y_pred_email):.4f}\")\n",
    "\n",
    "# Test on new emails\n",
    "new_emails = [\n",
    "    \"Free money! Win now!\",\n",
    "    \"Can we reschedule the meeting?\"\n",
    "]\n",
    "\n",
    "X_new = tfidf.transform(new_emails)\n",
    "predictions_new = spam_classifier.predict(X_new)\n",
    "probs_new = spam_classifier.predict_proba(X_new)\n",
    "\n",
    "print(\"\\nNew Email Predictions:\\n\")\n",
    "for email, pred, prob in zip(new_emails, predictions_new, probs_new):\n",
    "    print(f\"Email: '{email}'\")\n",
    "    print(f\"Prediction: {pred}\")\n",
    "    print(f\"Confidence: {max(prob):.2%}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bernoulli Naive Bayes\n",
    "\n",
    "Used for **binary/boolean features**:\n",
    "\n",
    "$$P(x_i|C_k) = P(i|C_k)x_i + (1-P(i|C_k))(1-x_i)$$\n",
    "\n",
    "Each feature is either present (1) or absent (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary features example\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Binarize the email data\n",
    "binarizer = Binarizer()\n",
    "X_binary = binarizer.transform(X_email.toarray())\n",
    "\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(\n",
    "    X_binary, email_labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Compare Multinomial vs Bernoulli\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_bin, y_train_bin)\n",
    "\n",
    "mnb_score = MultinomialNB().fit(X_train_email, y_train_email).score(X_test_email, y_test_email)\n",
    "bnb_score = bnb.score(X_test_bin, y_test_bin)\n",
    "\n",
    "print(f\"Multinomial NB Accuracy: {mnb_score:.4f}\")\n",
    "print(f\"Bernoulli NB Accuracy: {bnb_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparing All Naive Bayes Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer, y_cancer = cancer.data, cancer.target\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train different variants\n",
    "models = {\n",
    "    'Gaussian NB': GaussianNB(),\n",
    "    'Multinomial NB': MultinomialNB(),\n",
    "    'Bernoulli NB': BernoulliNB()\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_c, y_train_c, cv=5)\n",
    "    \n",
    "    # Train and test\n",
    "    model.fit(X_train_c, y_train_c)\n",
    "    test_score = model.score(X_test_c, y_test_c)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'CV Mean': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std(),\n",
    "        'Test Score': test_score\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_pos = np.arange(len(results_df))\n",
    "ax.bar(x_pos, results_df['Test Score'], yerr=results_df['CV Std'], \n",
    "       alpha=0.7, capsize=10, color=['skyblue', 'lightgreen', 'coral'])\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(results_df['Model'])\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Naive Bayes Variants Comparison', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Bayes' Theorem**: Foundation of probabilistic classification\n",
    "2. **Naive Assumption**: Features are conditionally independent (rarely true but works well)\n",
    "3. **Three Variants**:\n",
    "   - **Gaussian**: Continuous features (normal distribution)\n",
    "   - **Multinomial**: Discrete counts (text, word frequencies)\n",
    "   - **Bernoulli**: Binary features (presence/absence)\n",
    "4. **Laplace Smoothing**: Handles zero probabilities\n",
    "5. **Fast Training**: Only needs to calculate statistics\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "**Pros:**\n",
    "- Fast training and prediction\n",
    "- Works well with high-dimensional data\n",
    "- Requires small training dataset\n",
    "- Naturally handles multi-class problems\n",
    "- Provides probability estimates\n",
    "- Not sensitive to irrelevant features\n",
    "\n",
    "**Cons:**\n",
    "- Strong independence assumption (rarely true)\n",
    "- Can be outperformed by more sophisticated models\n",
    "- Zero-frequency problem (solved with Laplace smoothing)\n",
    "- Gaussian NB assumes normal distribution\n",
    "\n",
    "### When to Use Naive Bayes\n",
    "\n",
    "**Best for:**\n",
    "- Text classification (spam detection, sentiment analysis)\n",
    "- Document categorization\n",
    "- Real-time prediction (fast)\n",
    "- High-dimensional data\n",
    "- Baseline model\n",
    "\n",
    "**Avoid when:**\n",
    "- Features are highly correlated\n",
    "- Need highly accurate probability estimates\n",
    "- Features have complex interactions\n",
    "\n",
    "### Practice Problems\n",
    "\n",
    "1. Implement Laplace smoothing in the scratch version\n",
    "2. Build a sentiment analyzer for movie reviews\n",
    "3. Compare NB with Logistic Regression on text data\n",
    "4. Analyze the effect of alpha (smoothing parameter) on performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
