{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees - Complete Guide\n",
    "\n",
    "## From Theory to Implementation\n",
    "\n",
    "Decision Trees are **hierarchical models** that make decisions by asking a series of questions about features.\n",
    "\n",
    "### What You'll Learn\n",
    "1. Tree structure and terminology\n",
    "2. Splitting criteria (Gini, Entropy, Information Gain)\n",
    "3. CART algorithm\n",
    "4. Implementation from scratch\n",
    "5. Pruning and regularization\n",
    "6. Feature importance\n",
    "7. Regression trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, load_wine, make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tree Structure and Terminology\n",
    "\n",
    "- **Root Node**: Top of tree, entire dataset\n",
    "- **Internal Node**: Decision point (split based on feature)\n",
    "- **Leaf Node**: Final prediction (no further splits)\n",
    "- **Branch**: Connection between nodes\n",
    "- **Depth**: Length from root to leaf\n",
    "- **Splitting**: Dividing node into sub-nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple decision tree visualization\n",
    "iris = load_iris()\n",
    "X, y = iris.data[:, [2, 3]], iris.target  # Use only 2 features for visualization\n",
    "feature_names = ['Petal Length', 'Petal Width']\n",
    "target_names = iris.target_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a simple tree\n",
    "tree_simple = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_simple.fit(X_train, y_train)\n",
    "\n",
    "# Visualize the tree\n",
    "plt.figure(figsize=(16, 8))\n",
    "plot_tree(tree_simple, feature_names=feature_names, class_names=target_names.tolist(),\n",
    "          filled=True, rounded=True, fontsize=12)\n",
    "plt.title('Decision Tree Structure (max_depth=2)', fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Tree depth: {tree_simple.get_depth()}\")\n",
    "print(f\"Number of leaves: {tree_simple.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting Criteria\n",
    "\n",
    "### Gini Impurity\n",
    "$$Gini = 1 - \\sum_{i=1}^{C} p_i^2$$\n",
    "\n",
    "### Entropy (Information Gain)\n",
    "$$Entropy = -\\sum_{i=1}^{C} p_i \\log_2(p_i)$$\n",
    "\n",
    "$$Information\\ Gain = Entropy_{parent} - \\sum \\frac{n_j}{n} Entropy_{child_j}$$\n",
    "\n",
    "Where $p_i$ is the proportion of class $i$ samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Gini vs Entropy\n",
    "p = np.linspace(0.001, 0.999, 100)\n",
    "\n",
    "# Binary classification\n",
    "gini = 1 - (p**2 + (1-p)**2)\n",
    "entropy = -(p * np.log2(p) + (1-p) * np.log2(1-p))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Gini Impurity\n",
    "axes[0].plot(p, gini, 'b-', linewidth=3, label='Gini')\n",
    "axes[0].fill_between(p, gini, alpha=0.3)\n",
    "axes[0].set_xlabel('Proportion of Class 1', fontsize=12)\n",
    "axes[0].set_ylabel('Impurity', fontsize=12)\n",
    "axes[0].set_title('Gini Impurity', fontsize=14)\n",
    "axes[0].axvline(0.5, color='r', linestyle='--', label='Max impurity (p=0.5)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Entropy\n",
    "axes[1].plot(p, entropy, 'g-', linewidth=3, label='Entropy')\n",
    "axes[1].fill_between(p, entropy, alpha=0.3, color='green')\n",
    "axes[1].set_xlabel('Proportion of Class 1', fontsize=12)\n",
    "axes[1].set_ylabel('Entropy', fontsize=12)\n",
    "axes[1].set_title('Entropy', fontsize=14)\n",
    "axes[1].axvline(0.5, color='r', linestyle='--', label='Max entropy (p=0.5)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparison\n",
    "axes[2].plot(p, gini, 'b-', linewidth=3, label='Gini')\n",
    "axes[2].plot(p, entropy, 'g-', linewidth=3, label='Entropy')\n",
    "axes[2].set_xlabel('Proportion of Class 1', fontsize=12)\n",
    "axes[2].set_ylabel('Impurity/Entropy', fontsize=12)\n",
    "axes[2].set_title('Gini vs Entropy Comparison', fontsize=14)\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Both measures are similar but:\")\n",
    "print(\"- Gini: Faster to compute (no logarithm)\")\n",
    "print(\"- Entropy: More theoretically grounded (information theory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Tree Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Decision Tree Node\"\"\"\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature      # Feature index to split on\n",
    "        self.threshold = threshold  # Threshold value for split\n",
    "        self.left = left           # Left child\n",
    "        self.right = right         # Right child\n",
    "        self.value = value         # Class label (for leaf nodes)\n",
    "\n",
    "class DecisionTreeScratch:\n",
    "    \"\"\"Decision Tree Classifier from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=None, min_samples_split=2, criterion='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "        self.root = None\n",
    "    \n",
    "    def _gini(self, y):\n",
    "        \"\"\"Calculate Gini impurity\"\"\"\n",
    "        m = len(y)\n",
    "        if m == 0:\n",
    "            return 0\n",
    "        counter = Counter(y)\n",
    "        impurity = 1.0\n",
    "        for count in counter.values():\n",
    "            p = count / m\n",
    "            impurity -= p ** 2\n",
    "        return impurity\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        \"\"\"Calculate entropy\"\"\"\n",
    "        m = len(y)\n",
    "        if m == 0:\n",
    "            return 0\n",
    "        counter = Counter(y)\n",
    "        entropy = 0.0\n",
    "        for count in counter.values():\n",
    "            p = count / m\n",
    "            if p > 0:\n",
    "                entropy -= p * np.log2(p)\n",
    "        return entropy\n",
    "    \n",
    "    def _impurity(self, y):\n",
    "        \"\"\"Calculate impurity based on criterion\"\"\"\n",
    "        if self.criterion == 'gini':\n",
    "            return self._gini(y)\n",
    "        else:\n",
    "            return self._entropy(y)\n",
    "    \n",
    "    def _information_gain(self, X_column, y, threshold):\n",
    "        \"\"\"Calculate information gain for a split\"\"\"\n",
    "        # Parent impurity\n",
    "        parent_impurity = self._impurity(y)\n",
    "        \n",
    "        # Split\n",
    "        left_mask = X_column <= threshold\n",
    "        right_mask = X_column > threshold\n",
    "        \n",
    "        if sum(left_mask) == 0 or sum(right_mask) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Weighted child impurity\n",
    "        n = len(y)\n",
    "        n_left, n_right = sum(left_mask), sum(right_mask)\n",
    "        impurity_left = self._impurity(y[left_mask])\n",
    "        impurity_right = self._impurity(y[right_mask])\n",
    "        child_impurity = (n_left / n) * impurity_left + (n_right / n) * impurity_right\n",
    "        \n",
    "        return parent_impurity - child_impurity\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"Find the best split\"\"\"\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        for feature_idx in range(n_features):\n",
    "            X_column = X[:, feature_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(X_column, y, threshold)\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold, best_gain\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        \"\"\"Recursively build the tree\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if (depth >= self.max_depth if self.max_depth else False) or \\\n",
    "           n_classes == 1 or \\\n",
    "           n_samples < self.min_samples_split:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        # Find best split\n",
    "        best_feature, best_threshold, best_gain = self._best_split(X, y)\n",
    "        \n",
    "        if best_gain == 0:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        # Split\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = X[:, best_feature] > best_threshold\n",
    "        \n",
    "        left = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        \n",
    "        return Node(best_feature, best_threshold, left, right)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build decision tree\"\"\"\n",
    "        self.root = self._build_tree(X, y)\n",
    "        return self\n",
    "    \n",
    "    def _predict_single(self, x, node):\n",
    "        \"\"\"Predict single sample\"\"\"\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._predict_single(x, node.left)\n",
    "        else:\n",
    "            return self._predict_single(x, node.right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict multiple samples\"\"\"\n",
    "        return np.array([self._predict_single(x, self.root) for x in X])\n",
    "\n",
    "# Train and evaluate\n",
    "tree_scratch = DecisionTreeScratch(max_depth=5, criterion='gini')\n",
    "tree_scratch.fit(X_train, y_train)\n",
    "y_pred_scratch = tree_scratch.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy (from scratch): {accuracy_score(y_test, y_pred_scratch):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scikit-learn Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full iris dataset\n",
    "iris_full = load_iris()\n",
    "X_full, y_full = iris_full.data, iris_full.target\n",
    "\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(\n",
    "    X_full, y_full, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train tree\n",
    "tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_clf.fit(X_train_f, y_train_f)\n",
    "\n",
    "y_pred_f = tree_clf.predict(X_test_f)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test_f, y_pred_f):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_f, y_pred_f, target_names=iris_full.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize full tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(tree_clf, feature_names=iris_full.feature_names, \n",
    "          class_names=iris_full.target_names.tolist(),\n",
    "          filled=True, rounded=True, fontsize=10)\n",
    "plt.title('Full Decision Tree (max_depth=3)', fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundaries for different depths\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "depths = [1, 2, 3, 5, 10, None]\n",
    "\n",
    "for idx, depth in enumerate(depths):\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    # Create mesh\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "    axes[idx].scatter(X_train[:, 0], X_train[:, 1], c=y_train, \n",
    "                     cmap='RdYlBu', edgecolors='black', s=50)\n",
    "    \n",
    "    accuracy = tree.score(X_test, y_test)\n",
    "    depth_str = depth if depth else 'Unlimited'\n",
    "    axes[idx].set_title(f'Depth = {depth_str}\\nAcc = {accuracy:.3f}', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(feature_names[0])\n",
    "    axes[idx].set_ylabel(feature_names[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overfitting and Regularization\n",
    "\n",
    "### Regularization Parameters:\n",
    "- `max_depth`: Maximum depth of tree\n",
    "- `min_samples_split`: Minimum samples to split a node\n",
    "- `min_samples_leaf`: Minimum samples in a leaf\n",
    "- `max_features`: Maximum features to consider for split\n",
    "- `max_leaf_nodes`: Maximum number of leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate overfitting\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "depths = range(1, 21)\n",
    "\n",
    "for depth in depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train_f, y_train_f)\n",
    "    \n",
    "    train_scores.append(tree.score(X_train_f, y_train_f))\n",
    "    test_scores.append(tree.score(X_test_f, y_test_f))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(depths, train_scores, 'b-o', linewidth=2, label='Training Accuracy')\n",
    "plt.plot(depths, test_scores, 'r-s', linewidth=2, label='Test Accuracy')\n",
    "plt.xlabel('Tree Depth', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Overfitting: Training vs Test Accuracy', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(x=depths[np.argmax(test_scores)], color='green', \n",
    "           linestyle='--', label=f'Optimal depth = {depths[np.argmax(test_scores)]}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importances = tree_clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Create DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': [iris_full.feature_names[i] for i in indices],\n",
    "    'Importance': importances[indices]\n",
    "})\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "print(feature_importance_df.to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], \n",
    "        color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('Feature Importances', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression data\n",
    "np.random.seed(42)\n",
    "X_reg = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
    "y_reg = np.sin(X_reg).ravel() + np.random.randn(100) * 0.1\n",
    "\n",
    "# Train regression trees with different depths\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "X_test_reg = np.linspace(0, 5, 500)[:, np.newaxis]\n",
    "\n",
    "for idx, depth in enumerate([2, 5, 20]):\n",
    "    tree_reg = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    tree_reg.fit(X_reg, y_reg)\n",
    "    y_pred_reg = tree_reg.predict(X_test_reg)\n",
    "    \n",
    "    axes[idx].scatter(X_reg, y_reg, color='blue', s=50, label='Training data')\n",
    "    axes[idx].plot(X_test_reg, y_pred_reg, 'r-', linewidth=2, label=f'Tree (depth={depth})')\n",
    "    axes[idx].plot(X_test_reg, np.sin(X_test_reg), 'g--', linewidth=2, label='True function')\n",
    "    axes[idx].set_xlabel('X', fontsize=12)\n",
    "    axes[idx].set_ylabel('y', fontsize=12)\n",
    "    axes[idx].set_title(f'Decision Tree Regression\\n(max_depth={depth})', fontsize=14)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for best parameters\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 9, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), \n",
    "                          param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_f, y_train_f)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test Score: {grid_search.score(X_test_f, y_test_f):.4f}\")\n",
    "\n",
    "# Visualize grid search results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_df = results_df.sort_values('rank_test_score').head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(results_df)), results_df['mean_test_score'], \n",
    "        xerr=results_df['std_test_score'], color='skyblue', edgecolor='black')\n",
    "plt.yticks(range(len(results_df)), \n",
    "          [f\"{i+1}. depth={row['param_max_depth']}, split={row['param_min_samples_split']}\" \n",
    "           for i, (_, row) in enumerate(results_df.iterrows())])\n",
    "plt.xlabel('Mean CV Accuracy', fontsize=12)\n",
    "plt.title('Top 10 Hyperparameter Combinations', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Tree Structure**: Hierarchical model with internal nodes (decisions) and leaf nodes (predictions)\n",
    "2. **Splitting Criteria**: Gini impurity and Entropy measure node purity\n",
    "3. **CART Algorithm**: Greedy recursive splitting\n",
    "4. **Overfitting**: Trees easily overfit without regularization\n",
    "5. **Interpretability**: Easy to visualize and understand\n",
    "6. **Feature Importance**: Automatically calculated\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "**Pros:**\n",
    "- Easy to understand and interpret\n",
    "- Visual representation possible\n",
    "- No feature scaling needed\n",
    "- Handles both numerical and categorical features\n",
    "- Non-parametric (no assumptions about data)\n",
    "- Captures non-linear relationships\n",
    "- Feature importance automatically calculated\n",
    "\n",
    "**Cons:**\n",
    "- Prone to overfitting\n",
    "- Unstable (small changes in data can change tree structure)\n",
    "- Biased toward features with more levels\n",
    "- Greedy algorithm (may not find global optimum)\n",
    "- Can create overly complex trees\n",
    "\n",
    "### When to Use Decision Trees\n",
    "\n",
    "**Best for:**\n",
    "- Need interpretable model\n",
    "- Mixed feature types\n",
    "- Non-linear relationships\n",
    "- Feature importance needed\n",
    "- Building ensemble methods (Random Forest, Gradient Boosting)\n",
    "\n",
    "**Avoid when:**\n",
    "- Need stable predictions\n",
    "- Small datasets (prone to overfitting)\n",
    "- Linear relationships (simpler models better)\n",
    "\n",
    "### Practice Problems\n",
    "\n",
    "1. Implement cost-complexity pruning\n",
    "2. Compare Gini vs Entropy on different datasets\n",
    "3. Build a regression tree from scratch\n",
    "4. Analyze feature importance on high-dimensional data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
