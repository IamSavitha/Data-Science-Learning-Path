{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) - Complete Guide\n",
    "\n",
    "## From Basics to Advanced with Visualizations\n",
    "\n",
    "KNN is a **non-parametric**, **instance-based** learning algorithm used for classification and regression.\n",
    "\n",
    "### What You'll Learn\n",
    "1. KNN fundamentals and distance metrics\n",
    "2. Choosing optimal K\n",
    "3. Implementation from scratch\n",
    "4. Scikit-learn implementation\n",
    "5. Distance weighting\n",
    "6. Dimensionality curse\n",
    "7. Real-world applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, load_iris, make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How KNN Works\n",
    "\n",
    "### Algorithm Steps:\n",
    "1. Store all training data\n",
    "2. For a new point:\n",
    "   - Calculate distance to all training points\n",
    "   - Find K nearest neighbors\n",
    "   - **Classification**: Majority vote\n",
    "   - **Regression**: Average of K neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple dataset\n",
    "np.random.seed(42)\n",
    "X_train = np.array([[1, 2], [2, 3], [3, 1], [6, 5], [7, 7], [8, 6]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "X_new = np.array([[5, 4]])\n",
    "\n",
    "# Visualize KNN concept\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for idx, k in enumerate([1, 3, 5]):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot training data\n",
    "    scatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, \n",
    "                        cmap='RdYlBu', s=200, edgecolors='black', linewidth=2)\n",
    "    \n",
    "    # Plot new point\n",
    "    ax.scatter(X_new[:, 0], X_new[:, 1], c='green', s=300, \n",
    "              marker='*', edgecolors='black', linewidth=2, label='New Point')\n",
    "    \n",
    "    # Calculate distances\n",
    "    distances = np.sqrt(np.sum((X_train - X_new)**2, axis=1))\n",
    "    nearest_idx = np.argsort(distances)[:k]\n",
    "    \n",
    "    # Draw circles to k nearest neighbors\n",
    "    for i in nearest_idx:\n",
    "        ax.plot([X_new[0, 0], X_train[i, 0]], \n",
    "               [X_new[0, 1], X_train[i, 1]], \n",
    "               'g--', alpha=0.5, linewidth=2)\n",
    "        circle = plt.Circle(X_train[i], 0.3, color='green', fill=False, linewidth=2)\n",
    "        ax.add_patch(circle)\n",
    "    \n",
    "    # Prediction\n",
    "    prediction = Counter(y_train[nearest_idx]).most_common(1)[0][0]\n",
    "    ax.set_title(f'K = {k}\\nPrediction: Class {prediction}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1', fontsize=12)\n",
    "    ax.set_ylabel('Feature 2', fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Distance Metrics\n",
    "\n",
    "Common distance metrics:\n",
    "\n",
    "1. **Euclidean**: $d(p,q) = \\sqrt{\\sum_{i=1}^n (p_i - q_i)^2}$\n",
    "2. **Manhattan**: $d(p,q) = \\sum_{i=1}^n |p_i - q_i|$\n",
    "3. **Minkowski**: $d(p,q) = \\left(\\sum_{i=1}^n |p_i - q_i|^p\\right)^{1/p}$\n",
    "4. **Cosine**: $similarity = \\frac{p \\cdot q}{||p|| \\cdot ||q||}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different distance metrics\n",
    "point_a = np.array([2, 2])\n",
    "point_b = np.array([5, 6])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Euclidean\n",
    "euclidean = np.sqrt(np.sum((point_a - point_b)**2))\n",
    "axes[0].plot([point_a[0], point_b[0]], [point_a[1], point_b[1]], 'r-', linewidth=3, label=f'Euclidean = {euclidean:.2f}')\n",
    "axes[0].scatter(*point_a, s=200, c='blue', edgecolors='black', zorder=5)\n",
    "axes[0].scatter(*point_b, s=200, c='red', edgecolors='black', zorder=5)\n",
    "axes[0].set_title('Euclidean Distance\\n(Straight line)', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Manhattan\n",
    "manhattan = np.sum(np.abs(point_a - point_b))\n",
    "axes[1].plot([point_a[0], point_b[0]], [point_a[1], point_a[1]], 'b-', linewidth=3)\n",
    "axes[1].plot([point_b[0], point_b[0]], [point_a[1], point_b[1]], 'b-', linewidth=3, label=f'Manhattan = {manhattan:.2f}')\n",
    "axes[1].scatter(*point_a, s=200, c='blue', edgecolors='black', zorder=5)\n",
    "axes[1].scatter(*point_b, s=200, c='red', edgecolors='black', zorder=5)\n",
    "axes[1].set_title('Manhattan Distance\\n(Grid path)', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Minkowski (p=3)\n",
    "minkowski = np.power(np.sum(np.abs(point_a - point_b)**3), 1/3)\n",
    "axes[2].plot([point_a[0], point_b[0]], [point_a[1], point_b[1]], 'g-', linewidth=3, label=f'Minkowski (p=3) = {minkowski:.2f}')\n",
    "axes[2].scatter(*point_a, s=200, c='blue', edgecolors='black', zorder=5)\n",
    "axes[2].scatter(*point_b, s=200, c='red', edgecolors='black', zorder=5)\n",
    "axes[2].set_title('Minkowski Distance (p=3)', fontsize=14)\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifierScratch:\n",
    "    \"\"\"K-Nearest Neighbors Classifier from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, k=3, distance_metric='euclidean'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Store training data\"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        return self\n",
    "    \n",
    "    def _calculate_distance(self, x1, x2):\n",
    "        \"\"\"Calculate distance between two points\"\"\"\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((x1 - x2)**2, axis=1))\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return np.sum(np.abs(x1 - x2), axis=1)\n",
    "        elif self.distance_metric == 'minkowski':\n",
    "            p = 3\n",
    "            return np.power(np.sum(np.abs(x1 - x2)**p, axis=1), 1/p)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for samples in X\"\"\"\n",
    "        predictions = [self._predict_single(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def _predict_single(self, x):\n",
    "        \"\"\"Predict class label for a single sample\"\"\"\n",
    "        # Calculate distances to all training samples\n",
    "        distances = self._calculate_distance(self.X_train, x)\n",
    "        \n",
    "        # Get indices of k nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        \n",
    "        # Get labels of k nearest neighbors\n",
    "        k_nearest_labels = self.y_train[k_indices]\n",
    "        \n",
    "        # Return most common class label\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "\n",
    "# Test on Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train our KNN\n",
    "knn_scratch = KNNClassifierScratch(k=5)\n",
    "knn_scratch.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = knn_scratch.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy (from scratch): {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Choosing Optimal K\n",
    "\n",
    "- **Small K**: More sensitive to noise (overfitting)\n",
    "- **Large K**: Smoother boundaries (underfitting)\n",
    "- **Rule of thumb**: K = âˆšn (where n = number of samples)\n",
    "- **Best practice**: Use cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different K values\n",
    "k_values = range(1, 31)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_scores.append(knn.score(X_train_scaled, y_train))\n",
    "    test_scores.append(knn.score(X_test_scaled, y_test))\n",
    "    cv_scores.append(cross_val_score(knn, X_train_scaled, y_train, cv=5).mean())\n",
    "\n",
    "# Find optimal K\n",
    "optimal_k = k_values[np.argmax(cv_scores)]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(k_values, train_scores, 'b-o', label='Training Accuracy', linewidth=2)\n",
    "plt.plot(k_values, test_scores, 'r-s', label='Test Accuracy', linewidth=2)\n",
    "plt.plot(k_values, cv_scores, 'g-^', label='CV Accuracy', linewidth=2)\n",
    "plt.axvline(x=optimal_k, color='purple', linestyle='--', linewidth=2, label=f'Optimal K = {optimal_k}')\n",
    "plt.xlabel('K (Number of Neighbors)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Model Performance vs K Value', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal K: {optimal_k}\")\n",
    "print(f\"Best CV Accuracy: {max(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Boundaries Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-linear dataset\n",
    "X_moons, y_moons = make_moons(n_samples=200, noise=0.15, random_state=42)\n",
    "\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train KNN with different K values\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "k_values_viz = [1, 3, 5, 10, 20, 50]\n",
    "\n",
    "for idx, k in enumerate(k_values_viz):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_m, y_train_m)\n",
    "    \n",
    "    # Create decision boundary\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_moons[:, 0].min() - 0.5, X_moons[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_moons[:, 1].min() - 0.5, X_moons[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "    axes[idx].scatter(X_train_m[:, 0], X_train_m[:, 1], c=y_train_m, \n",
    "                     cmap='RdYlBu', edgecolors='black', s=50)\n",
    "    \n",
    "    accuracy = knn.score(X_test_m, y_test_m)\n",
    "    axes[idx].set_title(f'K = {k}\\nAccuracy = {accuracy:.3f}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Distance Weighting\n",
    "\n",
    "Give more weight to closer neighbors:\n",
    "\n",
    "$$w_i = \\frac{1}{d_i^2}$$\n",
    "\n",
    "where $d_i$ is the distance to neighbor $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare uniform vs distance weighting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, weights in enumerate(['uniform', 'distance']):\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights=weights)\n",
    "    knn.fit(X_train_m, y_train_m)\n",
    "    \n",
    "    # Decision boundary\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_moons[:, 0].min() - 0.5, X_moons[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_moons[:, 1].min() - 0.5, X_moons[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "    axes[idx].scatter(X_train_m[:, 0], X_train_m[:, 1], c=y_train_m, \n",
    "                     cmap='RdYlBu', edgecolors='black', s=50)\n",
    "    \n",
    "    accuracy = knn.score(X_test_m, y_test_m)\n",
    "    axes[idx].set_title(f'Weights: {weights}\\nAccuracy = {accuracy:.3f}', \n",
    "                       fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Feature 1', fontsize=12)\n",
    "    axes[idx].set_ylabel('Feature 2', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Curse of Dimensionality\n",
    "\n",
    "KNN struggles with high-dimensional data because:\n",
    "- Distances become less meaningful\n",
    "- All points become equidistant\n",
    "- Computational cost increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate curse of dimensionality\n",
    "dimensions = [2, 5, 10, 20, 50, 100]\n",
    "accuracies = []\n",
    "training_times = []\n",
    "\n",
    "for n_features in dimensions:\n",
    "    # Generate dataset\n",
    "    X_dim, y_dim = make_classification(n_samples=500, n_features=n_features, \n",
    "                                       n_informative=min(n_features, 10),\n",
    "                                       n_redundant=0, random_state=42)\n",
    "    \n",
    "    X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
    "        X_dim, y_dim, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    scaler_d = StandardScaler()\n",
    "    X_train_d_scaled = scaler_d.fit_transform(X_train_d)\n",
    "    X_test_d_scaled = scaler_d.transform(X_test_d)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    \n",
    "    import time\n",
    "    start = time.time()\n",
    "    knn.fit(X_train_d_scaled, y_train_d)\n",
    "    knn.predict(X_test_d_scaled)\n",
    "    end = time.time()\n",
    "    \n",
    "    accuracies.append(knn.score(X_test_d_scaled, y_test_d))\n",
    "    training_times.append(end - start)\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(dimensions, accuracies, 'b-o', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Dimensions', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Accuracy vs Dimensionality', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(dimensions, training_times, 'r-s', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Dimensions', fontsize=12)\n",
    "axes[1].set_ylabel('Time (seconds)', fontsize=12)\n",
    "axes[1].set_title('Computational Cost vs Dimensionality', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. KNN for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression data\n",
    "np.random.seed(42)\n",
    "X_reg = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
    "y_reg = np.sin(X_reg).ravel() + np.random.randn(100) * 0.1\n",
    "\n",
    "# Train KNN regressors with different K\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "X_test_reg = np.linspace(0, 5, 500)[:, np.newaxis]\n",
    "\n",
    "for idx, k in enumerate([1, 5, 20]):\n",
    "    knn_reg = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn_reg.fit(X_reg, y_reg)\n",
    "    y_pred_reg = knn_reg.predict(X_test_reg)\n",
    "    \n",
    "    axes[idx].scatter(X_reg, y_reg, color='blue', s=50, label='Training data')\n",
    "    axes[idx].plot(X_test_reg, y_pred_reg, 'r-', linewidth=2, label=f'KNN (K={k})')\n",
    "    axes[idx].plot(X_test_reg, np.sin(X_test_reg), 'g--', linewidth=2, label='True function')\n",
    "    axes[idx].set_xlabel('X', fontsize=12)\n",
    "    axes[idx].set_ylabel('y', fontsize=12)\n",
    "    axes[idx].set_title(f'KNN Regression (K={k})', fontsize=14)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "knn_grid = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn_grid, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test Score: {grid_search.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "# Visualize grid search results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "pivot_table = results_df.pivot_table(\n",
    "    values='mean_test_score', \n",
    "    index='param_n_neighbors',\n",
    "    columns='param_weights'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlGnBu')\n",
    "plt.title('Grid Search Results: Mean CV Accuracy', fontsize=14)\n",
    "plt.xlabel('Weights', fontsize=12)\n",
    "plt.ylabel('Number of Neighbors', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **KNN is lazy learning**: No training phase, all computation at prediction time\n",
    "2. **Distance matters**: Feature scaling is critical\n",
    "3. **K selection**: Use cross-validation to find optimal K\n",
    "4. **Distance weighting**: Can improve performance\n",
    "5. **Curse of dimensionality**: Struggles with high-dimensional data\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "**Pros:**\n",
    "- Simple and intuitive\n",
    "- No training required\n",
    "- Non-parametric (no assumptions about data)\n",
    "- Works for both classification and regression\n",
    "- Naturally handles multi-class problems\n",
    "\n",
    "**Cons:**\n",
    "- Slow prediction time (O(n) for each prediction)\n",
    "- Requires feature scaling\n",
    "- Suffers from curse of dimensionality\n",
    "- Memory intensive (stores all training data)\n",
    "- Sensitive to irrelevant features\n",
    "\n",
    "### When to Use KNN\n",
    "\n",
    "**Use when:**\n",
    "- Small to medium datasets\n",
    "- Low-dimensional feature space\n",
    "- Non-linear decision boundaries\n",
    "- Need interpretable results\n",
    "\n",
    "**Avoid when:**\n",
    "- Large datasets (slow prediction)\n",
    "- High-dimensional data\n",
    "- Real-time predictions needed\n",
    "- Many irrelevant features\n",
    "\n",
    "### Practice Problems\n",
    "\n",
    "1. Implement weighted KNN in the scratch version\n",
    "2. Compare KNN with other algorithms on imbalanced datasets\n",
    "3. Use KNN for anomaly detection\n",
    "4. Implement KNN with custom distance metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
