{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron - Complete Guide\n",
    "\n",
    "## From Neural Network Foundations to Implementation\n",
    "\n",
    "The Perceptron is the **simplest neural network** - a single-layer linear binary classifier. It's the foundation of modern deep learning.\n",
    "\n",
    "### What You'll Learn\n",
    "1. Perceptron algorithm and history\n",
    "2. Activation functions\n",
    "3. Learning rule and convergence\n",
    "4. Linear separability\n",
    "5. Implementation from scratch\n",
    "6. Multi-layer perceptron (MLP)\n",
    "7. XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_circles, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perceptron Model\n",
    "\n",
    "### Mathematical Model:\n",
    "$$y = f(\\mathbf{w}^T\\mathbf{x} + b)$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x}$ = input features\n",
    "- $\\mathbf{w}$ = weights\n",
    "- $b$ = bias\n",
    "- $f$ = activation function (step function)\n",
    "\n",
    "### Step Activation Function:\n",
    "$$f(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize perceptron architecture\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Step function\n",
    "z = np.linspace(-5, 5, 100)\n",
    "step = np.where(z >= 0, 1, 0)\n",
    "\n",
    "axes[0].plot(z, step, 'b-', linewidth=3)\n",
    "axes[0].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0].axvline(x=0, color='g', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('z = w·x + b', fontsize=12)\n",
    "axes[0].set_ylabel('Output', fontsize=12)\n",
    "axes[0].set_title('Step Activation Function', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(-0.1, 1.1)\n",
    "\n",
    "# Perceptron diagram\n",
    "axes[1].text(0.5, 0.9, 'Perceptron Architecture', ha='center', fontsize=16, fontweight='bold')\n",
    "axes[1].text(0.15, 0.7, 'x₁', ha='center', fontsize=12, bbox=dict(boxstyle='circle', facecolor='lightblue'))\n",
    "axes[1].text(0.15, 0.5, 'x₂', ha='center', fontsize=12, bbox=dict(boxstyle='circle', facecolor='lightblue'))\n",
    "axes[1].text(0.15, 0.3, 'x₃', ha='center', fontsize=12, bbox=dict(boxstyle='circle', facecolor='lightblue'))\n",
    "axes[1].text(0.5, 0.5, '∑', ha='center', fontsize=20, bbox=dict(boxstyle='circle', facecolor='yellow'))\n",
    "axes[1].text(0.75, 0.5, 'f', ha='center', fontsize=14, bbox=dict(boxstyle='circle', facecolor='lightgreen'))\n",
    "axes[1].text(0.9, 0.5, 'y', ha='center', fontsize=12, bbox=dict(boxstyle='circle', facecolor='coral'))\n",
    "\n",
    "# Arrows\n",
    "for y_pos, label in zip([0.7, 0.5, 0.3], ['w₁', 'w₂', 'w₃']):\n",
    "    axes[1].arrow(0.2, y_pos, 0.25, 0.5-y_pos, head_width=0.02, head_length=0.03, fc='black')\n",
    "    axes[1].text(0.3, (y_pos + 0.5)/2, label, fontsize=10, color='red')\n",
    "\n",
    "axes[1].arrow(0.55, 0.5, 0.15, 0, head_width=0.03, head_length=0.03, fc='black')\n",
    "axes[1].arrow(0.8, 0.5, 0.05, 0, head_width=0.03, head_length=0.03, fc='black')\n",
    "axes[1].text(0.5, 0.2, '+ b (bias)', fontsize=10, color='blue')\n",
    "axes[1].set_xlim(0, 1)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perceptron Learning Rule\n",
    "\n",
    "### Update Rule:\n",
    "$$w_i \\leftarrow w_i + \\eta(y_{true} - y_{pred})x_i$$\n",
    "$$b \\leftarrow b + \\eta(y_{true} - y_{pred})$$\n",
    "\n",
    "Where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronScratch:\n",
    "    \"\"\"Perceptron implementation from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.errors = []\n",
    "        \n",
    "    def _activation(self, x):\n",
    "        \"\"\"Step activation function\"\"\"\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the perceptron\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Convert labels to 0 and 1 if necessary\n",
    "        y_ = np.where(y <= 0, 0, 1)\n",
    "        \n",
    "        # Training\n",
    "        for iteration in range(self.n_iterations):\n",
    "            errors = 0\n",
    "            \n",
    "            for idx, x_i in enumerate(X):\n",
    "                # Calculate prediction\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                y_pred = self._activation(linear_output)\n",
    "                \n",
    "                # Update weights and bias\n",
    "                update = self.learning_rate * (y_[idx] - y_pred)\n",
    "                self.weights += update * x_i\n",
    "                self.bias += update\n",
    "                \n",
    "                # Count errors\n",
    "                errors += int(update != 0.0)\n",
    "            \n",
    "            self.errors.append(errors)\n",
    "            \n",
    "            # Early stopping if converged\n",
    "            if errors == 0:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return self._activation(linear_output)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate accuracy\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        y_ = np.where(y <= 0, 0, 1)\n",
    "        return np.mean(y_pred == y_)\n",
    "\n",
    "# Generate linearly separable data\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0,\n",
    "                          n_informative=2, n_clusters_per_class=1,\n",
    "                          flip_y=0, class_sep=2, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train perceptron\n",
    "perceptron = PerceptronScratch(learning_rate=0.1, n_iterations=100)\n",
    "perceptron.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {perceptron.score(X_train_scaled, y_train):.4f}\")\n",
    "print(f\"Test Accuracy: {perceptron.score(X_test_scaled, y_test):.4f}\")\n",
    "print(f\"Learned Weights: {perceptron.weights}\")\n",
    "print(f\"Learned Bias: {perceptron.bias:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress and decision boundary\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training errors\n",
    "axes[0].plot(range(1, len(perceptron.errors) + 1), perceptron.errors, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Misclassifications', fontsize=12)\n",
    "axes[0].set_title('Perceptron Learning Curve', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Decision boundary\n",
    "h = 0.02\n",
    "x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = perceptron.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axes[1].contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train,\n",
    "               cmap='RdYlBu', edgecolors='black', s=100)\n",
    "axes[1].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[1].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[1].set_title('Decision Boundary', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Separability\n",
    "\n",
    "**Key Limitation**: Perceptron can only learn **linearly separable** patterns.\n",
    "\n",
    "### Perceptron Convergence Theorem:\n",
    "If data is linearly separable, the perceptron is **guaranteed to converge** in finite steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate linearly separable vs non-separable\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Dataset 1: Linearly separable\n",
    "X1, y1 = make_classification(n_samples=100, n_features=2, n_redundant=0,\n",
    "                            n_informative=2, n_clusters_per_class=1,\n",
    "                            flip_y=0, class_sep=2, random_state=42)\n",
    "axes[0].scatter(X1[:, 0], X1[:, 1], c=y1, cmap='RdYlBu', edgecolors='black', s=100)\n",
    "axes[0].set_title('Linearly Separable\\n(Perceptron will converge)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "# Dataset 2: XOR (not linearly separable)\n",
    "X2 = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y2 = np.array([0, 1, 1, 0])  # XOR\n",
    "axes[1].scatter(X2[:, 0], X2[:, 1], c=y2, cmap='RdYlBu', edgecolors='black', s=300)\n",
    "axes[1].set_title('XOR Problem\\n(NOT Linearly Separable)', fontsize=12, fontweight='bold', color='red')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].set_xlim(-0.5, 1.5)\n",
    "axes[1].set_ylim(-0.5, 1.5)\n",
    "\n",
    "# Dataset 3: Circles (not linearly separable)\n",
    "X3, y3 = make_circles(n_samples=100, noise=0.1, factor=0.5, random_state=42)\n",
    "axes[2].scatter(X3[:, 0], X3[:, 1], c=y3, cmap='RdYlBu', edgecolors='black', s=100)\n",
    "axes[2].set_title('Concentric Circles\\n(NOT Linearly Separable)', fontsize=12, fontweight='bold', color='red')\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "axes[2].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The XOR Problem\n",
    "\n",
    "The XOR problem was historically significant - it showed the limitation of single-layer perceptrons and led to the development of **multi-layer neural networks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to learn XOR with single perceptron (will fail)\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "perceptron_xor = PerceptronScratch(learning_rate=0.1, n_iterations=1000)\n",
    "perceptron_xor.fit(X_xor, y_xor)\n",
    "\n",
    "print(\"Single Perceptron on XOR:\")\n",
    "print(f\"Accuracy: {perceptron_xor.score(X_xor, y_xor):.4f}\")\n",
    "print(f\"Predictions: {perceptron_xor.predict(X_xor)}\")\n",
    "print(f\"True labels: {y_xor}\")\n",
    "print(f\"\\nFinal errors in last 10 iterations: {perceptron_xor.errors[-10:]}\")\n",
    "print(\"\\n⚠️ Single perceptron CANNOT learn XOR!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scikit-learn Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn's Perceptron\n",
    "iris = load_iris()\n",
    "X_iris = iris.data[:100, :2]  # Use only 2 classes and 2 features\n",
    "y_iris = iris.target[:100]\n",
    "\n",
    "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler_i = StandardScaler()\n",
    "X_train_i_scaled = scaler_i.fit_transform(X_train_i)\n",
    "X_test_i_scaled = scaler_i.transform(X_test_i)\n",
    "\n",
    "# Train sklearn perceptron\n",
    "sklearn_perceptron = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sklearn_perceptron.fit(X_train_i_scaled, y_train_i)\n",
    "\n",
    "y_pred_i = sklearn_perceptron.predict(X_test_i_scaled)\n",
    "\n",
    "print(f\"Sklearn Perceptron Accuracy: {accuracy_score(y_test_i, y_pred_i):.4f}\")\n",
    "print(f\"\\nWeights: {sklearn_perceptron.coef_[0]}\")\n",
    "print(f\"Bias: {sklearn_perceptron.intercept_[0]:.4f}\")\n",
    "print(f\"Number of iterations: {sklearn_perceptron.n_iter_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Layer Perceptron (MLP)\n",
    "\n",
    "**Solution to XOR**: Add hidden layers!\n",
    "\n",
    "MLP can learn non-linear decision boundaries by stacking multiple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve XOR with MLP\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(4,), activation='relu', \n",
    "                    max_iter=10000, random_state=42)\n",
    "mlp.fit(X_xor, y_xor)\n",
    "\n",
    "print(\"Multi-Layer Perceptron on XOR:\")\n",
    "print(f\"Accuracy: {mlp.score(X_xor, y_xor):.4f}\")\n",
    "print(f\"Predictions: {mlp.predict(X_xor)}\")\n",
    "print(f\"True labels: {y_xor}\")\n",
    "print(\"\\n✓ MLP successfully learns XOR!\")\n",
    "\n",
    "# Visualize MLP decision boundary\n",
    "h = 0.01\n",
    "x_min, x_max = -0.5, 1.5\n",
    "y_min, y_max = -0.5, 1.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "Z_mlp = mlp.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_mlp = Z_mlp.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z_mlp, alpha=0.4, cmap='RdYlBu')\n",
    "plt.scatter(X_xor[:, 0], X_xor[:, 1], c=y_xor, cmap='RdYlBu', \n",
    "           edgecolors='black', s=300, linewidth=2)\n",
    "plt.xlabel('Input 1', fontsize=12)\n",
    "plt.ylabel('Input 2', fontsize=12)\n",
    "plt.title('MLP Solution to XOR Problem', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(label='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison: Perceptron vs MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate various datasets\n",
    "datasets = [\n",
    "    make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2,\n",
    "                       n_clusters_per_class=1, flip_y=0, class_sep=2, random_state=42),\n",
    "    make_circles(n_samples=100, noise=0.1, factor=0.5, random_state=42),\n",
    "    (X_xor, y_xor)\n",
    "]\n",
    "\n",
    "titles = ['Linearly Separable', 'Circles', 'XOR']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "for idx, (X_data, y_data) in enumerate(datasets):\n",
    "    # Train both models\n",
    "    perc = PerceptronScratch(learning_rate=0.1, n_iterations=1000)\n",
    "    perc.fit(X_data, y_data)\n",
    "    \n",
    "    mlp_model = MLPClassifier(hidden_layer_sizes=(10,), max_iter=10000, random_state=42)\n",
    "    mlp_model.fit(X_data, y_data)\n",
    "    \n",
    "    # Create mesh\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_data[:, 0].min() - 0.5, X_data[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_data[:, 1].min() - 0.5, X_data[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Perceptron\n",
    "    Z_p = perc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z_p = Z_p.reshape(xx.shape)\n",
    "    \n",
    "    axes[0, idx].contourf(xx, yy, Z_p, alpha=0.4, cmap='RdYlBu')\n",
    "    axes[0, idx].scatter(X_data[:, 0], X_data[:, 1], c=y_data, cmap='RdYlBu',\n",
    "                        edgecolors='black', s=50)\n",
    "    axes[0, idx].set_title(f'Perceptron: {titles[idx]}\\nAcc={perc.score(X_data, y_data):.2f}',\n",
    "                          fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # MLP\n",
    "    Z_m = mlp_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z_m = Z_m.reshape(xx.shape)\n",
    "    \n",
    "    axes[1, idx].contourf(xx, yy, Z_m, alpha=0.4, cmap='RdYlBu')\n",
    "    axes[1, idx].scatter(X_data[:, 0], X_data[:, 1], c=y_data, cmap='RdYlBu',\n",
    "                        edgecolors='black', s=50)\n",
    "    axes[1, idx].set_title(f'MLP: {titles[idx]}\\nAcc={mlp_model.score(X_data, y_data):.2f}',\n",
    "                          fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Activation Functions Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different activation functions\n",
    "z = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Activation functions\n",
    "step = np.where(z >= 0, 1, 0)\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "tanh = np.tanh(z)\n",
    "relu = np.maximum(0, z)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "activations = [step, sigmoid, tanh, relu]\n",
    "names = ['Step (Perceptron)', 'Sigmoid', 'Tanh', 'ReLU']\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "\n",
    "for idx, (activation, name, color) in enumerate(zip(activations, names, colors)):\n",
    "    axes[idx].plot(z, activation, color=color, linewidth=3)\n",
    "    axes[idx].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[idx].axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[idx].set_xlabel('z', fontsize=12)\n",
    "    axes[idx].set_ylabel('f(z)', fontsize=12)\n",
    "    axes[idx].set_title(name, fontsize=14, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Activation Functions:\")\n",
    "print(\"- Step: Used in classic perceptron (non-differentiable)\")\n",
    "print(\"- Sigmoid: Smooth, outputs [0,1], used in binary classification\")\n",
    "print(\"- Tanh: Smooth, outputs [-1,1], zero-centered\")\n",
    "print(\"- ReLU: Fast, most popular in deep learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Perceptron**: Simplest neural network, linear binary classifier\n",
    "2. **Learning Rule**: Updates weights based on misclassifications\n",
    "3. **Convergence**: Guaranteed for linearly separable data\n",
    "4. **Limitation**: Cannot learn non-linear patterns (XOR problem)\n",
    "5. **Solution**: Multi-layer perceptron with hidden layers\n",
    "6. **Historical Importance**: Foundation of deep learning\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "**Pros:**\n",
    "- Simple and fast\n",
    "- Online learning capable\n",
    "- No hyperparameters (except learning rate)\n",
    "- Guaranteed convergence for linearly separable data\n",
    "- Foundation for understanding neural networks\n",
    "\n",
    "**Cons:**\n",
    "- Only works for linearly separable data\n",
    "- No probability outputs (hard predictions)\n",
    "- Sensitive to feature scaling\n",
    "- Can oscillate without convergence for non-separable data\n",
    "- Superseded by better algorithms\n",
    "\n",
    "### When to Use Perceptron\n",
    "\n",
    "**Use when:**\n",
    "- Data is linearly separable\n",
    "- Need fast, simple baseline\n",
    "- Online learning required\n",
    "- Teaching/understanding neural networks\n",
    "\n",
    "**Avoid when:**\n",
    "- Non-linear relationships exist\n",
    "- Need probability outputs\n",
    "- Data is not linearly separable\n",
    "\n",
    "### Practice Problems\n",
    "\n",
    "1. Implement perceptron with different learning rates\n",
    "2. Modify to use sigmoid activation (logistic regression)\n",
    "3. Build a 2-layer network to solve XOR from scratch\n",
    "4. Compare convergence speed on different datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
