{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Guide to Regression Evaluation Metrics\n",
    "## MSE, RMSE, MAE, R¬≤, and More\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "1. Why we need evaluation metrics\n",
    "2. Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\n",
    "3. Mean Absolute Error (MAE)\n",
    "4. R¬≤ Score (Coefficient of Determination)\n",
    "5. When to use which metric\n",
    "6. Advanced metrics (MAPE, MSLE, Adjusted R¬≤)\n",
    "7. Residual analysis\n",
    "8. Best practices for model evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Do We Need Evaluation Metrics?\n",
    "\n",
    "### The Core Question\n",
    "\n",
    "**How do we measure if our model is good?**\n",
    "\n",
    "We need metrics to:\n",
    "1. **Quantify performance**: Convert \"good\" into numbers\n",
    "2. **Compare models**: Which model is better?\n",
    "3. **Track improvements**: Is our new model better than the old one?\n",
    "4. **Set goals**: \"We need R¬≤ > 0.8\"\n",
    "5. **Communicate results**: Report to stakeholders\n",
    "\n",
    "### Prediction Error\n",
    "\n",
    "For a single prediction:\n",
    "$$\\text{Error}_i = y_i - \\hat{y}_i$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ : true value\n",
    "- $\\hat{y}_i$ : predicted value\n",
    "- Positive error: underprediction\n",
    "- Negative error: overprediction\n",
    "\n",
    "But we need to aggregate errors across all predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_regression, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    "    max_error\n",
    ")\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate sample data\n",
    "X = np.linspace(0, 10, 50).reshape(-1, 1)\n",
    "y_true = 2 * X.flatten() + 3\n",
    "y_pred = y_true + np.random.randn(50) * 1.5\n",
    "\n",
    "# Visualize predictions\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Predictions vs True\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X, y_true, color='green', s=50, alpha=0.6, label='True values')\n",
    "plt.scatter(X, y_pred, color='blue', s=50, alpha=0.6, label='Predictions')\n",
    "for i in range(len(X)):\n",
    "    plt.plot([X[i], X[i]], [y_true[i], y_pred[i]], 'r--', alpha=0.3, linewidth=1)\n",
    "plt.xlabel('Feature (x)', fontsize=11)\n",
    "plt.ylabel('Target (y)', fontsize=11)\n",
    "plt.title('Predictions vs True Values\\n(Red lines = errors)', fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "errors = y_true - y_pred\n",
    "plt.hist(errors, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero error')\n",
    "plt.axvline(x=np.mean(errors), color='green', linestyle='--', linewidth=2, \n",
    "           label=f'Mean error: {np.mean(errors):.2f}')\n",
    "plt.xlabel('Error (True - Predicted)', fontsize=11)\n",
    "plt.ylabel('Frequency', fontsize=11)\n",
    "plt.title('Distribution of Errors', fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insight:\")\n",
    "print(\"‚Ä¢ Individual errors vary (some positive, some negative)\")\n",
    "print(\"‚Ä¢ We need a single number to summarize overall performance\")\n",
    "print(\"‚Ä¢ Different metrics emphasize different aspects of errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Mean Squared Error (MSE)\n",
    "\n",
    "### Definition\n",
    "\n",
    "**MSE** is the average of squared errors:\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "### Properties:\n",
    "1. **Always positive** (errors are squared)\n",
    "2. **Units**: squared units of target (e.g., dollars¬≤)\n",
    "3. **Penalizes large errors more** than small errors\n",
    "4. **Differentiable**: Good for optimization\n",
    "5. **Lower is better**: 0 = perfect predictions\n",
    "\n",
    "### Why Square Errors?\n",
    "- Positive and negative errors don't cancel out\n",
    "- Penalizes outliers heavily\n",
    "- Mathematically convenient (differentiable)\n",
    "- Related to variance\n",
    "\n",
    "### Advantages:\n",
    "- ‚úÖ Most common metric\n",
    "- ‚úÖ Good for optimization\n",
    "- ‚úÖ Heavily penalizes large errors\n",
    "- ‚úÖ Theoretical foundation (Gaussian assumption)\n",
    "\n",
    "### Disadvantages:\n",
    "- ‚ùå Not in original units\n",
    "- ‚ùå Sensitive to outliers\n",
    "- ‚ùå Hard to interpret magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE from scratch\n",
    "def mse_from_scratch(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error\n",
    "    \"\"\"\n",
    "    errors = y_true - y_pred\n",
    "    squared_errors = errors ** 2\n",
    "    mse = np.mean(squared_errors)\n",
    "    return mse\n",
    "\n",
    "# Test implementation\n",
    "print(\"=== MEAN SQUARED ERROR (MSE) ===\")\n",
    "\n",
    "# Simple example\n",
    "y_true = np.array([3, -0.5, 2, 7])\n",
    "y_pred = np.array([2.5, 0.0, 2, 8])\n",
    "\n",
    "print(\"\\nSimple Example:\")\n",
    "print(f\"True values:     {y_true}\")\n",
    "print(f\"Predictions:     {y_pred}\")\n",
    "print(f\"Errors:          {y_true - y_pred}\")\n",
    "print(f\"Squared errors:  {(y_true - y_pred)**2}\")\n",
    "\n",
    "mse_manual = mse_from_scratch(y_true, y_pred)\n",
    "mse_sklearn = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nMSE (manual):    {mse_manual:.4f}\")\n",
    "print(f\"MSE (sklearn):   {mse_sklearn:.4f}\")\n",
    "print(f\"Match: {np.isclose(mse_manual, mse_sklearn)}\")\n",
    "\n",
    "# Visualize how MSE penalizes errors\n",
    "errors_range = np.linspace(-5, 5, 100)\n",
    "squared_errors = errors_range ** 2\n",
    "absolute_errors = np.abs(errors_range)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss functions\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(errors_range, squared_errors, 'b-', linewidth=2, label='Squared (MSE)')\n",
    "plt.plot(errors_range, absolute_errors, 'r-', linewidth=2, label='Absolute (MAE)')\n",
    "plt.xlabel('Error', fontsize=11)\n",
    "plt.ylabel('Loss', fontsize=11)\n",
    "plt.title('MSE vs MAE: How They Penalize Errors', fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "# MSE calculation breakdown\n",
    "plt.subplot(1, 2, 2)\n",
    "x_pos = np.arange(len(y_true))\n",
    "errors = y_true - y_pred\n",
    "squared_errors = errors ** 2\n",
    "\n",
    "plt.bar(x_pos, squared_errors, alpha=0.7, edgecolor='black')\n",
    "plt.axhline(y=mse_manual, color='red', linestyle='--', linewidth=2,\n",
    "           label=f'MSE = {mse_manual:.3f}')\n",
    "plt.xlabel('Sample Index', fontsize=11)\n",
    "plt.ylabel('Squared Error', fontsize=11)\n",
    "plt.title('Squared Errors per Sample', fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"‚Ä¢ Squared loss penalizes large errors much more than small ones\")\n",
    "print(\"‚Ä¢ Error of 2 contributes 4 to MSE\")\n",
    "print(\"‚Ä¢ Error of 4 contributes 16 to MSE (4√ó worse)\")\n",
    "print(\"‚Ä¢ MSE is very sensitive to outliers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Root Mean Squared Error (RMSE)\n",
    "\n",
    "### Definition\n",
    "\n",
    "**RMSE** is the square root of MSE:\n",
    "\n",
    "$$RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "### Properties:\n",
    "1. **Same units as target**: If predicting dollars, RMSE is in dollars\n",
    "2. **Still penalizes large errors**: Due to squaring before averaging\n",
    "3. **More interpretable** than MSE\n",
    "4. **Lower is better**: 0 = perfect predictions\n",
    "\n",
    "### Interpretation:\n",
    "RMSE represents the **typical prediction error** in the original units.\n",
    "\n",
    "Example: RMSE = $5,000 for house prices\n",
    "- Typical prediction is off by about $5,000\n",
    "\n",
    "### Advantages:\n",
    "- ‚úÖ In original units (interpretable)\n",
    "- ‚úÖ Penalizes large errors\n",
    "- ‚úÖ More intuitive than MSE\n",
    "\n",
    "### Disadvantages:\n",
    "- ‚ùå Still sensitive to outliers\n",
    "- ‚ùå Not scale-invariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE from scratch\n",
    "def rmse_from_scratch(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Root Mean Squared Error\n",
    "    \"\"\"\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "print(\"=== ROOT MEAN SQUARED ERROR (RMSE) ===\")\n",
    "\n",
    "# House price example\n",
    "y_true_prices = np.array([300000, 450000, 250000, 600000, 350000])\n",
    "y_pred_prices = np.array([310000, 440000, 260000, 580000, 355000])\n",
    "\n",
    "mse_prices = mean_squared_error(y_true_prices, y_pred_prices)\n",
    "rmse_prices = np.sqrt(mse_prices)\n",
    "\n",
    "print(\"\\nHouse Price Predictions:\")\n",
    "print(f\"True prices: {y_true_prices}\")\n",
    "print(f\"Predictions: {y_pred_prices}\")\n",
    "print(f\"Errors:      {y_true_prices - y_pred_prices}\")\n",
    "\n",
    "print(f\"\\nMSE:  ${mse_prices:,.0f}¬≤ (hard to interpret!)\")\n",
    "print(f\"RMSE: ${rmse_prices:,.0f} (typical error)\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  On average, predictions are off by about ${rmse_prices:,.0f}\")\n",
    "\n",
    "# Demonstrate outlier sensitivity\n",
    "print(\"\\n=== OUTLIER SENSITIVITY ===\")\n",
    "\n",
    "y_true_no_outlier = np.array([10, 12, 11, 13, 12])\n",
    "y_pred_no_outlier = np.array([10.5, 11.5, 11.5, 12.5, 11.5])\n",
    "\n",
    "y_true_with_outlier = np.array([10, 12, 11, 13, 100])  # Added outlier\n",
    "y_pred_with_outlier = np.array([10.5, 11.5, 11.5, 12.5, 11.5])\n",
    "\n",
    "rmse_no_outlier = rmse_from_scratch(y_true_no_outlier, y_pred_no_outlier)\n",
    "rmse_with_outlier = rmse_from_scratch(y_true_with_outlier, y_pred_with_outlier)\n",
    "\n",
    "print(f\"\\nWithout outlier: RMSE = {rmse_no_outlier:.3f}\")\n",
    "print(f\"With outlier:    RMSE = {rmse_with_outlier:.3f}\")\n",
    "print(f\"\\nOutlier increased RMSE by {(rmse_with_outlier/rmse_no_outlier - 1)*100:.1f}%!\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# No outlier\n",
    "axes[0].scatter(range(len(y_true_no_outlier)), y_true_no_outlier, \n",
    "               color='green', s=100, label='True', zorder=3)\n",
    "axes[0].scatter(range(len(y_pred_no_outlier)), y_pred_no_outlier,\n",
    "               color='blue', s=100, label='Predicted', zorder=3)\n",
    "for i in range(len(y_true_no_outlier)):\n",
    "    axes[0].plot([i, i], [y_true_no_outlier[i], y_pred_no_outlier[i]], \n",
    "                'r--', linewidth=2, alpha=0.5)\n",
    "axes[0].set_xlabel('Sample Index', fontsize=11)\n",
    "axes[0].set_ylabel('Value', fontsize=11)\n",
    "axes[0].set_title(f'Without Outlier\\nRMSE = {rmse_no_outlier:.3f}',\n",
    "                 fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# With outlier\n",
    "axes[1].scatter(range(len(y_true_with_outlier)), y_true_with_outlier,\n",
    "               color='green', s=100, label='True', zorder=3)\n",
    "axes[1].scatter(range(len(y_pred_with_outlier)), y_pred_with_outlier,\n",
    "               color='blue', s=100, label='Predicted', zorder=3)\n",
    "for i in range(len(y_true_with_outlier)):\n",
    "    axes[1].plot([i, i], [y_true_with_outlier[i], y_pred_with_outlier[i]],\n",
    "                'r--', linewidth=2, alpha=0.5)\n",
    "axes[1].scatter([4], [100], color='red', s=200, marker='*', \n",
    "               label='Outlier!', zorder=4)\n",
    "axes[1].set_xlabel('Sample Index', fontsize=11)\n",
    "axes[1].set_ylabel('Value', fontsize=11)\n",
    "axes[1].set_title(f'With Outlier\\nRMSE = {rmse_with_outlier:.3f} (Much Higher!)',\n",
    "                 fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  RMSE is very sensitive to outliers due to squaring!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Mean Absolute Error (MAE)\n",
    "\n",
    "### Definition\n",
    "\n",
    "**MAE** is the average of absolute errors:\n",
    "\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$$\n",
    "\n",
    "### Properties:\n",
    "1. **Same units as target**: Direct interpretation\n",
    "2. **Linear penalty**: All errors weighted equally\n",
    "3. **Robust to outliers**: Less sensitive than MSE/RMSE\n",
    "4. **Lower is better**: 0 = perfect predictions\n",
    "\n",
    "### MAE vs RMSE:\n",
    "\n",
    "| Property | MAE | RMSE |\n",
    "|----------|-----|------|\n",
    "| Outlier sensitivity | Low | High |\n",
    "| Error penalty | Linear | Quadratic |\n",
    "| Interpretability | High | High |\n",
    "| Differentiability | Not at zero | Everywhere |\n",
    "\n",
    "### Advantages:\n",
    "- ‚úÖ Robust to outliers\n",
    "- ‚úÖ Easy to interpret\n",
    "- ‚úÖ In original units\n",
    "- ‚úÖ All errors weighted equally\n",
    "\n",
    "### Disadvantages:\n",
    "- ‚ùå Not differentiable at zero\n",
    "- ‚ùå Doesn't penalize large errors heavily\n",
    "- ‚ùå Less commonly used in optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE from scratch\n",
    "def mae_from_scratch(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Error\n",
    "    \"\"\"\n",
    "    errors = y_true - y_pred\n",
    "    absolute_errors = np.abs(errors)\n",
    "    mae = np.mean(absolute_errors)\n",
    "    return mae\n",
    "\n",
    "print(\"=== MEAN ABSOLUTE ERROR (MAE) ===\")\n",
    "\n",
    "# Compare MAE and RMSE\n",
    "y_true = np.array([3, -0.5, 2, 7])\n",
    "y_pred = np.array([2.5, 0.0, 2, 8])\n",
    "\n",
    "mae = mae_from_scratch(y_true, y_pred)\n",
    "rmse = rmse_from_scratch(y_true, y_pred)\n",
    "\n",
    "print(\"\\nSimple Example:\")\n",
    "print(f\"True values:      {y_true}\")\n",
    "print(f\"Predictions:      {y_pred}\")\n",
    "print(f\"Errors:           {y_true - y_pred}\")\n",
    "print(f\"Absolute errors:  {np.abs(y_true - y_pred)}\")\n",
    "\n",
    "print(f\"\\nMAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"\\nRMSE ‚â• MAE always (due to squaring)\")\n",
    "\n",
    "# Outlier comparison\n",
    "print(\"\\n=== MAE vs RMSE: OUTLIER ROBUSTNESS ===\")\n",
    "\n",
    "# Without outlier\n",
    "y_true_clean = np.array([10, 12, 11, 13, 12])\n",
    "y_pred_clean = np.array([10.5, 11.5, 11.5, 12.5, 11.5])\n",
    "\n",
    "# With outlier\n",
    "y_true_outlier = np.array([10, 12, 11, 13, 100])\n",
    "y_pred_outlier = np.array([10.5, 11.5, 11.5, 12.5, 11.5])\n",
    "\n",
    "# Calculate metrics\n",
    "mae_clean = mae_from_scratch(y_true_clean, y_pred_clean)\n",
    "rmse_clean = rmse_from_scratch(y_true_clean, y_pred_clean)\n",
    "\n",
    "mae_outlier = mae_from_scratch(y_true_outlier, y_pred_outlier)\n",
    "rmse_outlier = rmse_from_scratch(y_true_outlier, y_pred_outlier)\n",
    "\n",
    "print(\"\\nWithout outlier:\")\n",
    "print(f\"  MAE:  {mae_clean:.3f}\")\n",
    "print(f\"  RMSE: {rmse_clean:.3f}\")\n",
    "\n",
    "print(\"\\nWith outlier:\")\n",
    "print(f\"  MAE:  {mae_outlier:.3f}\")\n",
    "print(f\"  RMSE: {rmse_outlier:.3f}\")\n",
    "\n",
    "mae_increase = (mae_outlier / mae_clean - 1) * 100\n",
    "rmse_increase = (rmse_outlier / rmse_clean - 1) * 100\n",
    "\n",
    "print(f\"\\nIncrease due to outlier:\")\n",
    "print(f\"  MAE:  +{mae_increase:.1f}%\")\n",
    "print(f\"  RMSE: +{rmse_increase:.1f}%\")\n",
    "print(f\"\\n‚úì MAE is more robust to outliers!\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Metric comparison\n",
    "metrics = ['MAE', 'RMSE']\n",
    "clean_values = [mae_clean, rmse_clean]\n",
    "outlier_values = [mae_outlier, rmse_outlier]\n",
    "\n",
    "x_pos = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x_pos - width/2, clean_values, width, label='Without outlier', alpha=0.8)\n",
    "axes[0].bar(x_pos + width/2, outlier_values, width, label='With outlier', alpha=0.8)\n",
    "axes[0].set_xlabel('Metric', fontsize=11)\n",
    "axes[0].set_ylabel('Value', fontsize=11)\n",
    "axes[0].set_title('MAE vs RMSE: Outlier Sensitivity', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Percentage increase\n",
    "increases = [mae_increase, rmse_increase]\n",
    "colors = ['green' if i < 50 else 'orange' if i < 100 else 'red' for i in increases]\n",
    "\n",
    "axes[1].bar(metrics, increases, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Metric', fontsize=11)\n",
    "axes[1].set_ylabel('Percentage Increase (%)', fontsize=11)\n",
    "axes[1].set_title('Impact of Outlier', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, (metric, increase) in enumerate(zip(metrics, increases)):\n",
    "    axes[1].text(i, increase + 5, f'+{increase:.0f}%', \n",
    "                ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. R¬≤ Score (Coefficient of Determination)\n",
    "\n",
    "### Definition\n",
    "\n",
    "**R¬≤** measures the proportion of variance explained by the model:\n",
    "\n",
    "$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}$$\n",
    "\n",
    "Where:\n",
    "- $SS_{res}$ : Residual sum of squares (model error)\n",
    "- $SS_{tot}$ : Total sum of squares (variance in data)\n",
    "- $\\bar{y}$ : Mean of true values\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- **R¬≤ = 1**: Perfect predictions (all variance explained)\n",
    "- **R¬≤ = 0**: Model no better than predicting the mean\n",
    "- **R¬≤ < 0**: Model worse than predicting the mean!\n",
    "- **R¬≤ = 0.8**: Model explains 80% of variance\n",
    "\n",
    "### Properties:\n",
    "1. **Scale-invariant**: Same R¬≤ regardless of units\n",
    "2. **Relative measure**: Compares to baseline (mean)\n",
    "3. **Easy to interpret**: Percentage of variance explained\n",
    "4. **Can be negative**: For very bad models\n",
    "\n",
    "### Advantages:\n",
    "- ‚úÖ Normalized (between -‚àû and 1)\n",
    "- ‚úÖ Easy to interpret\n",
    "- ‚úÖ Scale-invariant\n",
    "- ‚úÖ Compares to baseline\n",
    "\n",
    "### Disadvantages:\n",
    "- ‚ùå Can be misleading with small samples\n",
    "- ‚ùå Always increases with more features\n",
    "- ‚ùå Not suitable for all regression problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R¬≤ from scratch\n",
    "def r2_from_scratch(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate R¬≤ Score\n",
    "    \"\"\"\n",
    "    # Residual sum of squares\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    \n",
    "    # Total sum of squares\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    \n",
    "    # R¬≤\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return r2, ss_res, ss_tot\n",
    "\n",
    "print(\"=== R¬≤ SCORE (COEFFICIENT OF DETERMINATION) ===\")\n",
    "\n",
    "# Example with different model qualities\n",
    "np.random.seed(42)\n",
    "X_demo = np.linspace(0, 10, 50)\n",
    "y_true_demo = 2 * X_demo + 3\n",
    "\n",
    "# Perfect model\n",
    "y_pred_perfect = y_true_demo\n",
    "\n",
    "# Good model\n",
    "y_pred_good = y_true_demo + np.random.randn(50) * 1\n",
    "\n",
    "# Poor model\n",
    "y_pred_poor = y_true_demo + np.random.randn(50) * 5\n",
    "\n",
    "# Baseline (mean)\n",
    "y_pred_baseline = np.full_like(y_true_demo, np.mean(y_true_demo))\n",
    "\n",
    "# Terrible model\n",
    "y_pred_terrible = y_true_demo + np.random.randn(50) * 10\n",
    "\n",
    "# Calculate R¬≤ for each\n",
    "r2_perfect, _, _ = r2_from_scratch(y_true_demo, y_pred_perfect)\n",
    "r2_good, _, _ = r2_from_scratch(y_true_demo, y_pred_good)\n",
    "r2_poor, _, _ = r2_from_scratch(y_true_demo, y_pred_poor)\n",
    "r2_baseline, _, _ = r2_from_scratch(y_true_demo, y_pred_baseline)\n",
    "r2_terrible, _, _ = r2_from_scratch(y_true_demo, y_pred_terrible)\n",
    "\n",
    "print(\"\\nR¬≤ for Different Model Qualities:\")\n",
    "print(f\"Perfect model:    R¬≤ = {r2_perfect:.4f} (explains 100% of variance)\")\n",
    "print(f\"Good model:       R¬≤ = {r2_good:.4f} (explains {r2_good*100:.1f}% of variance)\")\n",
    "print(f\"Poor model:       R¬≤ = {r2_poor:.4f} (explains {r2_poor*100:.1f}% of variance)\")\n",
    "print(f\"Baseline (mean):  R¬≤ = {r2_baseline:.4f} (no better than guessing mean)\")\n",
    "print(f\"Terrible model:   R¬≤ = {r2_terrible:.4f} (WORSE than guessing mean!)\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "models = [\n",
    "    ('Perfect', y_pred_perfect, r2_perfect),\n",
    "    ('Good', y_pred_good, r2_good),\n",
    "    ('Poor', y_pred_poor, r2_poor),\n",
    "    ('Baseline (Mean)', y_pred_baseline, r2_baseline),\n",
    "    ('Terrible', y_pred_terrible, r2_terrible)\n",
    "]\n",
    "\n",
    "for ax, (name, y_pred, r2) in zip(axes[:-1], models):\n",
    "    ax.scatter(y_true_demo, y_pred, alpha=0.6, s=30)\n",
    "    ax.plot([y_true_demo.min(), y_true_demo.max()],\n",
    "           [y_true_demo.min(), y_true_demo.max()],\n",
    "           'r--', linewidth=2, label='Perfect prediction')\n",
    "    ax.set_xlabel('True Values', fontsize=10)\n",
    "    ax.set_ylabel('Predictions', fontsize=10)\n",
    "    ax.set_title(f'{name} Model\\nR¬≤ = {r2:.3f}',\n",
    "                fontsize=11, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# R¬≤ interpretation guide\n",
    "axes[-1].axis('off')\n",
    "interpretation = \"\"\"\n",
    "R¬≤ INTERPRETATION GUIDE\n",
    "\n",
    "R¬≤ = 1.0 ‚Üí Perfect predictions\n",
    "           All variance explained\n",
    "\n",
    "R¬≤ = 0.9 ‚Üí Excellent model\n",
    "           90% variance explained\n",
    "\n",
    "R¬≤ = 0.7 ‚Üí Good model\n",
    "           70% variance explained\n",
    "\n",
    "R¬≤ = 0.5 ‚Üí Moderate model\n",
    "           50% variance explained\n",
    "\n",
    "R¬≤ = 0.0 ‚Üí Baseline (mean)\n",
    "           No better than average\n",
    "\n",
    "R¬≤ < 0.0 ‚Üí Bad model\n",
    "           Worse than guessing mean!\n",
    "\n",
    "Context matters:\n",
    "‚Ä¢ Social sciences: R¬≤>0.3 good\n",
    "‚Ä¢ Physical sciences: R¬≤>0.9 expected\n",
    "‚Ä¢ Finance: R¬≤>0.1 can be useful\n",
    "\"\"\"\n",
    "axes[-1].text(0.1, 0.95, interpretation, transform=axes[-1].transAxes,\n",
    "             fontsize=10, verticalalignment='top', family='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposing R¬≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize R¬≤ decomposition\n",
    "print(\"=== R¬≤ DECOMPOSITION ===\")\n",
    "\n",
    "# Simple example\n",
    "y_true_simple = np.array([10, 15, 20, 25, 30])\n",
    "y_pred_simple = np.array([12, 14, 21, 24, 29])\n",
    "y_mean = np.mean(y_true_simple)\n",
    "\n",
    "# Calculate components\n",
    "ss_tot = np.sum((y_true_simple - y_mean) ** 2)\n",
    "ss_res = np.sum((y_true_simple - y_pred_simple) ** 2)\n",
    "ss_reg = np.sum((y_pred_simple - y_mean) ** 2)\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"\\nData: {y_true_simple}\")\n",
    "print(f\"Predictions: {y_pred_simple}\")\n",
    "print(f\"Mean: {y_mean:.1f}\")\n",
    "\n",
    "print(f\"\\nSum of Squares:\")\n",
    "print(f\"  SS_tot (Total):      {ss_tot:.1f} (variance to explain)\")\n",
    "print(f\"  SS_reg (Explained):  {ss_reg:.1f} (variance explained by model)\")\n",
    "print(f\"  SS_res (Residual):   {ss_res:.1f} (unexplained variance)\")\n",
    "print(f\"\\n  SS_tot = SS_reg + SS_res\")\n",
    "print(f\"  {ss_tot:.1f} = {ss_reg:.1f} + {ss_res:.1f} ‚úì\")\n",
    "\n",
    "print(f\"\\nR¬≤ = 1 - (SS_res / SS_tot)\")\n",
    "print(f\"R¬≤ = 1 - ({ss_res:.1f} / {ss_tot:.1f})\")\n",
    "print(f\"R¬≤ = {r2:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  Model explains {r2*100:.1f}% of the variance\")\n",
    "print(f\"  {(1-r2)*100:.1f}% remains unexplained (residual)\")\n",
    "\n",
    "# Visualize decomposition\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "x_pos = np.arange(len(y_true_simple))\n",
    "\n",
    "# Total variance (from mean)\n",
    "axes[0].bar(x_pos, y_true_simple - y_mean, alpha=0.7, color='blue')\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=2, label=f'Mean = {y_mean:.1f}')\n",
    "axes[0].set_xlabel('Sample', fontsize=11)\n",
    "axes[0].set_ylabel('Deviation from Mean', fontsize=11)\n",
    "axes[0].set_title(f'Total Variance (SS_tot = {ss_tot:.1f})',\n",
    "                 fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Explained variance (predictions from mean)\n",
    "axes[1].bar(x_pos, y_pred_simple - y_mean, alpha=0.7, color='green')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', linewidth=2, label=f'Mean = {y_mean:.1f}')\n",
    "axes[1].set_xlabel('Sample', fontsize=11)\n",
    "axes[1].set_ylabel('Prediction Deviation from Mean', fontsize=11)\n",
    "axes[1].set_title(f'Explained by Model (SS_reg = {ss_reg:.1f})',\n",
    "                 fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Residual variance (true - predicted)\n",
    "axes[2].bar(x_pos, y_true_simple - y_pred_simple, alpha=0.7, color='red')\n",
    "axes[2].axhline(y=0, color='black', linestyle='--', linewidth=2)\n",
    "axes[2].set_xlabel('Sample', fontsize=11)\n",
    "axes[2].set_ylabel('Residual (Error)', fontsize=11)\n",
    "axes[2].set_title(f'Unexplained (SS_res = {ss_res:.1f})',\n",
    "                 fontsize=12, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. When to Use Which Metric?\n",
    "\n",
    "### Decision Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"METRIC SELECTION GUIDE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä METRIC COMPARISON\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Metric':<12} {'Units':<18} {'Outliers':<12} {'Interpret':<12} {'Use When'}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'MSE':<12} {'Target¬≤':<18} {'Sensitive':<12} {'Hard':<12} {'Optimization'}\")\n",
    "print(f\"{'RMSE':<12} {'Same as target':<18} {'Sensitive':<12} {'Easy':<12} {'Reporting'}\")\n",
    "print(f\"{'MAE':<12} {'Same as target':<18} {'Robust':<12} {'Easy':<12} {'Outliers present'}\")\n",
    "print(f\"{'R¬≤':<12} {'Normalized':<18} {'Sensitive':<12} {'Easy':<12} {'Compare models'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nüéØ USE MSE/RMSE WHEN:\")\n",
    "print(\"  ‚úì Training models (optimization)\")\n",
    "print(\"  ‚úì Large errors are especially bad\")\n",
    "print(\"  ‚úì Data is clean (few outliers)\")\n",
    "print(\"  ‚úì Normal distribution of errors expected\")\n",
    "print(\"  ‚úì Need differentiable loss\")\n",
    "\n",
    "print(\"\\nüéØ USE MAE WHEN:\")\n",
    "print(\"  ‚úì Outliers are present\")\n",
    "print(\"  ‚úì All errors should be weighted equally\")\n",
    "print(\"  ‚úì Robust metric needed\")\n",
    "print(\"  ‚úì Reporting to non-technical audience\")\n",
    "print(\"  ‚úì Median-based predictions\")\n",
    "\n",
    "print(\"\\nüéØ USE R¬≤ WHEN:\")\n",
    "print(\"  ‚úì Comparing different models\")\n",
    "print(\"  ‚úì Comparing different datasets\")\n",
    "print(\"  ‚úì Want normalized metric\")\n",
    "print(\"  ‚úì Need to communicate % variance explained\")\n",
    "print(\"  ‚úì Comparing to baseline (mean)\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è  BEST PRACTICE:\")\n",
    "print(\"  ‚Üí Report multiple metrics!\")\n",
    "print(\"  ‚Üí Use different metrics for different purposes\")\n",
    "print(\"  ‚Üí Consider your specific problem context\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Practical example comparing all metrics\n",
    "print(\"\\n=== PRACTICAL EXAMPLE ===\")\n",
    "\n",
    "# Generate realistic data\n",
    "np.random.seed(42)\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate all metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nModel Evaluation on Test Set:\")\n",
    "print(f\"  MSE:  {mse:.4f}\")\n",
    "print(f\"  RMSE: {rmse:.4f} (typical error)\")\n",
    "print(f\"  MAE:  {mae:.4f} (average absolute error)\")\n",
    "print(f\"  R¬≤:   {r2:.4f} ({r2*100:.1f}% variance explained)\")\n",
    "\n",
    "print(\"\\nWhat each metric tells us:\")\n",
    "print(f\"  ‚Ä¢ On average, predictions are off by {mae:.2f} units (MAE)\")\n",
    "print(f\"  ‚Ä¢ Typical prediction error is {rmse:.2f} units (RMSE)\")\n",
    "print(f\"  ‚Ä¢ Model explains {r2*100:.1f}% of variance in the data (R¬≤)\")\n",
    "print(f\"  ‚Ä¢ RMSE > MAE indicates some larger errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Advanced Metrics\n",
    "\n",
    "### Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "$$MAPE = \\frac{100\\%}{n}\\sum_{i=1}^{n}\\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|$$\n",
    "\n",
    "- **Scale-independent**: Percentage error\n",
    "- **Interpretable**: \"On average, predictions are off by X%\"\n",
    "- **Problem**: Undefined when $y_i = 0$, biased toward negative errors\n",
    "\n",
    "### Mean Squared Logarithmic Error (MSLE)\n",
    "\n",
    "$$MSLE = \\frac{1}{n}\\sum_{i=1}^{n}(\\log(1 + y_i) - \\log(1 + \\hat{y}_i))^2$$\n",
    "\n",
    "- **Penalizes underestimation** more than overestimation\n",
    "- **Good for**: Positive targets with large range\n",
    "- **Example**: Counts, prices\n",
    "\n",
    "### Adjusted R¬≤\n",
    "\n",
    "$$R^2_{adj} = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}$$\n",
    "\n",
    "Where:\n",
    "- $n$ : number of samples\n",
    "- $p$ : number of features\n",
    "\n",
    "- **Penalizes** adding more features\n",
    "- **Better for model comparison** with different numbers of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate advanced metrics\n",
    "print(\"=== ADVANCED METRICS ===\")\n",
    "\n",
    "# MAPE example\n",
    "print(\"\\n1. MEAN ABSOLUTE PERCENTAGE ERROR (MAPE)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "y_true_price = np.array([100, 200, 300, 400, 500])\n",
    "y_pred_price = np.array([110, 190, 320, 380, 510])\n",
    "\n",
    "# Calculate MAPE manually\n",
    "mape = np.mean(np.abs((y_true_price - y_pred_price) / y_true_price)) * 100\n",
    "\n",
    "print(f\"True prices:    {y_true_price}\")\n",
    "print(f\"Predicted:      {y_pred_price}\")\n",
    "print(f\"\\nMAPE: {mape:.2f}%\")\n",
    "print(f\"\\nInterpretation: On average, predictions are off by {mape:.1f}%\")\n",
    "\n",
    "# MSLE example\n",
    "print(\"\\n2. MEAN SQUARED LOGARITHMIC ERROR (MSLE)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "y_true_count = np.array([10, 100, 1000, 10000])\n",
    "y_pred_under = np.array([5, 50, 500, 5000])  # Underestimate\n",
    "y_pred_over = np.array([15, 150, 1500, 15000])  # Overestimate\n",
    "\n",
    "msle_under = mean_squared_log_error(y_true_count, y_pred_under)\n",
    "msle_over = mean_squared_log_error(y_true_count, y_pred_over)\n",
    "\n",
    "print(f\"True counts: {y_true_count}\")\n",
    "print(f\"\\nUnderestimating by 50%: MSLE = {msle_under:.4f}\")\n",
    "print(f\"Overestimating by 50%:  MSLE = {msle_over:.4f}\")\n",
    "print(f\"\\n‚úì MSLE penalizes underestimation more!\")\n",
    "\n",
    "# Adjusted R¬≤\n",
    "print(\"\\n3. ADJUSTED R¬≤\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def adjusted_r2(r2, n, p):\n",
    "    \"\"\"Calculate adjusted R¬≤\"\"\"\n",
    "    return 1 - ((1 - r2) * (n - 1) / (n - p - 1))\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Model with 5 features\n",
    "X_5, y_5 = make_regression(n_samples=n_samples, n_features=5, noise=10, random_state=42)\n",
    "X_train_5, X_test_5, y_train_5, y_test_5 = train_test_split(X_5, y_5, test_size=0.2)\n",
    "\n",
    "model_5 = LinearRegression()\n",
    "model_5.fit(X_train_5, y_train_5)\n",
    "r2_5 = model_5.score(X_test_5, y_test_5)\n",
    "adj_r2_5 = adjusted_r2(r2_5, len(X_test_5), 5)\n",
    "\n",
    "# Model with 20 features (some irrelevant)\n",
    "X_20, y_20 = make_regression(n_samples=n_samples, n_features=20, \n",
    "                             n_informative=5, noise=10, random_state=42)\n",
    "X_train_20, X_test_20, y_train_20, y_test_20 = train_test_split(X_20, y_20, test_size=0.2)\n",
    "\n",
    "model_20 = LinearRegression()\n",
    "model_20.fit(X_train_20, y_train_20)\n",
    "r2_20 = model_20.score(X_test_20, y_test_20)\n",
    "adj_r2_20 = adjusted_r2(r2_20, len(X_test_20), 20)\n",
    "\n",
    "print(\"\\nModel with 5 features:\")\n",
    "print(f\"  R¬≤:          {r2_5:.4f}\")\n",
    "print(f\"  Adjusted R¬≤: {adj_r2_5:.4f}\")\n",
    "\n",
    "print(\"\\nModel with 20 features (15 irrelevant):\")\n",
    "print(f\"  R¬≤:          {r2_20:.4f}\")\n",
    "print(f\"  Adjusted R¬≤: {adj_r2_20:.4f}\")\n",
    "\n",
    "print(\"\\n‚úì Adjusted R¬≤ penalizes unnecessary features!\")\n",
    "print(\"  Use for comparing models with different numbers of features\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# R¬≤ vs Adjusted R¬≤\n",
    "models = ['5 features', '20 features']\n",
    "r2_vals = [r2_5, r2_20]\n",
    "adj_r2_vals = [adj_r2_5, adj_r2_20]\n",
    "\n",
    "x_pos = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x_pos - width/2, r2_vals, width, label='R¬≤', alpha=0.8)\n",
    "axes[0].bar(x_pos + width/2, adj_r2_vals, width, label='Adjusted R¬≤', alpha=0.8)\n",
    "axes[0].set_ylabel('Score', fontsize=11)\n",
    "axes[0].set_title('R¬≤ vs Adjusted R¬≤', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(models)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# MAPE visualization\n",
    "percentage_errors = np.abs((y_true_price - y_pred_price) / y_true_price) * 100\n",
    "axes[1].bar(range(len(percentage_errors)), percentage_errors, alpha=0.7, edgecolor='black')\n",
    "axes[1].axhline(y=mape, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'MAPE = {mape:.1f}%')\n",
    "axes[1].set_xlabel('Sample', fontsize=11)\n",
    "axes[1].set_ylabel('Absolute Percentage Error (%)', fontsize=11)\n",
    "axes[1].set_title('MAPE: Per-Sample Percentage Errors', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Residual Analysis\n",
    "\n",
    "### Why Analyze Residuals?\n",
    "\n",
    "Metrics give us numbers, but residuals show us **patterns**:\n",
    "- Are errors random or systematic?\n",
    "- Do errors depend on input values?\n",
    "- Are there outliers?\n",
    "- Is the model appropriate?\n",
    "\n",
    "### Key Plots:\n",
    "1. **Residuals vs Predicted**: Check for patterns\n",
    "2. **Residual distribution**: Should be normal\n",
    "3. **Q-Q plot**: Check normality\n",
    "4. **Residuals vs Features**: Check relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive residual analysis\n",
    "print(\"=== RESIDUAL ANALYSIS ===\")\n",
    "\n",
    "# Load real dataset\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Residuals\n",
    "train_residuals = y_train - y_train_pred\n",
    "test_residuals = y_test - y_test_pred\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.2f}\")\n",
    "print(f\"  MAE:  {mean_absolute_error(y_test, y_test_pred):.2f}\")\n",
    "print(f\"  R¬≤:   {r2_score(y_test, y_test_pred):.4f}\")\n",
    "\n",
    "# Residual statistics\n",
    "print(\"\\nResidual Statistics:\")\n",
    "print(f\"  Mean:   {np.mean(test_residuals):.4f} (should be ~0)\")\n",
    "print(f\"  Std:    {np.std(test_residuals):.2f}\")\n",
    "print(f\"  Min:    {np.min(test_residuals):.2f}\")\n",
    "print(f\"  Max:    {np.max(test_residuals):.2f}\")\n",
    "\n",
    "# Comprehensive residual plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# 1. Residuals vs Predicted\n",
    "axes[0, 0].scatter(y_test_pred, test_residuals, alpha=0.6, s=30)\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Predicted Values', fontsize=10)\n",
    "axes[0, 0].set_ylabel('Residuals', fontsize=10)\n",
    "axes[0, 0].set_title('Residuals vs Predicted\\n(Should be random)', \n",
    "                    fontsize=11, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residual Distribution\n",
    "axes[0, 1].hist(test_residuals, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 1].axvline(x=np.mean(test_residuals), color='green', linestyle='--', \n",
    "                  linewidth=2, label=f'Mean={np.mean(test_residuals):.2f}')\n",
    "axes[0, 1].set_xlabel('Residuals', fontsize=10)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=10)\n",
    "axes[0, 1].set_title('Residual Distribution\\n(Should be normal)', \n",
    "                    fontsize=11, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=9)\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Q-Q Plot\n",
    "stats.probplot(test_residuals, dist=\"norm\", plot=axes[0, 2])\n",
    "axes[0, 2].set_title('Q-Q Plot\\n(Check normality)', fontsize=11, fontweight='bold')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Scale-Location (sqrt of standardized residuals vs predicted)\n",
    "standardized_residuals = test_residuals / np.std(test_residuals)\n",
    "axes[1, 0].scatter(y_test_pred, np.sqrt(np.abs(standardized_residuals)), \n",
    "                  alpha=0.6, s=30)\n",
    "axes[1, 0].set_xlabel('Predicted Values', fontsize=10)\n",
    "axes[1, 0].set_ylabel('‚àö|Standardized Residuals|', fontsize=10)\n",
    "axes[1, 0].set_title('Scale-Location Plot\\n(Check homoscedasticity)', \n",
    "                    fontsize=11, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Residuals vs Actual\n",
    "axes[1, 1].scatter(y_test, test_residuals, alpha=0.6, s=30)\n",
    "axes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('True Values', fontsize=10)\n",
    "axes[1, 1].set_ylabel('Residuals', fontsize=10)\n",
    "axes[1, 1].set_title('Residuals vs Actual', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Ordered residuals\n",
    "axes[1, 2].plot(sorted(test_residuals), 'o-', alpha=0.6, markersize=4)\n",
    "axes[1, 2].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 2].set_xlabel('Ordered Index', fontsize=10)\n",
    "axes[1, 2].set_ylabel('Residual', fontsize=10)\n",
    "axes[1, 2].set_title('Ordered Residuals\\n(Spot outliers)', \n",
    "                    fontsize=11, fontweight='bold')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== RESIDUAL ANALYSIS CHECKLIST ===\")\n",
    "print(\"\\n‚úì GOOD SIGNS:\")\n",
    "print(\"  ‚Ä¢ Residuals randomly scattered around zero\")\n",
    "print(\"  ‚Ä¢ No clear patterns in residual plots\")\n",
    "print(\"  ‚Ä¢ Residuals roughly normally distributed\")\n",
    "print(\"  ‚Ä¢ Mean residual close to zero\")\n",
    "print(\"  ‚Ä¢ Constant variance (homoscedasticity)\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  WARNING SIGNS:\")\n",
    "print(\"  ‚Ä¢ Curved pattern in residuals vs predicted\")\n",
    "print(\"  ‚Ä¢ Funnel shape (heteroscedasticity)\")\n",
    "print(\"  ‚Ä¢ Heavy-tailed or skewed distribution\")\n",
    "print(\"  ‚Ä¢ Clear outliers\")\n",
    "print(\"  ‚Ä¢ Systematic over/under prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Complete Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation function\n",
    "def comprehensive_evaluation(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with all metrics and visualizations\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"COMPREHENSIVE EVALUATION: {model_name}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    max_err = max_error(y_true, y_pred)\n",
    "    \n",
    "    # Residuals\n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nüìä PERFORMANCE METRICS\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"  MSE:        {mse:.4f}\")\n",
    "    print(f\"  RMSE:       {rmse:.4f}  (typical error)\")\n",
    "    print(f\"  MAE:        {mae:.4f}  (average absolute error)\")\n",
    "    print(f\"  R¬≤:         {r2:.4f}  ({r2*100:.1f}% variance explained)\")\n",
    "    print(f\"  Max Error:  {max_err:.4f}  (worst prediction)\")\n",
    "    \n",
    "    # Residual statistics\n",
    "    print(\"\\nüìà RESIDUAL STATISTICS\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"  Mean:       {np.mean(residuals):.4f}  (should be ~0)\")\n",
    "    print(f\"  Std Dev:    {np.std(residuals):.4f}\")\n",
    "    print(f\"  Min:        {np.min(residuals):.4f}\")\n",
    "    print(f\"  Max:        {np.max(residuals):.4f}\")\n",
    "    print(f\"  Median:     {np.median(residuals):.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\nüí° INTERPRETATION\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    if r2 > 0.9:\n",
    "        quality = \"Excellent\"\n",
    "    elif r2 > 0.7:\n",
    "        quality = \"Good\"\n",
    "    elif r2 > 0.5:\n",
    "        quality = \"Moderate\"\n",
    "    elif r2 > 0:\n",
    "        quality = \"Poor\"\n",
    "    else:\n",
    "        quality = \"Very Poor (worse than mean)\"\n",
    "    \n",
    "    print(f\"  Model Quality: {quality}\")\n",
    "    print(f\"  ‚Ä¢ Model explains {r2*100:.1f}% of variance in target\")\n",
    "    print(f\"  ‚Ä¢ Typical prediction is off by {rmse:.2f} units\")\n",
    "    print(f\"  ‚Ä¢ Average absolute error is {mae:.2f} units\")\n",
    "    \n",
    "    if rmse > mae * 1.5:\n",
    "        print(f\"  ‚ö†Ô∏è  RMSE >> MAE suggests presence of outliers\")\n",
    "    \n",
    "    if abs(np.mean(residuals)) > mae * 0.1:\n",
    "        print(f\"  ‚ö†Ô∏è  Non-zero mean residual suggests bias\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Predictions vs Actual\n",
    "    axes[0, 0].scatter(y_true, y_pred, alpha=0.6, s=30)\n",
    "    axes[0, 0].plot([y_true.min(), y_true.max()], \n",
    "                    [y_true.min(), y_true.max()], \n",
    "                    'r--', linewidth=2, label='Perfect prediction')\n",
    "    axes[0, 0].set_xlabel('True Values', fontsize=11)\n",
    "    axes[0, 0].set_ylabel('Predictions', fontsize=11)\n",
    "    axes[0, 0].set_title(f'{model_name}\\nR¬≤ = {r2:.3f}', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].legend(fontsize=10)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals vs Predicted\n",
    "    axes[0, 1].scatter(y_pred, residuals, alpha=0.6, s=30)\n",
    "    axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Predicted Values', fontsize=11)\n",
    "    axes[0, 1].set_ylabel('Residuals', fontsize=11)\n",
    "    axes[0, 1].set_title('Residual Plot', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual Distribution\n",
    "    axes[1, 0].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Residuals', fontsize=11)\n",
    "    axes[1, 0].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[1, 0].set_title('Residual Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Metrics summary\n",
    "    axes[1, 1].axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    METRIC SUMMARY\n",
    "    \n",
    "    MSE:   {mse:.3f}\n",
    "    RMSE:  {rmse:.3f}\n",
    "    MAE:   {mae:.3f}\n",
    "    R¬≤:    {r2:.3f}\n",
    "    \n",
    "    Quality: {quality}\n",
    "    \n",
    "    Variance Explained: {r2*100:.1f}%\n",
    "    Unexplained: {(1-r2)*100:.1f}%\n",
    "    \n",
    "    Typical Error: {rmse:.2f}\n",
    "    Worst Error: {max_err:.2f}\n",
    "    \"\"\"\n",
    "    axes[1, 1].text(0.1, 0.9, summary_text, transform=axes[1, 1].transAxes,\n",
    "                   fontsize=11, verticalalignment='top', family='monospace',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'max_error': max_err\n",
    "    }\n",
    "\n",
    "# Test comprehensive evaluation\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "results = comprehensive_evaluation(y_test, y_pred, \"Ridge Regression (Œ±=1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Key Takeaways\n",
    "\n",
    "### Core Concepts:\n",
    "1. ‚úÖ **Multiple metrics** give complete picture\n",
    "2. ‚úÖ **MSE/RMSE** penalize large errors heavily\n",
    "3. ‚úÖ **MAE** is robust to outliers\n",
    "4. ‚úÖ **R¬≤** shows % variance explained\n",
    "5. ‚úÖ **Residual analysis** reveals patterns\n",
    "\n",
    "### Metric Formulas:\n",
    "\n",
    "**MSE**: $\\frac{1}{n}\\sum(y_i - \\hat{y}_i)^2$\n",
    "\n",
    "**RMSE**: $\\sqrt{MSE}$\n",
    "\n",
    "**MAE**: $\\frac{1}{n}\\sum|y_i - \\hat{y}_i|$\n",
    "\n",
    "**R¬≤**: $1 - \\frac{SS_{res}}{SS_{tot}}$\n",
    "\n",
    "### Best Practices:\n",
    "1. **Report multiple metrics**: MSE, MAE, and R¬≤\n",
    "2. **Always check residuals**: Look for patterns\n",
    "3. **Consider context**: What matters for your problem?\n",
    "4. **Use appropriate metric**: Outliers ‚Üí MAE, Optimization ‚Üí MSE\n",
    "5. **Compare to baseline**: Is model better than predicting mean?\n",
    "\n",
    "### Quick Guide:\n",
    "- **Training**: Use MSE (differentiable)\n",
    "- **Reporting**: Use RMSE and R¬≤ (interpretable)\n",
    "- **Outliers**: Use MAE (robust)\n",
    "- **Comparing**: Use R¬≤ (normalized)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You understand regression evaluation metrics! üéâ**\n",
    "\n",
    "**You can now properly assess model performance!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
