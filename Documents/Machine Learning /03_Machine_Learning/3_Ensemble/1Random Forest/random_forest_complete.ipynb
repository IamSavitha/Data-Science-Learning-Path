{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest: Complete Guide\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Ensemble Methods](#1.-Introduction-to-Ensemble-Methods)\n",
    "2. [Bootstrap Aggregating (Bagging)](#2.-Bootstrap-Aggregating-(Bagging))\n",
    "3. [Random Forest Algorithm](#3.-Random-Forest-Algorithm)\n",
    "4. [Out-of-Bag (OOB) Error Estimation](#4.-Out-of-Bag-(OOB)-Error-Estimation)\n",
    "5. [Feature Importance](#5.-Feature-Importance)\n",
    "6. [Implementation from Scratch](#6.-Implementation-from-Scratch)\n",
    "7. [Scikit-learn Implementation](#7.-Scikit-learn-Implementation)\n",
    "8. [Hyperparameter Tuning](#8.-Hyperparameter-Tuning)\n",
    "9. [Comparison with Decision Tree](#9.-Comparison-with-Decision-Tree)\n",
    "10. [Variable Importance Plots](#10.-Variable-Importance-Plots)\n",
    "11. [Partial Dependence Plots](#11.-Partial-Dependence-Plots)\n",
    "12. [Real-world Applications](#12.-Real-world-Applications)\n",
    "13. [Practice Problems](#13.-Practice-Problems)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Ensemble Methods\n",
    "\n",
    "### What are Ensemble Methods?\n",
    "\n",
    "**Ensemble methods** combine multiple machine learning models to create a more powerful predictive model. The key idea is that a group of \"weak learners\" can come together to form a \"strong learner.\"\n",
    "\n",
    "### Why Use Ensemble Methods?\n",
    "\n",
    "1. **Reduced Overfitting**: By averaging multiple models, we reduce variance\n",
    "2. **Improved Accuracy**: Combined predictions are often more accurate\n",
    "3. **Robustness**: Less sensitive to noise and outliers\n",
    "4. **Stability**: More consistent predictions across different datasets\n",
    "\n",
    "### Types of Ensemble Methods\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating)**\n",
    "   - Train models on random subsets of data\n",
    "   - Example: Random Forest\n",
    "\n",
    "2. **Boosting**\n",
    "   - Train models sequentially, focusing on errors\n",
    "   - Example: AdaBoost, Gradient Boosting, XGBoost\n",
    "\n",
    "3. **Stacking**\n",
    "   - Combine different types of models\n",
    "   - Meta-learner makes final prediction\n",
    "\n",
    "### The Wisdom of Crowds\n",
    "\n",
    "Ensemble methods are based on the \"wisdom of crowds\" principle:\n",
    "- Individual predictions may be noisy\n",
    "- Aggregate prediction is more stable and accurate\n",
    "- Works best when individual models are diverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingClassifier\n",
    "from sklearn.datasets import make_classification, make_regression, load_iris, load_breast_cancer, load_diabetes\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bootstrap Aggregating (Bagging)\n",
    "\n",
    "### What is Bagging?\n",
    "\n",
    "**Bagging** (Bootstrap Aggregating) is an ensemble technique that:\n",
    "1. Creates multiple bootstrap samples from the training data\n",
    "2. Trains a separate model on each sample\n",
    "3. Aggregates predictions (voting for classification, averaging for regression)\n",
    "\n",
    "### Bootstrap Sampling\n",
    "\n",
    "- Sample **with replacement** from original dataset\n",
    "- Each sample has same size as original\n",
    "- On average, each bootstrap sample contains ~63.2% unique instances\n",
    "- Remaining ~36.8% are \"out-of-bag\" (OOB) samples\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "For classification:\n",
    "$$\\hat{y} = \\text{mode}(\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_B)$$\n",
    "\n",
    "For regression:\n",
    "$$\\hat{y} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{y}_b$$\n",
    "\n",
    "where $B$ is the number of bootstrap samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of Bootstrap Sampling\n",
    "def demonstrate_bootstrap():\n",
    "    # Original dataset\n",
    "    original_data = np.arange(1, 11)\n",
    "    print(\"Original Data:\", original_data)\n",
    "    print(\"\\nBootstrap Samples:\")\n",
    "    \n",
    "    # Create 5 bootstrap samples\n",
    "    for i in range(5):\n",
    "        bootstrap_sample = np.random.choice(original_data, size=len(original_data), replace=True)\n",
    "        unique_pct = len(np.unique(bootstrap_sample)) / len(original_data) * 100\n",
    "        print(f\"Sample {i+1}: {bootstrap_sample} (Unique: {unique_pct:.1f}%)\")\n",
    "\n",
    "demonstrate_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing variance reduction through bagging\n",
    "def visualize_bagging_variance_reduction():\n",
    "    # Generate synthetic data\n",
    "    X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "    y_true = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Single decision tree\n",
    "    tree = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "    tree.fit(X, y_true)\n",
    "    y_pred_tree = tree.predict(X)\n",
    "    \n",
    "    axes[0].scatter(X, y_true, alpha=0.5, label='Data')\n",
    "    axes[0].plot(X, y_pred_tree, 'r-', linewidth=2, label='Single Tree')\n",
    "    axes[0].set_title('Single Decision Tree\\n(High Variance)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('X')\n",
    "    axes[0].set_ylabel('y')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Multiple trees (bagging)\n",
    "    n_estimators = 10\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(n_estimators):\n",
    "        # Bootstrap sample\n",
    "        indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "        X_boot, y_boot = X[indices], y_true[indices]\n",
    "        \n",
    "        # Train tree\n",
    "        tree = DecisionTreeRegressor(max_depth=5, random_state=i)\n",
    "        tree.fit(X_boot, y_boot)\n",
    "        y_pred = tree.predict(X)\n",
    "        predictions.append(y_pred)\n",
    "        \n",
    "        # Plot individual tree\n",
    "        axes[1].plot(X, y_pred, alpha=0.3, linewidth=1)\n",
    "    \n",
    "    axes[1].scatter(X, y_true, alpha=0.5, label='Data')\n",
    "    axes[1].set_title(f'{n_estimators} Individual Trees\\n(Bootstrap Samples)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('X')\n",
    "    axes[1].set_ylabel('y')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bagged prediction (average)\n",
    "    y_pred_bagged = np.mean(predictions, axis=0)\n",
    "    \n",
    "    axes[2].scatter(X, y_true, alpha=0.5, label='Data')\n",
    "    axes[2].plot(X, y_pred_bagged, 'g-', linewidth=2, label='Bagged Prediction')\n",
    "    axes[2].set_title('Bagged Prediction (Average)\\n(Reduced Variance)', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_xlabel('X')\n",
    "    axes[2].set_ylabel('y')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate variance\n",
    "    var_tree = np.var(y_pred_tree - y_true)\n",
    "    var_bagged = np.var(y_pred_bagged - y_true)\n",
    "    print(f\"\\nVariance Reduction:\")\n",
    "    print(f\"Single Tree Variance: {var_tree:.4f}\")\n",
    "    print(f\"Bagged Variance: {var_bagged:.4f}\")\n",
    "    print(f\"Reduction: {(1 - var_bagged/var_tree)*100:.2f}%\")\n",
    "\n",
    "visualize_bagging_variance_reduction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest Algorithm\n",
    "\n",
    "### What is Random Forest?\n",
    "\n",
    "**Random Forest** is an ensemble method that extends bagging by adding random feature selection:\n",
    "1. Create bootstrap samples (like bagging)\n",
    "2. At each split, randomly select a subset of features\n",
    "3. Choose the best split from the selected features only\n",
    "4. Aggregate predictions from all trees\n",
    "\n",
    "### Key Differences from Bagging\n",
    "\n",
    "| Aspect | Bagging | Random Forest |\n",
    "|--------|---------|---------------|\n",
    "| Data Sampling | Bootstrap samples | Bootstrap samples |\n",
    "| Feature Selection | All features | Random subset at each split |\n",
    "| Tree Correlation | Higher | Lower (more diverse) |\n",
    "| Variance Reduction | Good | Better |\n",
    "\n",
    "### Algorithm Steps\n",
    "\n",
    "1. **For b = 1 to B:**\n",
    "   - Draw a bootstrap sample of size n from training data\n",
    "   - Grow a decision tree $T_b$ on this sample:\n",
    "     - At each node, randomly select $m$ features from $p$ total features\n",
    "     - Choose best split from these $m$ features only\n",
    "     - Split the node and repeat until stopping criterion\n",
    "\n",
    "2. **Prediction:**\n",
    "   - Classification: Majority vote from all trees\n",
    "   - Regression: Average prediction from all trees\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "- **n_estimators**: Number of trees (typically 100-500)\n",
    "- **max_features**: Number of features to consider at each split\n",
    "  - Classification: $\\sqrt{p}$ (default)\n",
    "  - Regression: $p/3$ (default)\n",
    "- **max_depth**: Maximum depth of trees\n",
    "- **min_samples_split**: Minimum samples to split a node\n",
    "- **min_samples_leaf**: Minimum samples in leaf node\n",
    "- **bootstrap**: Whether to use bootstrap samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Random Forest concept\n",
    "def visualize_random_forest_concept():\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Generate data\n",
    "    X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
    "                                n_informative=2, n_clusters_per_class=1,\n",
    "                                random_state=42)\n",
    "    \n",
    "    # Create mesh for decision boundary\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "    \n",
    "    # Train individual trees\n",
    "    for idx in range(5):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        # Bootstrap sample\n",
    "        indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "        X_boot, y_boot = X[indices], y[indices]\n",
    "        \n",
    "        # Train tree\n",
    "        tree = DecisionTreeClassifier(max_depth=5, max_features='sqrt', random_state=idx)\n",
    "        tree.fit(X_boot, y_boot)\n",
    "        \n",
    "        # Plot decision boundary\n",
    "        Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "        ax.scatter(X_boot[:, 0], X_boot[:, 1], c=y_boot, cmap='RdYlBu', \n",
    "                   edgecolors='black', alpha=0.7)\n",
    "        ax.set_title(f'Tree {idx+1}', fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "    \n",
    "    # Random Forest (ensemble)\n",
    "    ax = axes[1, 2]\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    Z = rf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', \n",
    "               edgecolors='black', alpha=0.7)\n",
    "    ax.set_title('Random Forest\\n(100 Trees Combined)', fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_random_forest_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Out-of-Bag (OOB) Error Estimation\n",
    "\n",
    "### What is OOB Error?\n",
    "\n",
    "**Out-of-Bag (OOB) error** is a method to estimate the test error of a Random Forest without using a separate validation set.\n",
    "\n",
    "### How it Works\n",
    "\n",
    "1. For each tree, ~36.8% of samples are not in the bootstrap sample (OOB samples)\n",
    "2. Use these OOB samples to evaluate that tree's performance\n",
    "3. For each data point, aggregate predictions from trees where it was OOB\n",
    "4. Calculate error on these aggregated predictions\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Free validation**: No need for separate validation set\n",
    "- **Efficient**: Uses all data for training and validation\n",
    "- **Reliable**: Often similar to cross-validation error\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "For each observation $i$:\n",
    "$$\\hat{y}_i^{OOB} = \\text{aggregate}\\{\\hat{y}_b(x_i) : i \\notin S_b\\}$$\n",
    "\n",
    "where $S_b$ is the bootstrap sample for tree $b$.\n",
    "\n",
    "OOB Error:\n",
    "$$\\text{OOB Error} = \\frac{1}{n} \\sum_{i=1}^{n} L(y_i, \\hat{y}_i^{OOB})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating OOB Error Estimation\n",
    "def demonstrate_oob_error():\n",
    "    # Load data\n",
    "    X, y = load_breast_cancer(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train Random Forest with OOB score\n",
    "    rf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "    rf_oob.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate scores\n",
    "    oob_score = rf_oob.oob_score_\n",
    "    train_score = rf_oob.score(X_train, y_train)\n",
    "    test_score = rf_oob.score(X_test, y_test)\n",
    "    \n",
    "    print(\"Score Comparison:\")\n",
    "    print(f\"Training Score: {train_score:.4f}\")\n",
    "    print(f\"OOB Score: {oob_score:.4f}\")\n",
    "    print(f\"Test Score: {test_score:.4f}\")\n",
    "    print(f\"\\nOOB vs Test Difference: {abs(oob_score - test_score):.4f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    scores = [train_score, oob_score, test_score]\n",
    "    labels = ['Training', 'OOB', 'Test']\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(labels, scores, color=colors, alpha=0.7, edgecolor='black')\n",
    "    plt.ylabel('Accuracy Score', fontsize=12)\n",
    "    plt.title('Comparison of Training, OOB, and Test Scores', fontsize=14, fontweight='bold')\n",
    "    plt.ylim(0.9, 1.0)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, scores):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{score:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_oob_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOB Error vs Number of Trees\n",
    "def plot_oob_error_vs_trees():\n",
    "    X, y = load_breast_cancer(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Track errors\n",
    "    n_trees = range(1, 201, 5)\n",
    "    oob_errors = []\n",
    "    test_errors = []\n",
    "    \n",
    "    for n in n_trees:\n",
    "        rf = RandomForestClassifier(n_estimators=n, oob_score=True, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        oob_errors.append(1 - rf.oob_score_)\n",
    "        test_errors.append(1 - rf.score(X_test, y_test))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(n_trees, oob_errors, 'o-', label='OOB Error', linewidth=2, markersize=4)\n",
    "    plt.plot(n_trees, test_errors, 's-', label='Test Error', linewidth=2, markersize=4)\n",
    "    plt.xlabel('Number of Trees', fontsize=12)\n",
    "    plt.ylabel('Error Rate', fontsize=12)\n",
    "    plt.title('OOB Error vs Test Error (Number of Trees)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final OOB Error: {oob_errors[-1]:.4f}\")\n",
    "    print(f\"Final Test Error: {test_errors[-1]:.4f}\")\n",
    "\n",
    "plot_oob_error_vs_trees()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance\n",
    "\n",
    "### What is Feature Importance?\n",
    "\n",
    "**Feature importance** measures the contribution of each feature to the model's predictions. Random Forests provide two main methods:\n",
    "\n",
    "### 1. Mean Decrease in Impurity (MDI)\n",
    "\n",
    "- Also called Gini importance\n",
    "- Measures total reduction in node impurity by each feature\n",
    "- Weighted by probability of reaching that node\n",
    "- Fast to compute (available after training)\n",
    "\n",
    "$$\\text{Importance}(X_j) = \\frac{1}{B} \\sum_{b=1}^{B} \\sum_{t \\in T_b} \\Delta i(t) \\cdot \\mathbb{1}(v(t) = X_j)$$\n",
    "\n",
    "where:\n",
    "- $\\Delta i(t)$ is the impurity decrease at node $t$\n",
    "- $v(t)$ is the feature used for split at node $t$\n",
    "\n",
    "### 2. Mean Decrease in Accuracy (MDA)\n",
    "\n",
    "- Also called permutation importance\n",
    "- Shuffle feature values and measure decrease in accuracy\n",
    "- More reliable but computationally expensive\n",
    "- Works on any model\n",
    "\n",
    "### Advantages of Random Forest Feature Importance\n",
    "\n",
    "- **Non-linear relationships**: Captures complex interactions\n",
    "- **No assumptions**: Works with any type of features\n",
    "- **Ranking**: Easy to rank features by importance\n",
    "- **Feature selection**: Can use for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Demonstration\n",
    "def demonstrate_feature_importance():\n",
    "    # Load data\n",
    "    data = load_breast_cancer()\n",
    "    X, y = data.data, data.target\n",
    "    feature_names = data.feature_names\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Method 1: MDI (built-in)\n",
    "    mdi_importance = rf.feature_importances_\n",
    "    \n",
    "    # Method 2: MDA (permutation)\n",
    "    perm_importance = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)\n",
    "    mda_importance = perm_importance.importances_mean\n",
    "    \n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'MDI': mdi_importance,\n",
    "        'MDA': mda_importance\n",
    "    })\n",
    "    \n",
    "    # Sort and get top 15\n",
    "    importance_df = importance_df.sort_values('MDI', ascending=False).head(15)\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # MDI\n",
    "    axes[0].barh(range(len(importance_df)), importance_df['MDI'], color='skyblue', edgecolor='black')\n",
    "    axes[0].set_yticks(range(len(importance_df)))\n",
    "    axes[0].set_yticklabels(importance_df['Feature'])\n",
    "    axes[0].set_xlabel('Importance Score', fontsize=12)\n",
    "    axes[0].set_title('Mean Decrease in Impurity (MDI)\\nGini Importance', \n",
    "                      fontsize=13, fontweight='bold')\n",
    "    axes[0].invert_yaxis()\n",
    "    axes[0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # MDA\n",
    "    importance_df_mda = importance_df.sort_values('MDA', ascending=False)\n",
    "    axes[1].barh(range(len(importance_df_mda)), importance_df_mda['MDA'], \n",
    "                 color='lightcoral', edgecolor='black')\n",
    "    axes[1].set_yticks(range(len(importance_df_mda)))\n",
    "    axes[1].set_yticklabels(importance_df_mda['Feature'])\n",
    "    axes[1].set_xlabel('Importance Score', fontsize=12)\n",
    "    axes[1].set_title('Mean Decrease in Accuracy (MDA)\\nPermutation Importance', \n",
    "                      fontsize=13, fontweight='bold')\n",
    "    axes[1].invert_yaxis()\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top 10\n",
    "    print(\"\\nTop 10 Features by MDI:\")\n",
    "    print(importance_df[['Feature', 'MDI']].head(10).to_string(index=False))\n",
    "\n",
    "demonstrate_feature_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementation from Scratch\n",
    "\n",
    "Let's implement a basic Random Forest classifier from scratch to understand the algorithm better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Decision Tree Node\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature      # Feature index for split\n",
    "        self.threshold = threshold  # Threshold value for split\n",
    "        self.left = left           # Left child node\n",
    "        self.right = right         # Right child node\n",
    "        self.value = value         # Value if leaf node\n",
    "\n",
    "# Simple Decision Tree\n",
    "class SimpleDecisionTree:\n",
    "    def __init__(self, max_depth=10, min_samples_split=2, max_features=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.root = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1]\n",
    "        self.root = self._grow_tree(X, y)\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if depth >= self.max_depth or n_classes == 1 or n_samples < self.min_samples_split:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        # Find best split\n",
    "        feature_indices = np.random.choice(n_features, self.max_features, replace=False)\n",
    "        best_feature, best_threshold = self._best_split(X, y, feature_indices)\n",
    "        \n",
    "        # Create child nodes\n",
    "        left_indices = X[:, best_feature] < best_threshold\n",
    "        right_indices = ~left_indices\n",
    "        left = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        \n",
    "        return Node(best_feature, best_threshold, left, right)\n",
    "    \n",
    "    def _best_split(self, X, y, feature_indices):\n",
    "        best_gini = float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature in feature_indices:\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gini = self._gini_impurity(X[:, feature], y, threshold)\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def _gini_impurity(self, X_column, y, threshold):\n",
    "        left_indices = X_column < threshold\n",
    "        right_indices = ~left_indices\n",
    "        \n",
    "        n = len(y)\n",
    "        n_left, n_right = np.sum(left_indices), np.sum(right_indices)\n",
    "        \n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        gini_left = self._gini(y[left_indices])\n",
    "        gini_right = self._gini(y[right_indices])\n",
    "        \n",
    "        weighted_gini = (n_left / n) * gini_left + (n_right / n) * gini_right\n",
    "        return weighted_gini\n",
    "    \n",
    "    def _gini(self, y):\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        return 1 - np.sum(proportions ** 2)\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] < node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Random Forest Implementation\n",
    "class SimpleRandomForest:\n",
    "    def __init__(self, n_estimators=100, max_depth=10, min_samples_split=2, max_features='sqrt'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Determine max_features\n",
    "        if self.max_features == 'sqrt':\n",
    "            max_features = int(np.sqrt(n_features))\n",
    "        elif self.max_features == 'log2':\n",
    "            max_features = int(np.log2(n_features))\n",
    "        else:\n",
    "            max_features = n_features\n",
    "        \n",
    "        # Train trees\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Bootstrap sample\n",
    "            indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "            X_bootstrap = X[indices]\n",
    "            y_bootstrap = y[indices]\n",
    "            \n",
    "            # Train tree\n",
    "            tree = SimpleDecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                max_features=max_features\n",
    "            )\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Get predictions from all trees\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        \n",
    "        # Majority vote\n",
    "        predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            predictions.append(np.bincount(tree_predictions[:, i].astype(int)).argmax())\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "# Test our implementation\n",
    "print(\"Testing Simple Random Forest Implementation...\\n\")\n",
    "\n",
    "# Load data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train our Random Forest\n",
    "print(\"Training custom Random Forest...\")\n",
    "custom_rf = SimpleRandomForest(n_estimators=50, max_depth=5)\n",
    "custom_rf.fit(X_train, y_train)\n",
    "custom_accuracy = custom_rf.score(X_test, y_test)\n",
    "\n",
    "# Compare with sklearn\n",
    "print(\"Training sklearn Random Forest...\")\n",
    "sklearn_rf = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
    "sklearn_rf.fit(X_train, y_train)\n",
    "sklearn_accuracy = sklearn_rf.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Custom Random Forest Accuracy: {custom_accuracy:.4f}\")\n",
    "print(f\"Sklearn Random Forest Accuracy: {sklearn_accuracy:.4f}\")\n",
    "print(f\"Difference: {abs(custom_accuracy - sklearn_accuracy):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scikit-learn Implementation\n",
    "\n",
    "### Random Forest for Classification\n",
    "\n",
    "Scikit-learn provides `RandomForestClassifier` for classification tasks:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=100,      # Number of trees\n",
    "    max_depth=None,        # Maximum depth of trees\n",
    "    max_features='sqrt',   # Number of features to consider\n",
    "    min_samples_split=2,   # Minimum samples to split\n",
    "    min_samples_leaf=1,    # Minimum samples in leaf\n",
    "    bootstrap=True,        # Use bootstrap samples\n",
    "    oob_score=False,       # Calculate OOB score\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "### Random Forest for Regression\n",
    "\n",
    "For regression tasks, use `RandomForestRegressor`:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_features='auto',   # For regression, default is n_features\n",
    "    random_state=42\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Example: Iris Dataset\n",
    "def classification_example():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"RANDOM FOREST CLASSIFICATION EXAMPLE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load data\n",
    "    data = load_iris()\n",
    "    X, y = data.data, data.target\n",
    "    feature_names = data.feature_names\n",
    "    target_names = data.target_names\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = rf_clf.predict(X_test)\n",
    "    \n",
    "    # Evaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title('Confusion Matrix - Iris Classification', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature Importance\n",
    "    importance = rf_clf.feature_importances_\n",
    "    indices = np.argsort(importance)[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(importance)), importance[indices], color='skyblue', edgecolor='black')\n",
    "    plt.xticks(range(len(importance)), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.ylabel('Importance', fontsize=12)\n",
    "    plt.title('Feature Importance - Iris Classification', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "classification_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Example: Diabetes Dataset\n",
    "def regression_example():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"RANDOM FOREST REGRESSION EXAMPLE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load data\n",
    "    data = load_diabetes()\n",
    "    X, y = data.data, data.target\n",
    "    feature_names = data.feature_names\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = rf_reg.predict(X_test)\n",
    "    \n",
    "    # Evaluation\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nMean Squared Error: {mse:.2f}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Visualize predictions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Actual vs Predicted\n",
    "    axes[0].scatter(y_test, y_pred, alpha=0.6, edgecolors='black')\n",
    "    axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[0].set_xlabel('Actual Values', fontsize=12)\n",
    "    axes[0].set_ylabel('Predicted Values', fontsize=12)\n",
    "    axes[0].set_title('Actual vs Predicted Values', fontsize=13, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals\n",
    "    residuals = y_test - y_pred\n",
    "    axes[1].scatter(y_pred, residuals, alpha=0.6, edgecolors='black')\n",
    "    axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    axes[1].set_xlabel('Predicted Values', fontsize=12)\n",
    "    axes[1].set_ylabel('Residuals', fontsize=12)\n",
    "    axes[1].set_title('Residual Plot', fontsize=13, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature Importance\n",
    "    importance = rf_reg.feature_importances_\n",
    "    indices = np.argsort(importance)[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(importance)), importance[indices], color='lightcoral', edgecolor='black')\n",
    "    plt.xticks(range(len(importance)), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.ylabel('Importance', fontsize=12)\n",
    "    plt.title('Feature Importance - Diabetes Regression', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "regression_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "#### 1. n_estimators\n",
    "- **Definition**: Number of trees in the forest\n",
    "- **Effect**: More trees → better performance but slower training\n",
    "- **Typical range**: 100-500\n",
    "- **Note**: Performance plateaus after certain point\n",
    "\n",
    "#### 2. max_depth\n",
    "- **Definition**: Maximum depth of each tree\n",
    "- **Effect**: Deeper trees → more complex model, risk of overfitting\n",
    "- **Typical range**: 5-20 or None (unlimited)\n",
    "\n",
    "#### 3. max_features\n",
    "- **Definition**: Number of features to consider at each split\n",
    "- **Options**:\n",
    "  - 'sqrt': $\\sqrt{n\\_features}$ (default for classification)\n",
    "  - 'log2': $\\log_2(n\\_features)$\n",
    "  - None: All features (default for regression)\n",
    "  - int: Specific number\n",
    "  - float: Percentage of features\n",
    "\n",
    "#### 4. min_samples_split\n",
    "- **Definition**: Minimum samples required to split a node\n",
    "- **Effect**: Higher values → simpler trees, less overfitting\n",
    "- **Typical range**: 2-20\n",
    "\n",
    "#### 5. min_samples_leaf\n",
    "- **Definition**: Minimum samples required in leaf node\n",
    "- **Effect**: Higher values → smoother decision boundary\n",
    "- **Typical range**: 1-10\n",
    "\n",
    "### Tuning Strategies\n",
    "\n",
    "1. **Grid Search**: Exhaustive search over parameter grid\n",
    "2. **Random Search**: Random sampling from parameter distributions\n",
    "3. **Bayesian Optimization**: Smart search using probabilistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of n_estimators\n",
    "def tune_n_estimators():\n",
    "    X, y = load_breast_cancer(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    n_estimators_range = [1, 5, 10, 20, 50, 100, 200, 300, 500]\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    for n in n_estimators_range:\n",
    "        rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        train_scores.append(rf.score(X_train, y_train))\n",
    "        test_scores.append(rf.score(X_test, y_test))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(n_estimators_range, train_scores, 'o-', label='Training Score', linewidth=2, markersize=8)\n",
    "    plt.plot(n_estimators_range, test_scores, 's-', label='Test Score', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Number of Estimators', fontsize=12)\n",
    "    plt.ylabel('Accuracy Score', fontsize=12)\n",
    "    plt.title('Effect of n_estimators on Model Performance', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPerformance vs n_estimators:\")\n",
    "    for n, train, test in zip(n_estimators_range, train_scores, test_scores):\n",
    "        print(f\"n={n:3d}: Train={train:.4f}, Test={test:.4f}\")\n",
    "\n",
    "tune_n_estimators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of max_depth\n",
    "def tune_max_depth():\n",
    "    X, y = load_breast_cancer(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    max_depth_range = [1, 2, 3, 5, 7, 10, 15, 20, None]\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    for depth in max_depth_range:\n",
    "        rf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        train_scores.append(rf.score(X_train, y_train))\n",
    "        test_scores.append(rf.score(X_test, y_test))\n",
    "    \n",
    "    # For plotting, replace None with a large number\n",
    "    plot_depths = [d if d is not None else 25 for d in max_depth_range]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(plot_depths, train_scores, 'o-', label='Training Score', linewidth=2, markersize=8)\n",
    "    plt.plot(plot_depths, test_scores, 's-', label='Test Score', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Maximum Depth', fontsize=12)\n",
    "    plt.ylabel('Accuracy Score', fontsize=12)\n",
    "    plt.title('Effect of max_depth on Model Performance', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(plot_depths, [str(d) if d is not None else 'None' for d in max_depth_range])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPerformance vs max_depth:\")\n",
    "    for depth, train, test in zip(max_depth_range, train_scores, test_scores):\n",
    "        depth_str = str(depth) if depth is not None else 'None'\n",
    "        print(f\"depth={depth_str:>4s}: Train={train:.4f}, Test={test:.4f}\")\n",
    "\n",
    "tune_max_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of max_features\n",
    "def tune_max_features():\n",
    "    X, y = load_breast_cancer(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    n_features = X.shape[1]\n",
    "    max_features_range = ['sqrt', 'log2', 0.3, 0.5, 0.7, 1.0]\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    labels = []\n",
    "    \n",
    "    for mf in max_features_range:\n",
    "        rf = RandomForestClassifier(n_estimators=100, max_features=mf, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        train_scores.append(rf.score(X_train, y_train))\n",
    "        test_scores.append(rf.score(X_test, y_test))\n",
    "        \n",
    "        if isinstance(mf, str):\n",
    "            labels.append(mf)\n",
    "        else:\n",
    "            labels.append(f'{int(mf*n_features)}')\n",
    "    \n",
    "    x_pos = np.arange(len(max_features_range))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(x_pos - width/2, train_scores, width, label='Training Score', \n",
    "            color='skyblue', edgecolor='black')\n",
    "    plt.bar(x_pos + width/2, test_scores, width, label='Test Score', \n",
    "            color='lightcoral', edgecolor='black')\n",
    "    plt.xlabel('max_features', fontsize=12)\n",
    "    plt.ylabel('Accuracy Score', fontsize=12)\n",
    "    plt.title('Effect of max_features on Model Performance', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(x_pos, labels)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.ylim(0.9, 1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPerformance vs max_features:\")\n",
    "    for mf, label, train, test in zip(max_features_range, labels, train_scores, test_scores):\n",
    "        print(f\"max_features={label:>4s}: Train={train:.4f}, Test={test:.4f}\")\n",
    "\n",
    "tune_max_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for Best Parameters\n",
    "def grid_search_tuning():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"GRID SEARCH HYPERPARAMETER TUNING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    X, y = load_breast_cancer(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 20, None],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nSearching over parameter grid:\")\n",
    "    print(param_grid)\n",
    "    print(f\"\\nTotal combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    \n",
    "    # Create and fit grid search\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Results\n",
    "    print(f\"\\nBest Parameters:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nBest Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Test set performance\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    test_score = best_rf.score(X_test, y_test)\n",
    "    print(f\"Test Set Score: {test_score:.4f}\")\n",
    "    \n",
    "    # Compare with default parameters\n",
    "    default_rf = RandomForestClassifier(random_state=42)\n",
    "    default_rf.fit(X_train, y_train)\n",
    "    default_score = default_rf.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"\\nDefault Parameters Test Score: {default_score:.4f}\")\n",
    "    print(f\"Improvement: {(test_score - default_score)*100:.2f}%\")\n",
    "    \n",
    "    # Visualize top parameter combinations\n",
    "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    results_df = results_df.sort_values('rank_test_score').head(10)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(range(len(results_df)), results_df['mean_test_score'], color='skyblue', edgecolor='black')\n",
    "    plt.yticks(range(len(results_df)), [f\"Rank {i+1}\" for i in range(len(results_df))])\n",
    "    plt.xlabel('Mean CV Score', fontsize=12)\n",
    "    plt.ylabel('Parameter Combination Rank', fontsize=12)\n",
    "    plt.title('Top 10 Parameter Combinations', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "grid_search_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison with Single Decision Tree\n",
    "\n",
    "### Why Random Forest is Better\n",
    "\n",
    "| Aspect | Decision Tree | Random Forest |\n",
    "|--------|---------------|---------------|\n",
    "| **Overfitting** | High risk | Low risk (averaging reduces variance) |\n",
    "| **Stability** | Sensitive to data changes | Robust and stable |\n",
    "| **Accuracy** | Good | Better (ensemble effect) |\n",
    "| **Interpretability** | High (single tree) | Lower (multiple trees) |\n",
    "| **Training Time** | Fast | Slower (multiple trees) |\n",
    "| **Prediction Time** | Fast | Slower (multiple predictions) |\n",
    "| **Feature Importance** | Biased | More reliable |\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "**Use Decision Tree when:**\n",
    "- Interpretability is crucial\n",
    "- Dataset is small\n",
    "- Speed is critical\n",
    "- Simple relationships expected\n",
    "\n",
    "**Use Random Forest when:**\n",
    "- Accuracy is priority\n",
    "- Dataset is large\n",
    "- Complex relationships expected\n",
    "- Robustness to noise needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Comparison\n",
    "def compare_tree_vs_forest():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DECISION TREE VS RANDOM FOREST COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load data\n",
    "    X, y = load_breast_cancer(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train models\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    dt.fit(X_train, y_train)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    dt_train_score = dt.score(X_train, y_train)\n",
    "    dt_test_score = dt.score(X_test, y_test)\n",
    "    rf_train_score = rf.score(X_train, y_train)\n",
    "    rf_test_score = rf.score(X_test, y_test)\n",
    "    \n",
    "    # Cross-validation\n",
    "    dt_cv_scores = cross_val_score(dt, X, y, cv=5)\n",
    "    rf_cv_scores = cross_val_score(rf, X, y, cv=5)\n",
    "    \n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Metric':<25} {'Decision Tree':>20} {'Random Forest':>20}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Training Accuracy':<25} {dt_train_score:>20.4f} {rf_train_score:>20.4f}\")\n",
    "    print(f\"{'Test Accuracy':<25} {dt_test_score:>20.4f} {rf_test_score:>20.4f}\")\n",
    "    print(f\"{'CV Mean':<25} {dt_cv_scores.mean():>20.4f} {rf_cv_scores.mean():>20.4f}\")\n",
    "    print(f\"{'CV Std':<25} {dt_cv_scores.std():>20.4f} {rf_cv_scores.std():>20.4f}\")\n",
    "    print(f\"{'Overfit (Train-Test)':<25} {(dt_train_score-dt_test_score):>20.4f} {(rf_train_score-rf_test_score):>20.4f}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Training vs Test Accuracy\n",
    "    models = ['Decision Tree', 'Random Forest']\n",
    "    train_scores = [dt_train_score, rf_train_score]\n",
    "    test_scores = [dt_test_score, rf_test_score]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 0].bar(x - width/2, train_scores, width, label='Training', color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].bar(x + width/2, test_scores, width, label='Test', color='lightcoral', edgecolor='black')\n",
    "    axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "    axes[0, 0].set_title('Training vs Test Accuracy', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(models)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "    axes[0, 0].set_ylim(0.9, 1.0)\n",
    "    \n",
    "    # 2. Cross-Validation Scores\n",
    "    axes[0, 1].boxplot([dt_cv_scores, rf_cv_scores], labels=models, \n",
    "                       patch_artist=True,\n",
    "                       boxprops=dict(facecolor='lightblue', edgecolor='black'),\n",
    "                       medianprops=dict(color='red', linewidth=2))\n",
    "    axes[0, 1].set_ylabel('Accuracy', fontsize=11)\n",
    "    axes[0, 1].set_title('Cross-Validation Scores (5-Fold)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. Overfitting comparison\n",
    "    overfitting = [dt_train_score - dt_test_score, rf_train_score - rf_test_score]\n",
    "    colors = ['#e74c3c' if o > 0.05 else '#2ecc71' for o in overfitting]\n",
    "    axes[1, 0].bar(models, overfitting, color=colors, edgecolor='black')\n",
    "    axes[1, 0].set_ylabel('Overfitting Gap\\n(Train - Test)', fontsize=11)\n",
    "    axes[1, 0].set_title('Overfitting Comparison', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].axhline(y=0.05, color='orange', linestyle='--', linewidth=2, label='Threshold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Learning curves\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "    dt_train_scores_lc = []\n",
    "    dt_test_scores_lc = []\n",
    "    rf_train_scores_lc = []\n",
    "    rf_test_scores_lc = []\n",
    "    \n",
    "    for size in train_sizes:\n",
    "        n_samples = int(len(X_train) * size)\n",
    "        X_subset = X_train[:n_samples]\n",
    "        y_subset = y_train[:n_samples]\n",
    "        \n",
    "        dt_temp = DecisionTreeClassifier(random_state=42)\n",
    "        dt_temp.fit(X_subset, y_subset)\n",
    "        dt_train_scores_lc.append(dt_temp.score(X_subset, y_subset))\n",
    "        dt_test_scores_lc.append(dt_temp.score(X_test, y_test))\n",
    "        \n",
    "        rf_temp = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf_temp.fit(X_subset, y_subset)\n",
    "        rf_train_scores_lc.append(rf_temp.score(X_subset, y_subset))\n",
    "        rf_test_scores_lc.append(rf_temp.score(X_test, y_test))\n",
    "    \n",
    "    axes[1, 1].plot(train_sizes * len(X_train), dt_test_scores_lc, 'o-', \n",
    "                    label='Decision Tree', linewidth=2, markersize=6)\n",
    "    axes[1, 1].plot(train_sizes * len(X_train), rf_test_scores_lc, 's-', \n",
    "                    label='Random Forest', linewidth=2, markersize=6)\n",
    "    axes[1, 1].set_xlabel('Training Set Size', fontsize=11)\n",
    "    axes[1, 1].set_ylabel('Test Accuracy', fontsize=11)\n",
    "    axes[1, 1].set_title('Learning Curves', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_tree_vs_forest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Variable Importance Plots\n",
    "\n",
    "Variable importance plots help us understand:\n",
    "1. Which features contribute most to predictions\n",
    "2. Which features can be removed (feature selection)\n",
    "3. Domain insights about the problem\n",
    "\n",
    "### Types of Importance Plots\n",
    "\n",
    "1. **Bar Plot**: Standard horizontal/vertical bars\n",
    "2. **Cumulative Importance**: Shows cumulative contribution\n",
    "3. **Grouped Importance**: Groups related features\n",
    "4. **Comparison Plot**: Compares different importance methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Variable Importance Visualization\n",
    "def comprehensive_importance_plots():\n",
    "    # Load data\n",
    "    data = load_breast_cancer()\n",
    "    X, y = data.data, data.target\n",
    "    feature_names = data.feature_names\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train model\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Get importances\n",
    "    mdi_importance = rf.feature_importances_\n",
    "    perm_result = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)\n",
    "    perm_importance = perm_result.importances_mean\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Top features bar plot\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    indices = np.argsort(mdi_importance)[::-1][:15]\n",
    "    ax1.barh(range(len(indices)), mdi_importance[indices], color='skyblue', edgecolor='black')\n",
    "    ax1.set_yticks(range(len(indices)))\n",
    "    ax1.set_yticklabels([feature_names[i] for i in indices])\n",
    "    ax1.set_xlabel('Importance Score', fontsize=11)\n",
    "    ax1.set_title('Top 15 Features by MDI Importance', fontsize=12, fontweight='bold')\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 2. Cumulative importance\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    sorted_importance = np.sort(mdi_importance)[::-1]\n",
    "    cumulative_importance = np.cumsum(sorted_importance)\n",
    "    ax2.plot(range(1, len(cumulative_importance) + 1), cumulative_importance, \n",
    "             'o-', linewidth=2, markersize=4, color='darkblue')\n",
    "    ax2.axhline(y=0.95, color='r', linestyle='--', linewidth=2, label='95% threshold')\n",
    "    ax2.axhline(y=0.90, color='orange', linestyle='--', linewidth=2, label='90% threshold')\n",
    "    ax2.set_xlabel('Number of Features', fontsize=11)\n",
    "    ax2.set_ylabel('Cumulative Importance', fontsize=11)\n",
    "    ax2.set_title('Cumulative Feature Importance', fontsize=12, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Find number of features for 95% importance\n",
    "    n_features_95 = np.argmax(cumulative_importance >= 0.95) + 1\n",
    "    ax2.axvline(x=n_features_95, color='r', linestyle=':', alpha=0.5)\n",
    "    ax2.text(n_features_95, 0.5, f'{n_features_95} features', rotation=90, \n",
    "             verticalalignment='center', fontsize=10)\n",
    "    \n",
    "    # 3. MDI vs Permutation importance\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    indices = np.argsort(mdi_importance)[::-1][:15]\n",
    "    x = np.arange(len(indices))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax3.barh(x - width/2, mdi_importance[indices], width, label='MDI', \n",
    "             color='skyblue', edgecolor='black')\n",
    "    ax3.barh(x + width/2, perm_importance[indices], width, label='Permutation', \n",
    "             color='lightcoral', edgecolor='black')\n",
    "    ax3.set_yticks(x)\n",
    "    ax3.set_yticklabels([feature_names[i] for i in indices])\n",
    "    ax3.set_xlabel('Importance Score', fontsize=11)\n",
    "    ax3.set_title('MDI vs Permutation Importance', fontsize=12, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.invert_yaxis()\n",
    "    ax3.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 4. Feature importance with error bars (permutation)\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    indices = np.argsort(perm_importance)[::-1][:15]\n",
    "    perm_std = perm_result.importances_std[indices]\n",
    "    \n",
    "    ax4.barh(range(len(indices)), perm_importance[indices], \n",
    "             xerr=perm_std, color='lightgreen', edgecolor='black', capsize=5)\n",
    "    ax4.set_yticks(range(len(indices)))\n",
    "    ax4.set_yticklabels([feature_names[i] for i in indices])\n",
    "    ax4.set_xlabel('Permutation Importance', fontsize=11)\n",
    "    ax4.set_title('Permutation Importance with Std Dev', fontsize=12, fontweight='bold')\n",
    "    ax4.invert_yaxis()\n",
    "    ax4.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print feature selection recommendations\n",
    "    print(f\"\\nFeature Selection Recommendations:\")\n",
    "    print(f\"For 90% importance: Use top {np.argmax(cumulative_importance >= 0.90) + 1} features\")\n",
    "    print(f\"For 95% importance: Use top {n_features_95} features\")\n",
    "    print(f\"For 99% importance: Use top {np.argmax(cumulative_importance >= 0.99) + 1} features\")\n",
    "\n",
    "comprehensive_importance_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Partial Dependence Plots\n",
    "\n",
    "### What are Partial Dependence Plots (PDP)?\n",
    "\n",
    "**Partial Dependence Plots** show the marginal effect of a feature on the predicted outcome:\n",
    "- Shows relationship between feature and prediction\n",
    "- Marginalizes over all other features\n",
    "- Helps understand feature effects\n",
    "- Useful for model interpretation\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "For feature $x_s$, the partial dependence function is:\n",
    "\n",
    "$$PD(x_s) = \\mathbb{E}_{x_c}[\\hat{f}(x_s, x_c)] = \\int \\hat{f}(x_s, x_c) p(x_c) dx_c$$\n",
    "\n",
    "where:\n",
    "- $x_s$ is the feature of interest\n",
    "- $x_c$ are all other features\n",
    "- $\\hat{f}$ is the model prediction\n",
    "- $p(x_c)$ is the marginal distribution\n",
    "\n",
    "### Types of PDPs\n",
    "\n",
    "1. **1D PDP**: Effect of single feature\n",
    "2. **2D PDP**: Interaction between two features\n",
    "3. **ICE plots**: Individual Conditional Expectation (shows variability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Dependence Plots\n",
    "def create_partial_dependence_plots():\n",
    "    # Load data\n",
    "    data = load_breast_cancer()\n",
    "    X, y = data.data, data.target\n",
    "    feature_names = data.feature_names\n",
    "    \n",
    "    # Train model\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    # Get top 4 important features\n",
    "    importances = rf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1][:4]\n",
    "    top_features = indices.tolist()\n",
    "    \n",
    "    print(\"Creating Partial Dependence Plots for top features:\")\n",
    "    for idx in top_features:\n",
    "        print(f\"  - {feature_names[idx]}\")\n",
    "    \n",
    "    # Create 1D PDPs\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, feature_idx in enumerate(top_features):\n",
    "        display = PartialDependenceDisplay.from_estimator(\n",
    "            rf, X, [feature_idx],\n",
    "            feature_names=feature_names,\n",
    "            ax=axes[i],\n",
    "            kind='both',  # Shows both PD and ICE\n",
    "            random_state=42\n",
    "        )\n",
    "        axes[i].set_title(f'PDP: {feature_names[feature_idx]}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create 2D PDP (interaction between top 2 features)\n",
    "    print(f\"\\nCreating 2D PDP for interaction between:\")\n",
    "    print(f\"  - {feature_names[top_features[0]]}\")\n",
    "    print(f\"  - {feature_names[top_features[1]]}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    display = PartialDependenceDisplay.from_estimator(\n",
    "        rf, X, [(top_features[0], top_features[1])],\n",
    "        feature_names=feature_names,\n",
    "        ax=ax,\n",
    "        kind='average'\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_partial_dependence_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Real-world Applications\n",
    "\n",
    "### Common Applications\n",
    "\n",
    "1. **Healthcare**\n",
    "   - Disease prediction\n",
    "   - Patient risk assessment\n",
    "   - Drug discovery\n",
    "   - Medical image analysis\n",
    "\n",
    "2. **Finance**\n",
    "   - Credit scoring\n",
    "   - Fraud detection\n",
    "   - Stock market prediction\n",
    "   - Risk management\n",
    "\n",
    "3. **E-commerce**\n",
    "   - Customer churn prediction\n",
    "   - Recommendation systems\n",
    "   - Price optimization\n",
    "   - Demand forecasting\n",
    "\n",
    "4. **Marketing**\n",
    "   - Customer segmentation\n",
    "   - Campaign optimization\n",
    "   - Lead scoring\n",
    "   - Click-through rate prediction\n",
    "\n",
    "5. **Manufacturing**\n",
    "   - Quality control\n",
    "   - Predictive maintenance\n",
    "   - Process optimization\n",
    "   - Defect detection\n",
    "\n",
    "### Case Study: Credit Risk Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Risk Assessment Case Study\n",
    "def credit_risk_case_study():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CASE STUDY: CREDIT RISK ASSESSMENT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Simulate credit data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Features\n",
    "    age = np.random.randint(18, 70, n_samples)\n",
    "    income = np.random.exponential(50000, n_samples)\n",
    "    debt_ratio = np.random.uniform(0, 1, n_samples)\n",
    "    credit_score = np.random.normal(650, 100, n_samples)\n",
    "    num_accounts = np.random.randint(0, 10, n_samples)\n",
    "    \n",
    "    # Create target (default risk)\n",
    "    # Higher risk if: low income, high debt ratio, low credit score\n",
    "    risk_score = (\n",
    "        -0.00001 * income +\n",
    "        0.3 * debt_ratio +\n",
    "        -0.002 * credit_score +\n",
    "        np.random.normal(0, 0.1, n_samples)\n",
    "    )\n",
    "    default = (risk_score > np.median(risk_score)).astype(int)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'age': age,\n",
    "        'income': income,\n",
    "        'debt_ratio': debt_ratio,\n",
    "        'credit_score': credit_score,\n",
    "        'num_accounts': num_accounts,\n",
    "        'default': default\n",
    "    })\n",
    "    \n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(df.describe())\n",
    "    print(f\"\\nDefault Rate: {default.mean()*100:.2f}%\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df.drop('default', axis=1).values\n",
    "    y = df['default'].values\n",
    "    feature_names = df.drop('default', axis=1).columns.tolist()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = rf.predict(X_test)\n",
    "    y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Default', 'Default']))\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n",
    "                xticklabels=['No Default', 'Default'],\n",
    "                yticklabels=['No Default', 'Default'])\n",
    "    axes[0, 0].set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('True Label')\n",
    "    axes[0, 0].set_xlabel('Predicted Label')\n",
    "    \n",
    "    # 2. Feature Importance\n",
    "    importance = rf.feature_importances_\n",
    "    indices = np.argsort(importance)[::-1]\n",
    "    axes[0, 1].barh(range(len(importance)), importance[indices], color='skyblue', edgecolor='black')\n",
    "    axes[0, 1].set_yticks(range(len(importance)))\n",
    "    axes[0, 1].set_yticklabels([feature_names[i] for i in indices])\n",
    "    axes[0, 1].set_xlabel('Importance')\n",
    "    axes[0, 1].set_title('Feature Importance', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].invert_yaxis()\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 3. Probability Distribution\n",
    "    axes[1, 0].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.6, \n",
    "                    label='No Default', color='green', edgecolor='black')\n",
    "    axes[1, 0].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.6, \n",
    "                    label='Default', color='red', edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('Predicted Probability')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Default Probability Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Risk Segmentation\n",
    "    risk_thresholds = [0.3, 0.5, 0.7]\n",
    "    risk_groups = ['Low Risk', 'Medium Risk', 'High Risk', 'Very High Risk']\n",
    "    risk_assignment = np.digitize(y_pred_proba, risk_thresholds)\n",
    "    \n",
    "    risk_counts = [np.sum(risk_assignment == i) for i in range(4)]\n",
    "    colors = ['#2ecc71', '#f39c12', '#e67e22', '#e74c3c']\n",
    "    \n",
    "    axes[1, 1].pie(risk_counts, labels=risk_groups, autopct='%1.1f%%',\n",
    "                   colors=colors, startangle=90)\n",
    "    axes[1, 1].set_title('Customer Risk Segmentation', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Business Insights\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BUSINESS INSIGHTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, group in enumerate(risk_groups):\n",
    "        count = risk_counts[i]\n",
    "        pct = count / len(y_test) * 100\n",
    "        print(f\"{group:>15s}: {count:>4d} customers ({pct:>5.1f}%)\")\n",
    "    \n",
    "    print(\"\\nRecommendations:\")\n",
    "    print(\"  - Low Risk: Approve with standard rates\")\n",
    "    print(\"  - Medium Risk: Approve with slightly higher rates\")\n",
    "    print(\"  - High Risk: Approve with higher rates or collateral\")\n",
    "    print(\"  - Very High Risk: Reject or require significant collateral\")\n",
    "\n",
    "credit_risk_case_study()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Practice Problems\n",
    "\n",
    "### Problem 1: Wine Quality Classification\n",
    "\n",
    "Build a Random Forest classifier to predict wine quality (good vs bad) using the wine quality dataset.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load and explore the data\n",
    "2. Split into train/test sets\n",
    "3. Train a Random Forest classifier\n",
    "4. Evaluate performance\n",
    "5. Find top 5 important features\n",
    "6. Compare with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: Wine Quality Classification\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "def wine_quality_problem():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PROBLEM 1: WINE QUALITY CLASSIFICATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load data\n",
    "    data = load_wine()\n",
    "    X, y = data.data, data.target\n",
    "    feature_names = data.feature_names\n",
    "    \n",
    "    print(\"\\nDataset Information:\")\n",
    "    print(f\"Number of samples: {len(X)}\")\n",
    "    print(f\"Number of features: {X.shape[1]}\")\n",
    "    print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "    \n",
    "    # TODO: Your code here\n",
    "    # 1. Split data into train/test (70/30)\n",
    "    # 2. Train Random Forest with 100 trees\n",
    "    # 3. Calculate accuracy\n",
    "    # 4. Print classification report\n",
    "    # 5. Plot feature importance\n",
    "    # 6. Compare with Decision Tree\n",
    "    \n",
    "    print(\"\\n[Your solution here]\")\n",
    "\n",
    "# Uncomment to test\n",
    "# wine_quality_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Housing Price Prediction\n",
    "\n",
    "Build a Random Forest regressor to predict housing prices.\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate synthetic housing data\n",
    "2. Train Random Forest regressor\n",
    "3. Calculate RMSE and R² score\n",
    "4. Create actual vs predicted plot\n",
    "5. Tune hyperparameters using Grid Search\n",
    "6. Compare tuned vs default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2: Housing Price Prediction\n",
    "def housing_price_problem():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PROBLEM 2: HOUSING PRICE PREDICTION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Generate synthetic housing data\n",
    "    X, y = make_regression(n_samples=1000, n_features=10, n_informative=8,\n",
    "                          noise=10, random_state=42)\n",
    "    \n",
    "    feature_names = [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "    \n",
    "    print(\"\\nDataset Information:\")\n",
    "    print(f\"Number of samples: {len(X)}\")\n",
    "    print(f\"Number of features: {X.shape[1]}\")\n",
    "    \n",
    "    # TODO: Your code here\n",
    "    # 1. Split data into train/test (80/20)\n",
    "    # 2. Train Random Forest regressor\n",
    "    # 3. Calculate RMSE and R²\n",
    "    # 4. Create actual vs predicted plot\n",
    "    # 5. Perform Grid Search for best parameters\n",
    "    # 6. Compare results\n",
    "    \n",
    "    print(\"\\n[Your solution here]\")\n",
    "\n",
    "# Uncomment to test\n",
    "# housing_price_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Customer Churn Prediction\n",
    "\n",
    "Build a Random Forest model to predict customer churn.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create synthetic customer data (age, tenure, monthly_charges, etc.)\n",
    "2. Handle class imbalance if present\n",
    "3. Train Random Forest with OOB scoring\n",
    "4. Calculate feature importances\n",
    "5. Create partial dependence plots for top 3 features\n",
    "6. Provide business recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3: Customer Churn Prediction\n",
    "def customer_churn_problem():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PROBLEM 3: CUSTOMER CHURN PREDICTION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Generate synthetic customer data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 2000\n",
    "    \n",
    "    age = np.random.randint(18, 80, n_samples)\n",
    "    tenure = np.random.randint(0, 72, n_samples)  # months\n",
    "    monthly_charges = np.random.uniform(20, 200, n_samples)\n",
    "    total_charges = monthly_charges * tenure + np.random.normal(0, 100, n_samples)\n",
    "    num_products = np.random.randint(1, 5, n_samples)\n",
    "    \n",
    "    # Create churn (more likely if: high charges, low tenure)\n",
    "    churn_prob = (\n",
    "        0.005 * monthly_charges +\n",
    "        -0.01 * tenure +\n",
    "        -0.05 * num_products +\n",
    "        np.random.normal(0, 0.5, n_samples)\n",
    "    )\n",
    "    churn = (churn_prob > np.median(churn_prob)).astype(int)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'age': age,\n",
    "        'tenure': tenure,\n",
    "        'monthly_charges': monthly_charges,\n",
    "        'total_charges': total_charges,\n",
    "        'num_products': num_products,\n",
    "        'churn': churn\n",
    "    })\n",
    "    \n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(df.describe())\n",
    "    print(f\"\\nChurn Rate: {churn.mean()*100:.2f}%\")\n",
    "    \n",
    "    # TODO: Your code here\n",
    "    # 1. Split data and train Random Forest with OOB\n",
    "    # 2. Evaluate model performance\n",
    "    # 3. Plot feature importances\n",
    "    # 4. Create partial dependence plots\n",
    "    # 5. Identify high-risk customer segments\n",
    "    # 6. Provide retention recommendations\n",
    "    \n",
    "    print(\"\\n[Your solution here]\")\n",
    "\n",
    "# Uncomment to test\n",
    "# customer_churn_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Ensemble Methods**: Combine multiple models for better performance\n",
    "2. **Bagging**: Bootstrap aggregating reduces variance\n",
    "3. **Random Forest**: Bagging + random feature selection\n",
    "4. **OOB Error**: Free validation without separate test set\n",
    "5. **Feature Importance**: Understanding variable contributions\n",
    "\n",
    "### Advantages of Random Forest\n",
    "\n",
    "- Reduces overfitting compared to single trees\n",
    "- Handles large datasets efficiently\n",
    "- Works well with high-dimensional data\n",
    "- Provides feature importance\n",
    "- Robust to outliers and noise\n",
    "- No need for feature scaling\n",
    "- Handles missing values well\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- Less interpretable than single tree\n",
    "- Slower training and prediction\n",
    "- Memory intensive\n",
    "- Can overfit on noisy datasets\n",
    "- Biased toward categorical features with many levels\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start with defaults**: Often work well\n",
    "2. **Increase n_estimators**: More trees usually help (with diminishing returns)\n",
    "3. **Use OOB score**: For quick validation\n",
    "4. **Check feature importance**: For insights and feature selection\n",
    "5. **Tune max_depth**: Control complexity\n",
    "6. **Monitor training time**: Balance accuracy and speed\n",
    "7. **Cross-validate**: Ensure robust performance\n",
    "\n",
    "### When to Use Random Forest\n",
    "\n",
    "**Use Random Forest when:**\n",
    "- High accuracy is priority\n",
    "- Large dataset available\n",
    "- Feature importance needed\n",
    "- Robustness to overfitting desired\n",
    "- Mixed feature types present\n",
    "\n",
    "**Consider alternatives when:**\n",
    "- Interpretability is critical (use Decision Tree)\n",
    "- Very large scale data (use Linear Models)\n",
    "- Real-time predictions needed (use simpler models)\n",
    "- Memory is constrained (use single tree)\n",
    "\n",
    "### Further Learning\n",
    "\n",
    "1. **Extra Trees**: Extremely Randomized Trees\n",
    "2. **Gradient Boosting**: XGBoost, LightGBM, CatBoost\n",
    "3. **Stacking**: Combining different model types\n",
    "4. **Feature Engineering**: Creating better features\n",
    "5. **Hyperparameter Optimization**: Bayesian optimization, Optuna\n",
    "\n",
    "---\n",
    "\n",
    "**Resources:**\n",
    "- Scikit-learn Documentation: https://scikit-learn.org/stable/modules/ensemble.html\n",
    "- \"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman\n",
    "- \"Random Forests\" by Leo Breiman (2001)\n",
    "- Kaggle competitions for practice\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
